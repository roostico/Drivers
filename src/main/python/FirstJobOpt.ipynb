{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cefe988a9aff7538",
   "metadata": {},
   "source": [
    "# Big Data project A.Y. 2024-2025 - First Job\n",
    "\n",
    "## Members\n",
    "\n",
    "- Giovanni Antonioni\n",
    "- Luca Rubboli - 0001083742"
   ]
  },
  {
   "cell_type": "code",
   "id": "c9c769832012f1b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:29:44.914378Z",
     "start_time": "2025-06-19T12:29:44.279834Z"
    }
   },
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder\n",
    "  .appName(\"First job\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val sc = spark.sparkContext"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5b7bc469\n",
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@352dc96a\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Define useful parameters\n",
    "\n",
    "- Dataset location\n",
    "- Iterator (defined like this to overcome different names for same columns in dataset)"
   ],
   "id": "72ce3e263b662de6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:29:45.216688Z",
     "start_time": "2025-06-19T12:29:44.919321Z"
    }
   },
   "source": [
    "val decimals: Int = 4\n",
    "val datasetDir = \"dataset\"\n",
    "val outputDir = \"output/firstJobOutputOpt\"\n",
    "val yellowDatasetDir = s\"$datasetDir/yellow_cab\"\n",
    "val greenDatasetDir = s\"$datasetDir/green_cab\"\n",
    "val fhvDatasetDir = s\"$datasetDir/fhv_cab\"\n",
    "val fhvhvDatasetDir = s\"$datasetDir/fhvhv_cab\"\n",
    "val datasetDirMap: Map[String, String] = Map(\"yellow\" -> yellowDatasetDir, \"green\" -> greenDatasetDir, \"fhv\" -> fhvDatasetDir, \"fhvhv\" -> fhvhvDatasetDir)\n",
    "val datasetIterator: Map[String, (String, String)] = Map(\n",
    "  \"yellow\" -> (\"tpep_dropoff_datetime\", \"tpep_pickup_datetime\"),\n",
    "  \"green\" -> (\"lpep_dropoff_datetime\", \"lpep_pickup_datetime\"),\n",
    "  // (\"fhv\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\"),\n",
    "  // (\"fhvhv\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\"),\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decimals: Int = 4\n",
       "datasetDir: String = dataset\n",
       "outputDir: String = output/firstJobOutputOpt\n",
       "yellowDatasetDir: String = dataset/yellow_cab\n",
       "greenDatasetDir: String = dataset/green_cab\n",
       "fhvDatasetDir: String = dataset/fhv_cab\n",
       "fhvhvDatasetDir: String = dataset/fhvhv_cab\n",
       "datasetDirMap: Map[String,String] = Map(yellow -> dataset/yellow_cab, green -> dataset/green_cab, fhv -> dataset/fhv_cab, fhvhv -> dataset/fhvhv_cab)\n",
       "datasetIterator: Map[String,(String, String)] = Map(yellow -> (tpep_dropoff_datetime,tpep_pickup_datetime), green -> (lpep_dropoff_datetime,lpep_pickup_datetime))\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "801ff1e945bd4c8",
   "metadata": {},
   "source": [
    "## Define Columns for analysis\n",
    "- Columns names\n",
    "- Time zones for overprice\n",
    "- Columns used in classification for average price calculation\n",
    "- Columns which values are used in analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "bfdb3f87a1c6a6c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:29:45.434243Z",
     "start_time": "2025-06-19T12:29:45.221120Z"
    }
   },
   "source": [
    "val colDurationMinutes: String = \"duration_minutes\"\n",
    "val colDurationMinutesBinLabel: String = \"duration_minutes_bin_label\"\n",
    "val colYear: String = \"year\"\n",
    "val colWeekdaySurcharge: String = \"weekday_surcharge\"\n",
    "val colAggregateFee: String = \"fees\"\n",
    "val colAggregateFeeBin: String = \"agg_fee_bin_label\"\n",
    "val colDistanceBin: String = \"distance_bin_label\"\n",
    "val colFareAmount: String = \"fare_amount\"\n",
    "val colPricePerDistance: String = \"cost_per_distance\"\n",
    "val colPricePerTime: String = \"cost_per_time\"\n",
    "val colAvgPricePerDistance: String = \"avg_cost_per_distance\"\n",
    "val colAvgPricePerTime: String = \"avg_cost_per_time\"\n",
    "val colPricePerDistanceDiff: String = \"cost_per_distance_diff\"\n",
    "val colPricePerDistanceDiffPcg: String = \"cost_per_distance_diff_pcg\"\n",
    "val colPricePerTimeDiff: String = \"cost_per_time_diff\"\n",
    "val colPricePerTimeDiffPcg: String = \"cost_per_time_diff_pcg\"\n",
    "val colPricePerDistanceDiffPcgLabel: String = colPricePerDistanceDiffPcg + \"_label\"\n",
    "val colPricePerTimeDiffPcgLabel: String = colPricePerTimeDiffPcg + \"_label\"\n",
    "\n",
    "val timeZoneOver: String = \"overnight\"\n",
    "val timeZones = Map(timeZoneOver -> (20, 6), \"regular\" -> (6, 20))\n",
    "val weekDaySurcharge: Double = 2.5\n",
    "\n",
    "val colDurationOvernightPcg: String = s\"${timeZoneOver}_duration_pcg\"\n",
    "\n",
    "val colToUse: Set[String] = Set(\n",
    "  \"tpep_pickup_datetime\",\n",
    "  \"tpep_dropoff_datetime\",\n",
    "  \"lpep_pickup_datetime\",\n",
    "  \"lpep_dropoff_datetime\",\n",
    "  \"passenger_count\",\n",
    "  \"trip_distance\",\n",
    "  \"ratecodeid\",\n",
    "  \"store_and_fwd_flag\",\n",
    "  \"payment_type\",\n",
    "  \"fare_amount\",\n",
    "  \"extra\",\n",
    "  \"mta_tax\",\n",
    "  \"tip_amount\",\n",
    "  \"tolls_amount\",\n",
    "  \"improvement_surcharge\",\n",
    "  \"total_amount\",\n",
    "  \"congestion_surcharge\",\n",
    "  \"airport_fee\")\n",
    "\n",
    "val colFees: Set[String] = Set(\n",
    "  \"extra\",\n",
    "  \"mta_tax\",\n",
    "  \"improvement_surcharge\",\n",
    "  \"congestion_surcharge\",\n",
    "  \"airport_fee\")\n",
    "\n",
    "val colsForClassification: Seq[String] = Seq(\n",
    "  \"passenger_count\",\n",
    "  \"store_and_fwd_flag\",\n",
    "  \"payment_type\",\n",
    "  colAggregateFeeBin,\n",
    "  colDurationMinutesBinLabel,\n",
    "  colDistanceBin,\n",
    "  colYear,\n",
    "  s\"${colDurationOvernightPcg}_label\",\n",
    "  colPricePerDistanceDiffPcgLabel,\n",
    "  colPricePerTimeDiffPcgLabel\n",
    ")\n",
    "\n",
    "val colsForValuesAnalysis: Seq[String] = Seq(\n",
    "  \"passenger_count\",\n",
    "  \"store_and_fwd_flag\",\n",
    "  \"payment_type\",\n",
    "  colAggregateFeeBin,\n",
    "  colDurationMinutesBinLabel,\n",
    "  colDistanceBin,\n",
    "  colYear,\n",
    "  s\"${colDurationOvernightPcg}_label\",\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colDurationMinutes: String = duration_minutes\n",
       "colDurationMinutesBinLabel: String = duration_minutes_bin_label\n",
       "colYear: String = year\n",
       "colWeekdaySurcharge: String = weekday_surcharge\n",
       "colAggregateFee: String = fees\n",
       "colAggregateFeeBin: String = agg_fee_bin_label\n",
       "colDistanceBin: String = distance_bin_label\n",
       "colFareAmount: String = fare_amount\n",
       "colPricePerDistance: String = cost_per_distance\n",
       "colPricePerTime: String = cost_per_time\n",
       "colAvgPricePerDistance: String = avg_cost_per_distance\n",
       "colAvgPricePerTime: String = avg_cost_per_time\n",
       "colPricePerDistanceDiff: String = cost_per_distance_diff\n",
       "colPricePerDistanceDiffPcg: String = cost_per_distance_diff_pcg\n",
       "colPricePerTimeDiff: String = cost_per_time_diff\n",
       "colPricePerTimeDiffPcg: String = cost_per_time_diff_pcg\n",
       "colPricePerDistanceDiffPcgLabel: String = ...\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "6251e1a01e2d34a6",
   "metadata": {},
   "source": [
    "### Define preprocess rules"
   ]
  },
  {
   "cell_type": "code",
   "id": "f3c66b94d590de92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:29:45.831506Z",
     "start_time": "2025-06-19T12:29:45.439435Z"
    }
   },
   "source": [
    "val featureFilters: Map[String, Any => Boolean] = Map(\n",
    "  \"passenger_count\" -> {\n",
    "    case i: Int => i > 0\n",
    "    case f: Float => val i = f.toInt; i > 0\n",
    "    case d: Double => val i = d.toInt; i > 0\n",
    "    case _ => false\n",
    "  },\n",
    "  \"trip_distance\" -> {\n",
    "    case i: Int => i > 0\n",
    "    case i: Float => i > 0\n",
    "    case i: Double => i > 0\n",
    "    case _ => false\n",
    "  },\n",
    "  \"ratecodeid\" -> {\n",
    "    case i: Int => (i >= 1 && i <= 6) || i == 99\n",
    "    case f: Float => val i = f.toInt; (i >= 1 && i <= 6) || i == 99\n",
    "    case d: Double => val i = d.toInt; (i >= 1 && i <= 6) || i == 99\n",
    "    case _ => false\n",
    "  },\n",
    "  \"store_and_fwd_flag\" -> {\n",
    "    case i: String => i == \"Y\" || i == \"N\"\n",
    "    case _ => false\n",
    "  },\n",
    "  \"payment_type\" -> {\n",
    "    case i: Int => i >= 1 && i <= 6\n",
    "    case f: Float => val i = f.toInt; i >= 1 && i <= 6\n",
    "    case d: Double => val i = d.toInt; i >= 1 && i <= 6\n",
    "    case _ => false\n",
    "  },\n",
    "  \"fare_amount\" -> {\n",
    "    case i: Int => i > 0\n",
    "    case i: Float => i > 0\n",
    "    case i: Double => i > 0\n",
    "    case _ => false\n",
    "  },\n",
    "  \"tolls_amount\" -> {\n",
    "    case i: Int => i >= 0 && i < 200\n",
    "    case i: Float => i >= 0 && i < 200\n",
    "    case i: Double => i >= 0 && i < 200\n",
    "    case _ => false\n",
    "  }\n",
    ")\n",
    "\n",
    "val taxFilter: Any => Boolean = {\n",
    "  case tax: Int => tax >= 0 && tax < 20\n",
    "  case tax: Float => tax >= 0 && tax < 20\n",
    "  case tax: Double => tax >= 0 && tax < 20\n",
    "  case _ => false\n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureFilters: Map[String,Any => Boolean] = Map(trip_distance -> $Lambda$5740/0x0000000801cb8040@7895dff9, tolls_amount -> $Lambda$5745/0x0000000801cbc040@758bf804, payment_type -> $Lambda$5743/0x0000000801cba840@7b0ab4a0, fare_amount -> $Lambda$5744/0x0000000801cbb040@402229d4, passenger_count -> $Lambda$5739/0x0000000801caf040@161e5c3e, store_and_fwd_flag -> $Lambda$5742/0x0000000801cb9840@3c7fcd21, ratecodeid -> $Lambda$5741/0x0000000801cb9040@12fed1ba)\n",
       "taxFilter: Any => Boolean = $Lambda$5746/0x0000000801cbc840@43c336b8\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "46150dfaa1006964",
   "metadata": {},
   "source": [
    "### Utils functions for rdd"
   ]
  },
  {
   "cell_type": "code",
   "id": "9aae38423ac3200c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:29:46.399177Z",
     "start_time": "2025-06-19T12:29:45.836324Z"
    }
   },
   "source": [
    "import java.time.temporal.ChronoUnit\n",
    "import java.time.{DayOfWeek, LocalDate, LocalDateTime}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Row\n",
    "import scala.math.BigDecimal.RoundingMode\n",
    "\n",
    "val projectDir: String = \"/Users/luca/Desktop/Luca/Università/Magistrale/Corsi/BigData/Drivers\"\n",
    "\n",
    "def getDatasetPath(localPath: String): String = {\n",
    "  \"file://\" + projectDir + \"/\" + localPath\n",
    "}\n",
    "\n",
    "def binColByStepValue(rdd: RDD[Row], indexOfColToDiscrete: Int, stepValue: Int = 5): RDD[Row] = {\n",
    "  rdd.map { row =>\n",
    "    val value: Double = row.get(indexOfColToDiscrete) match {\n",
    "      case i: Int => i.toDouble\n",
    "      case d: Double => d\n",
    "      case l: Long => l.toDouble\n",
    "      case s: String => try { s.toDouble } catch { case _: Throwable => Double.NaN}\n",
    "      case _ => Double.NaN\n",
    "    }\n",
    "\n",
    "    val rawBin = (value / stepValue).toInt * stepValue\n",
    "    val binBase = if (value < 0 && value % stepValue == 0) rawBin + stepValue else rawBin\n",
    "    val label = if (value < 0) { s\"[${(binBase - stepValue).toInt}|${binBase.toInt})\" } else { s\"[${binBase.toInt}|${(binBase + stepValue).toInt})\" }\n",
    "\n",
    "    Row.fromSeq(row.toSeq :+ label)\n",
    "  }\n",
    "}\n",
    "\n",
    "val castForFilter: Any => Any = {\n",
    "  case s: String => if (s.matches(\"\"\"^-?\\d+\\.\\d+$\"\"\")) s.toDouble else if (s.matches(\"\"\"^-?\\d+$\"\"\")) s.toInt else s.trim\n",
    "  case d: Double => d\n",
    "  case i: Int => i\n",
    "  case l: Long => l.toDouble\n",
    "  case f: Float => f.toDouble\n",
    "  case b: Boolean => b\n",
    "  case null => null\n",
    "  case other => other.toString.trim\n",
    "}\n",
    "\n",
    "val preciseBucketUDF: (Map[String, (Int, Int)], LocalDateTime, LocalDateTime, Int) => Map[String, Double] = { (timeZones: Map[String, (Int, Int)], start: LocalDateTime, end: LocalDateTime, decimals: Int) =>\n",
    "\n",
    "  val overlap: (LocalDateTime, LocalDateTime, LocalDateTime, LocalDateTime, Int) => Double = { (start1: LocalDateTime, end1: LocalDateTime, start2: LocalDateTime, end2: LocalDateTime, decimals: Int) =>\n",
    "    val overlapStart = if (start1.isAfter(start2)) start1 else start2\n",
    "    val overlapEnd = if (end1.isBefore(end2)) end1 else end2\n",
    "    if (overlapEnd.isAfter(overlapStart)) BigDecimal(ChronoUnit.MILLIS.between(overlapStart, overlapEnd) / 60000.0).setScale(decimals, RoundingMode.HALF_UP).toDouble else 0.0\n",
    "  }\n",
    "\n",
    "  var result = timeZones.keys.map(_ -> 0.0).toMap\n",
    "\n",
    "  if (!(start == null || end == null)) {\n",
    "\n",
    "    if (!end.isBefore(start)) {\n",
    "\n",
    "      var current = start.toLocalDate.atStartOfDay\n",
    "\n",
    "      while (!current.isAfter(end)) {\n",
    "        val nextDay = current.plusDays(1)\n",
    "\n",
    "        timeZones.foreach {\n",
    "          case (label, (startHour, endHour)) if startHour > endHour =>\n",
    "            val bucketStartBeforeMidnight = current.withHour(startHour).withMinute(0).withSecond(0).withNano(0)\n",
    "            val bucketEndBeforeMidnight = current.withHour(23).withMinute(59).withSecond(59)\n",
    "            val bucketStartAfterMidnight = current.withHour(0).withMinute(0).withSecond(0).withNano(0)\n",
    "            val bucketEndAfterMidnight = current.withHour(endHour).withMinute(0).withSecond(0).withNano(0)\n",
    "\n",
    "            val minutesBeforeMidnight = overlap(start, end, bucketStartBeforeMidnight, bucketEndBeforeMidnight, decimals)\n",
    "            val minutesAfterMidnight = overlap(start, end, bucketStartAfterMidnight, bucketEndAfterMidnight, decimals)\n",
    "\n",
    "            result = result.updated(label, result(label) + minutesBeforeMidnight + minutesAfterMidnight)\n",
    "\n",
    "          case (label, (startHour, endHour)) =>\n",
    "            val bucketStart = current.withHour(startHour).withMinute(0).withSecond(0).withNano(0)\n",
    "            val bucketEnd = if (endHour == 24) current.plusDays(1).withHour(0).withMinute(0).withSecond(0).withNano(0) else current.withHour(endHour).withMinute(0).withSecond(0).withNano(0)\n",
    "\n",
    "            val minutes = overlap(start, end, bucketStart, bucketEnd, decimals)\n",
    "\n",
    "            result = result.updated(label, result(label) + minutes)\n",
    "        }\n",
    "\n",
    "        current = nextDay\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  result\n",
    "}\n",
    "\n",
    "val isUSHolidayOrWeekend: LocalDate => Boolean = { date =>\n",
    "  val month = date.getMonthValue\n",
    "  val day = date.getDayOfMonth\n",
    "  val dayOfWeek = date.getDayOfWeek\n",
    "\n",
    "  val isIndependenceDay = month == 7 && day == 4\n",
    "  val isChristmas = month == 12 && day == 25\n",
    "  val isNewYear = month == 1 && day == 1\n",
    "  val isLaborDay = month == 9 && dayOfWeek == DayOfWeek.MONDAY && day <= 7\n",
    "\n",
    "  val isThanksgiving = month == 11 && dayOfWeek == DayOfWeek.THURSDAY && day >= 22 && day <= 28 && ((day - 1) / 7 + 1 == 4)\n",
    "\n",
    "  isIndependenceDay || isChristmas || isNewYear || isLaborDay || isThanksgiving || dayOfWeek == DayOfWeek.SATURDAY || dayOfWeek == DayOfWeek.SUNDAY\n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.temporal.ChronoUnit\n",
       "import java.time.{DayOfWeek, LocalDate, LocalDateTime}\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.sql.Row\n",
       "import scala.math.BigDecimal.RoundingMode\n",
       "projectDir: String = /Users/luca/Desktop/Luca/Università/Magistrale/Corsi/BigData/Drivers\n",
       "getDatasetPath: (localPath: String)String\n",
       "binColByStepValue: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], indexOfColToDiscrete: Int, stepValue: Int)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "castForFilter: Any => Any = $Lambda$5747/0x0000000801cbf040@4e8178d0\n",
       "preciseBucketUDF: (Map[String,(Int, Int)], java.time.LocalDateTime, java.time.LocalDateTime, Int) => Map[String,Double] = $Lambda$5748/0x0000000801cbfc40@1673a4c5\n",
       "isUSHolidayOrWeekend: java.time.LocalDate => Boolean = $Lambda$57...\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "d67c5a94825ecee9",
   "metadata": {},
   "source": [
    "# Actual job\n",
    "\n",
    "1) Select dataset [yellow or green]"
   ]
  },
  {
   "cell_type": "code",
   "id": "60aee06709329346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:29:46.699737Z",
     "start_time": "2025-06-19T12:29:46.403727Z"
    }
   },
   "source": [
    "val name: String = \"yellow\"\n",
    "val (dropoff, pickup) = datasetIterator(name)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: String = yellow\n",
       "dropoff: String = tpep_dropoff_datetime\n",
       "pickup: String = tpep_pickup_datetime\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2) Load dataset",
   "id": "6e3fcaeb36518cd9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:29:47.727971Z",
     "start_time": "2025-06-19T12:29:46.704834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val startTime = System.currentTimeMillis()\n",
    "\n",
    "val dataset = spark.read.parquet(getDatasetPath(datasetDirMap(name)))\n",
    "var headers: Seq[String] = dataset.columns.map(_.toLowerCase)\n",
    "val indexesToUse: Seq[Int] = headers.zipWithIndex.collect {\n",
    "  case(h, i) if colToUse.contains(h.toLowerCase) => i\n",
    "}"
   ],
   "id": "a846e646fac3bac4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startTime: Long = 1750336187435\n",
       "dataset: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: timestamp_ntz ... 17 more fields]\n",
       "headers: Seq[String] = ArraySeq(vendorid, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, ratecodeid, store_and_fwd_flag, pulocationid, dolocationid, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge, airport_fee)\n",
       "indexesToUse: Seq[Int] = ArraySeq(1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18)\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3) Filter taxes and features based on filter conditions previously defined",
   "id": "df209b8e24cd13e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:29:48.390020Z",
     "start_time": "2025-06-19T12:29:47.732810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "def transformRDD(dataset: DataFrame, headers: Seq[String], idxs: Seq[Int], castFunc: Any => Any): RDD[Row] = {\n",
    "  dataset.rdd.map(row => Row.fromSeq(idxs.map(row.get).map(castFunc)))\n",
    "}\n",
    "\n",
    "val rdd = transformRDD(dataset, headers, indexesToUse, castForFilter).persist(StorageLevel.MEMORY_ONLY)\n",
    "headers = headers.filter(head => colToUse.contains(head.toLowerCase))\n",
    "\n",
    "def applyFilters(rdd: RDD[Row], headers: Seq[String], colOfFees: Set[String], taxFilter: Any => Boolean, featFilter: Map[String, Any => Boolean]): RDD[Row] = {\n",
    "  rdd.filter { row =>\n",
    "    headers.zip(row.toSeq).forall {case(header: String, value) =>\n",
    "      val taxFilterCondition = if (colOfFees.contains(header.toLowerCase)) taxFilter(value) else true\n",
    "      featFilter.get(header.toLowerCase) match {\n",
    "        case Some(filterFunc) => taxFilterCondition && filterFunc(value)\n",
    "        case None => taxFilterCondition // no filter defined for this column, so accept it\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "val rddFiltered = applyFilters(rdd, headers, colFees, taxFilter, featureFilters)\n",
    "rdd.unpersist()"
   ],
   "id": "f8a8a0eaeaa2d294",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.storage.StorageLevel\n",
       "transformRDD: (dataset: org.apache.spark.sql.DataFrame, headers: Seq[String], idxs: Seq[Int], castFunc: Any => Any)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[75] at map at <console>:59\n",
       "headers: Seq[String] = ArraySeq(tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, ratecodeid, store_and_fwd_flag, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge, airport_fee)\n",
       "applyFilters: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], headers: Seq[String], colOfFees: Set[String], taxFilter: Any => Boolean, featFilter: Map[Str...\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4) Add duration and timezones",
   "id": "523a6cb64b4c03d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:29:48.747353Z",
     "start_time": "2025-06-19T12:29:48.395123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import java.time.format.DateTimeFormatter\n",
    "import java.time.Duration\n",
    "\n",
    "def addDuration(rdd: RDD[Row], headers: Seq[String], pickup: String, dropoff: String, decimals: Int): RDD[Row] = {\n",
    "  rdd.map {row =>\n",
    "    val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm[:ss]\")\n",
    "\n",
    "    val pickupStr = row.getAs[String](headers.indexOf(pickup)).trim\n",
    "    val dropoffStr = row.getAs[String](headers.indexOf(dropoff)).trim\n",
    "\n",
    "    val pickupTS = LocalDateTime.parse(pickupStr, formatter)\n",
    "    val dropoffTS = LocalDateTime.parse(dropoffStr, formatter)\n",
    "    val durationMillis = Duration.between(pickupTS, dropoffTS).toMillis\n",
    "    val durationMinutes = BigDecimal(durationMillis / 60000.0).setScale(decimals, RoundingMode.HALF_UP).toDouble\n",
    "\n",
    "    val pickupYear = pickupTS.getYear\n",
    "\n",
    "    Row.fromSeq(row.toSeq ++ Seq(durationMinutes, pickupYear))\n",
    "  }.filter { row => row.getAs[Double](row.toSeq.length - 2) > 0.0 }\n",
    "}\n",
    "\n",
    "val rddDuration = addDuration(rddFiltered, headers, pickup, dropoff, decimals)\n",
    "headers = headers ++ Seq(colDurationMinutes, colYear)\n",
    "\n",
    "val rddDurationBin = binColByStepValue(rddDuration, headers.indexOf(colDurationMinutes), 5)\n",
    "headers = headers :+ colDurationMinutesBinLabel\n",
    "\n",
    "def addTimeZones(rdd: RDD[Row], headers: Seq[String], timezones: Map[String, (Int, Int)], weekDaySurcharge: Double, colDuration: String, pickup: String, dropoff: String, decimals: Int, preciseBucketUDF: (Map[String, (Int, Int)], LocalDateTime, LocalDateTime, Int) => Map[String, Double], isUSHolidayOrWeekendTZ: LocalDate => Boolean): RDD[Row] = {\n",
    "  rdd.map { row =>\n",
    "    val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm[:ss]\")\n",
    "\n",
    "    val timeZonesDuration: Map[String, Double] = preciseBucketUDF(timezones, LocalDateTime.parse(row.getAs[String](headers.indexOf(pickup)).trim, formatter), LocalDateTime.parse(row.getAs[String](headers.indexOf(dropoff)).trim, formatter), decimals)\n",
    "\n",
    "    val weekday_surcharge: Double = if (isUSHolidayOrWeekendTZ(LocalDateTime.parse(row.getAs[String](headers.indexOf(pickup)).trim, formatter).toLocalDate)) 0 else weekDaySurcharge\n",
    "    val colsToAdd: Seq[Double] = timezones.keys.toSeq.flatMap { tz =>\n",
    "      val duration = timeZonesDuration.getOrElse(tz, 0.0)\n",
    "      val totalDuration = row.getAs[Double](headers.indexOf(colDuration))\n",
    "      Seq(duration, BigDecimal(duration * 100 / totalDuration).setScale(decimals, RoundingMode.HALF_UP).toDouble)\n",
    "    }\n",
    "    Row.fromSeq((row.toSeq ++ colsToAdd) :+ weekday_surcharge)\n",
    "  }\n",
    "}\n",
    "\n",
    "val rddTimeZones = addTimeZones(rddDurationBin, headers, timeZones, weekDaySurcharge, colDurationMinutes, pickup, dropoff, decimals, preciseBucketUDF, isUSHolidayOrWeekend)\n",
    "\n",
    "val headersToAdd: Seq[String] = timeZones.keys.toSeq.flatMap { tz =>\n",
    "  Seq(tz + \"_duration\", tz + \"_duration_pcg\")\n",
    "} :+ colWeekdaySurcharge\n",
    "\n",
    "headers = headers ++ headersToAdd"
   ],
   "id": "9f3672bebd356114",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.format.DateTimeFormatter\n",
       "import java.time.Duration\n",
       "addDuration: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], headers: Seq[String], pickup: String, dropoff: String, decimals: Int)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rddDuration: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[78] at filter at <console>:84\n",
       "headers: Seq[String] = ArraySeq(tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, ratecodeid, store_and_fwd_flag, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge, airport_fee, duration_minutes, year, duration_minutes_bin_label, overnight_duration, overnight_duration_pcg, regular_duration, regular_duration_pcg, weekday...\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5) Add Aggregate fees and bins",
   "id": "135d87a329e10208"
  },
  {
   "cell_type": "code",
   "id": "cfb99aef020692d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:29:49.047212Z",
     "start_time": "2025-06-19T12:29:48.752165Z"
    }
   },
   "source": [
    "def addAggregateFees(rdd: RDD[Row], headers: Seq[String], colOfFees: Set[String]): RDD[Row] = {\n",
    "  rdd.map { row =>\n",
    "    val fees = colOfFees\n",
    "      .filter(col => headers.contains(col.toLowerCase))\n",
    "      .map(col => row.getAs[Double](headers.indexOf(col.toLowerCase))).sum\n",
    "\n",
    "    Row.fromSeq(row.toSeq :+ fees)\n",
    "  }\n",
    "}\n",
    "\n",
    "val rddAggFees = addAggregateFees(rddTimeZones, headers, colFees)\n",
    "headers = headers :+ colAggregateFee\n",
    "\n",
    "val rddAggFeesBin = binColByStepValue(rddAggFees, headers.indexOf(colAggregateFee), 2)\n",
    "headers = headers :+ colAggregateFeeBin"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addAggregateFees: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], headers: Seq[String], colOfFees: Set[String])org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rddAggFees: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[81] at map at <console>:58\n",
       "headers: Seq[String] = ArraySeq(tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, ratecodeid, store_and_fwd_flag, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge, airport_fee, duration_minutes, year, duration_minutes_bin_label, overnight_duration, overnight_duration_pcg, regular_duration, regular_duration_pcg, weekday_surcharge, fees, agg_fee_bin_label)\n",
       "rddAggFeesBin: org.apache.spark.rdd.RDD[org.apache.spa...\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6) Add price per mile and minute",
   "id": "2612d914aaba250d"
  },
  {
   "cell_type": "code",
   "id": "7a05a60d734daf56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:29:49.241715Z",
     "start_time": "2025-06-19T12:29:49.051463Z"
    }
   },
   "source": [
    "def addPricePerDistanceAndTime(rdd: RDD[Row], headers: Seq[String], colFareAmount: String, colDuration: String, colDistance: String): RDD[Row] = {\n",
    "  rdd.map { row =>\n",
    "    val pricePerTime = Math.round(row.getAs[Double](headers.indexOf(colFareAmount)) / row.getAs[Double](headers.indexOf(colDuration)) * 100) / 100.0\n",
    "    val pricePerDistance = Math.round(row.getAs[Double](headers.indexOf(colFareAmount)) / row.getAs[Double](headers.indexOf(colDistance)) * 100) / 100.0\n",
    "\n",
    "    Row.fromSeq(row.toSeq ++ Seq(pricePerTime, pricePerDistance))\n",
    "  }\n",
    "}\n",
    "\n",
    "val rddPriced = addPricePerDistanceAndTime(rddAggFeesBin, headers, colFareAmount, colDurationMinutes, \"trip_distance\")\n",
    "headers = headers ++ Seq(colPricePerTime, colPricePerDistance)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addPricePerDistanceAndTime: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], headers: Seq[String], colFareAmount: String, colDuration: String, colDistance: String)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rddPriced: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[83] at map at <console>:57\n",
       "headers: Seq[String] = ArraySeq(tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, ratecodeid, store_and_fwd_flag, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge, airport_fee, duration_minutes, year, duration_minutes_bin_label, overnight_duration, overnight_duration_pcg, regular_duration, regular_duration_pcg, weekday_surcharge, fees, agg_fee_bin_label, cost...\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "7) Add distance bin and duration in overnight time zone",
   "id": "86d18a3f7203c64e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:29:49.411451Z",
     "start_time": "2025-06-19T12:29:49.246134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val rddDistBin = binColByStepValue(rddPriced, headers.indexOf(\"trip_distance\"), 5)\n",
    "headers = headers :+ colDistanceBin\n",
    "\n",
    "val rddOvernightBin = binColByStepValue(rddDistBin, headers.indexOf(colDurationOvernightPcg), 5)\n",
    "headers = headers :+ (colDurationOvernightPcg + \"_label\")"
   ],
   "id": "d8c2b74844f233ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddDistBin: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[84] at map at <console>:56\n",
       "headers: Seq[String] = ArraySeq(tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, ratecodeid, store_and_fwd_flag, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge, airport_fee, duration_minutes, year, duration_minutes_bin_label, overnight_duration, overnight_duration_pcg, regular_duration, regular_duration_pcg, weekday_surcharge, fees, agg_fee_bin_label, cost_per_time, cost_per_distance, distance_bin_label, overnight_duration_pcg_label)\n",
       "rddOvernightBin: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[85] at map at <console>:56\n",
       "headers: Seq[String] = Array...\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "8) Add key for average calculation based on columns for classification",
   "id": "6419c663ec4fef40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:29:49.628518Z",
     "start_time": "2025-06-19T12:29:49.416354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "val numPartitions = spark.sparkContext.defaultParallelism\n",
    "val partitioner = new HashPartitioner(numPartitions)\n",
    "\n",
    "val actualHeader = headers\n",
    "def addKey(rdd: RDD[Row], colsClassification: Seq[String], headers: Seq[String]): RDD[(String, Row)] = {\n",
    "  rdd.map { row =>\n",
    "    val key = colsClassification.filter(col => headers.contains(col.toLowerCase))\n",
    "    .map(col => row.get(headers.indexOf(col.toLowerCase)))\n",
    "    .mkString(\"_\")\n",
    "    (key, row)\n",
    "  }\n",
    "}\n",
    "\n",
    "val rddWithKey = addKey(rddOvernightBin, colsForClassification, actualHeader).partitionBy(partitioner).persist(StorageLevel.MEMORY_ONLY)"
   ],
   "id": "3b88a93a6da25df1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\n",
       "numPartitions: Int = 12\n",
       "partitioner: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@c\n",
       "actualHeader: Seq[String] = ArraySeq(tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, ratecodeid, store_and_fwd_flag, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge, airport_fee, duration_minutes, year, duration_minutes_bin_label, overnight_duration, overnight_duration_pcg, regular_duration, regular_duration_pcg, weekday_surcharge, fees, agg_fee_bin_label, cost_per_time, cost_per_distance, distance_bin_label, overnight_duration_pcg_label)\n",
       "addKey: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], colsClassification: Seq[String], ...\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "9) Calculate prices per distance and time",
   "id": "d200896a8f8a49d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:29:49.799397Z",
     "start_time": "2025-06-19T12:29:49.632772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculatePrices(rdd: RDD[(String, Row)], headers: Seq[String], colPriceDistance: String, colPriceTime: String): RDD[(String, (Double, Double, Long))] = {\n",
    "  rdd.mapValues { row =>\n",
    "    val costPerDistance = row.getAs[Double](headers.indexOf(colPriceDistance))\n",
    "    val costPerTime = row.getAs[Double](headers.indexOf(colPriceTime))\n",
    "    (costPerDistance, costPerTime, 1L)\n",
    "  }\n",
    "}\n",
    "\n",
    "val rddForAvg = calculatePrices(rddWithKey, headers, colPricePerDistance, colPricePerTime)"
   ],
   "id": "85e4e86bc9992218",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculatePrices: (rdd: org.apache.spark.rdd.RDD[(String, org.apache.spark.sql.Row)], headers: Seq[String], colPriceDistance: String, colPriceTime: String)org.apache.spark.rdd.RDD[(String, (Double, Double, Long))]\n",
       "rddForAvg: org.apache.spark.rdd.RDD[(String, (Double, Double, Long))] = MapPartitionsRDD[88] at mapValues at <console>:56\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "10) Calculate average prices per distance and time",
   "id": "ab90fed37146ba50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:29:50.054554Z",
     "start_time": "2025-06-19T12:29:49.803679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculateAvgPrices(rdd: RDD[(String, (Double, Double, Long))], decimals: Int): RDD[(String, (Double, Double))] = {\n",
    "  rdd.aggregateByKey((0.0, 0.0, 0L))((acc, v) => (acc._1 + v._1, acc._2 + v._2, acc._3 + v._3), (a, b) => (a._1 + b._1, a._2 + b._2, a._3 + b._3)).mapValues {\n",
    "    case(sumDist, sumTime, count) =>\n",
    "      val avgDist = BigDecimal(sumDist / count).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "      val avgTime = BigDecimal(sumTime / count).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "      (avgDist, avgTime)\n",
    "  }.filter { case(_, (dist, time)) => dist > 0.0 && time > 0.0 }\n",
    "}\n",
    "\n",
    "val rddWithAvgPrices = calculateAvgPrices(rddForAvg, decimals)"
   ],
   "id": "6c3896dc99dbd31b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculateAvgPrices: (rdd: org.apache.spark.rdd.RDD[(String, (Double, Double, Long))], decimals: Int)org.apache.spark.rdd.RDD[(String, (Double, Double))]\n",
       "rddWithAvgPrices: org.apache.spark.rdd.RDD[(String, (Double, Double))] = MapPartitionsRDD[91] at filter at <console>:59\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "11) Join average prices to previous rdd",
   "id": "2b46c67228dd6352"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:30:58.173208Z",
     "start_time": "2025-06-19T12:29:50.058749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.broadcast.Broadcast\n",
    "\n",
    "val broadcastAvgPrices: Broadcast[Map[String, (Double, Double)]] = spark.sparkContext.broadcast(rddWithAvgPrices.collectAsMap().toMap)\n",
    "\n",
    "def applyJoin(rdd: RDD[(String, Row)], broadcastMap: Broadcast[Map[String, (Double, Double)]]): RDD[Row] = {\n",
    "  rdd.flatMap { case (key, originalRow) =>\n",
    "    broadcastMap.value.get(key).map { case(avgCostPerDistance, avgCostPerTime) =>\n",
    "      Row.fromSeq(originalRow.toSeq ++ Seq(avgCostPerDistance, avgCostPerTime))\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "val rddJoined = applyJoin(rddWithKey, broadcastAvgPrices)\n",
    "\n",
    "rddWithKey.unpersist()\n",
    "\n",
    "headers = headers ++ Seq(colAvgPricePerDistance, colAvgPricePerTime)"
   ],
   "id": "c7da5320936ec0fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.broadcast.Broadcast\n",
       "broadcastAvgPrices: org.apache.spark.broadcast.Broadcast[Map[String,(Double, Double)]] = Broadcast(20)\n",
       "applyJoin: (rdd: org.apache.spark.rdd.RDD[(String, org.apache.spark.sql.Row)], broadcastMap: org.apache.spark.broadcast.Broadcast[Map[String,(Double, Double)]])org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rddJoined: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[92] at flatMap at <console>:63\n",
       "headers: Seq[String] = ArraySeq(tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, ratecodeid, store_and_fwd_flag, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge, airport_fee, duration_minutes, year, duration_minutes_bin_...\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "12) Add price comparison w.r.t. average price and actual price difference",
   "id": "b97ec9d1cf2b66bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:31:00.222090Z",
     "start_time": "2025-06-19T12:30:58.204498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def addPriceComparison(rdd: RDD[Row], headers: Seq[String], colPriceDistance: String, colAvgPriceDistance: String, colPriceTime: String, colAvgPriceTime: String, decimals: Int) = {\n",
    "  rdd.map { row =>\n",
    "    val priceColsToAdd: Seq[Double] = Seq((colPriceDistance, colAvgPriceDistance), (colPriceTime, colAvgPriceTime))\n",
    "      .flatMap { case (colPrice, colAvgPrice) =>\n",
    "        val price = row.getAs[Double](headers.indexOf(colPrice))\n",
    "        val priceAvg = row.getAs[Double](headers.indexOf(colAvgPrice))\n",
    "        val priceDiff = BigDecimal(price - priceAvg).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "        val priceDiffPcg = BigDecimal(priceDiff / priceAvg * 100).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "\n",
    "        Seq(priceDiff, priceDiffPcg)\n",
    "      }\n",
    "    Row.fromSeq(row.toSeq ++ priceColsToAdd)\n",
    "  }\n",
    "}\n",
    "\n",
    "val rddPriceComparison = addPriceComparison(rddJoined, headers, colPricePerDistance, colAvgPricePerDistance, colPricePerTime, colAvgPricePerTime, decimals)\n",
    "headers = headers ++ Seq(colPricePerDistanceDiff, colPricePerDistanceDiffPcg, colPricePerTimeDiff, colPricePerTimeDiffPcg)"
   ],
   "id": "794982d2cb593fd2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addPriceComparison: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], headers: Seq[String], colPriceDistance: String, colAvgPriceDistance: String, colPriceTime: String, colAvgPriceTime: String, decimals: Int)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rddPriceComparison: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[93] at map at <console>:64\n",
       "headers: Seq[String] = ArraySeq(tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, ratecodeid, store_and_fwd_flag, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge, airport_fee, duration_minutes, year, duration_minutes_bin_label, overnight_duration, overnight_duration_pcg, regular_duration, regular_duration_...\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "13) Bin price difference per time and distance",
   "id": "988b3766e90e9789"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:31:00.405656Z",
     "start_time": "2025-06-19T12:31:00.227653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val rddPriceDistBin = binColByStepValue(rddPriceComparison,headers.indexOf(colPricePerDistanceDiffPcg), 5)\n",
    "val rddPriceDistTimeBin = binColByStepValue(rddPriceDistBin, headers.indexOf(colPricePerTimeDiffPcg), 5)\n",
    "\n",
    "headers = headers ++ Seq(colPricePerDistanceDiffPcgLabel, colPricePerTimeDiffPcgLabel)"
   ],
   "id": "d43c4991694d18f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddPriceDistBin: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[94] at map at <console>:56\n",
       "rddPriceDistTimeBin: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[95] at map at <console>:56\n",
       "headers: Seq[String] = ArraySeq(tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, ratecodeid, store_and_fwd_flag, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge, airport_fee, duration_minutes, year, duration_minutes_bin_label, overnight_duration, overnight_duration_pcg, regular_duration, regular_duration_pcg, weekday_surcharge, fees, agg_fee_bin_label, cost_per_time, cost_per_distance, distance_bin_label, overnight_duration_pcg_label, avg_cost_per_distan...\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "14) Reduce to analysis columns only",
   "id": "d55dcfcdf1b91809"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:31:13.355602Z",
     "start_time": "2025-06-19T12:31:00.411547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val headersForAnalysis = headers.zipWithIndex.filter(head => colsForClassification.contains(head._1.toLowerCase))\n",
    "\n",
    "val headersForAnalysisIdxs = headersForAnalysis.map(_._2)\n",
    "val headersForAnalysisCols = headersForAnalysis.map(_._1)\n",
    "\n",
    "def reduceToAnalysis(rdd: RDD[Row], idxs: Seq[Int]): RDD[Row] = {\n",
    "  rdd.map { row =>\n",
    "    Row.fromSeq(idxs.map(row.get))\n",
    "  }\n",
    "}\n",
    "\n",
    "val rddAnalysis = reduceToAnalysis(rddPriceDistTimeBin, headersForAnalysisIdxs)\n",
    "\n",
    "val totalCount = rddAnalysis.count()"
   ],
   "id": "c56a14f69be905a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headersForAnalysis: Seq[(String, Int)] = ArraySeq((passenger_count,2), (store_and_fwd_flag,5), (payment_type,6), (year,17), (duration_minutes_bin_label,18), (agg_fee_bin_label,25), (distance_bin_label,28), (overnight_duration_pcg_label,29), (cost_per_distance_diff_pcg_label,36), (cost_per_time_diff_pcg_label,37))\n",
       "headersForAnalysisIdxs: Seq[Int] = ArraySeq(2, 5, 6, 17, 18, 25, 28, 29, 36, 37)\n",
       "headersForAnalysisCols: Seq[String] = ArraySeq(passenger_count, store_and_fwd_flag, payment_type, year, duration_minutes_bin_label, agg_fee_bin_label, distance_bin_label, overnight_duration_pcg_label, cost_per_distance_diff_pcg_label, cost_per_time_diff_pcg_label)\n",
       "reduceToAnalysis: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], idxs: Seq[Int])org.apache.spark.rdd.RDD[org.apache.spark.sql...\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "15) Group by feature value",
   "id": "9f6cdebc4d24c9de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:31:14.066371Z",
     "start_time": "2025-06-19T12:31:13.361966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def groupByFeatures(rdd: RDD[Row], colForValuesAnalysis: Seq[String], colPriceDistanceDiffPcgLabel: String, colPriceTimeDiffPcgLabel: String, headersAnalysis: Seq[String], decimals: Int, totalCount: Long): Seq[RDD[Row]] = {\n",
    "  colForValuesAnalysis.map { colName =>\n",
    "    val groupCols = Seq(colPriceDistanceDiffPcgLabel, colPriceTimeDiffPcgLabel):+ colName\n",
    "    val grouped = rdd.map { row =>\n",
    "      val key = groupCols.map(col => row.get(headersAnalysis.indexOf(col.toLowerCase)))\n",
    "      (key, 1)\n",
    "    }.reduceByKey(_ + _).map { case (keySeq, count) =>\n",
    "      val value = keySeq.last.toString\n",
    "      val costDistLabel = keySeq(0).toString\n",
    "      val costTimeLabel = keySeq(1).toString\n",
    "      val pcg = BigDecimal(count.toDouble / totalCount * 100).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "      Row.fromSeq(Seq(colName, value, count, pcg, costDistLabel, costTimeLabel))\n",
    "    }\n",
    "    grouped\n",
    "  }\n",
    "}\n",
    "\n",
    "val rddFeatures = groupByFeatures(rddAnalysis, colsForValuesAnalysis, colPricePerDistanceDiffPcgLabel, colPricePerTimeDiffPcgLabel, headersForAnalysisCols, decimals, totalCount)"
   ],
   "id": "298184e06179fe88",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "groupByFeatures: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], colForValuesAnalysis: Seq[String], colPriceDistanceDiffPcgLabel: String, colPriceTimeDiffPcgLabel: String, headersAnalysis: Seq[String], decimals: Int, totalCount: Long)Seq[org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]]\n",
       "rddFeatures: Seq[org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]] = List(MapPartitionsRDD[99] at map at <console>:65, MapPartitionsRDD[102] at map at <console>:65, MapPartitionsRDD[105] at map at <console>:65, MapPartitionsRDD[108] at map at <console>:65, MapPartitionsRDD[111] at map at <console>:65, MapPartitionsRDD[114] at map at <console>:65, MapPartitionsRDD[117] at map at <console>:65, MapPartitionsRDD[120] at map at <console>:65)\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "16) Reduce to single rdd and write output",
   "id": "795fbb90ba3dc7a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:32:36.548259Z",
     "start_time": "2025-06-19T12:31:14.072728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val headersForSchema = Seq(\n",
    "  StructField(\"feature\", StringType),\n",
    "  StructField(\"value\", StringType),\n",
    "  StructField(\"count\", IntegerType),\n",
    "  StructField(\"pcg\", DoubleType),\n",
    "  StructField(\"cost_distance_label\", StringType),\n",
    "  StructField(\"cost_time_label\", StringType)\n",
    ")\n",
    "\n",
    "val schema = StructType(headersForSchema)\n",
    "\n",
    "val dfForAnalysis = spark.createDataFrame(rddFeatures.reduce(_ union _).coalesce(1), schema)\n",
    "\n",
    "dfForAnalysis.show(1)\n",
    "val endTime = System.currentTimeMillis()\n",
    "val durationMs = endTime - startTime\n",
    "\n",
    "println(s\"Job $name-dataset optimized executed in $durationMs ms\")\n",
    "\n",
    "dfForAnalysis.write.mode(\"overwrite\").parquet(getDatasetPath(outputDir + f\"/$name\"))"
   ],
   "id": "f147c54fa9f01d8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-----+---+-------------------+---------------+\n",
      "|        feature|value|count|pcg|cost_distance_label|cost_time_label|\n",
      "+---------------+-----+-----+---+-------------------+---------------+\n",
      "|passenger_count|  1.0|    2|0.0|          [565|570)|      [-40|-35)|\n",
      "+---------------+-----+-----+---+-------------------+---------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Job yellow-dataset optimized executed in 165740 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "headersForSchema: Seq[org.apache.spark.sql.types.StructField] = List(StructField(feature,StringType,true), StructField(value,StringType,true), StructField(count,IntegerType,true), StructField(pcg,DoubleType,true), StructField(cost_distance_label,StringType,true), StructField(cost_time_label,StringType,true))\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(feature,StringType,true),StructField(value,StringType,true),StructField(count,IntegerType,true),StructField(pcg,DoubleType,true),StructField(cost_distance_label,StringType,true),StructField(cost_time_label,StringType,true))\n",
       "dfForAnalysis: org.apache.spark.sql.DataFrame = [feature: string, value: string ... 4 more fields]\n",
       "endTime: Long = 1750336353175\n",
       "durationMs: Long = 165740\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
