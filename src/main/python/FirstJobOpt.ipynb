{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cefe988a9aff7538",
   "metadata": {},
   "source": [
    "# Big Data project A.Y. 2024-2025 - First Job\n",
    "\n",
    "## Members\n",
    "\n",
    "- Giovanni Antonioni\n",
    "- Luca Rubboli - 0001083742"
   ]
  },
  {
   "cell_type": "code",
   "id": "c9c769832012f1b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:08.555761Z",
     "start_time": "2025-07-01T08:45:06.446809Z"
    }
   },
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder\n",
    "  .appName(\"First job\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val sc = spark.sparkContext"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@71f673ee\n",
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@3147ddef\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Define useful parameters\n",
    "\n",
    "- Dataset location\n",
    "- Iterator (defined like this to overcome different names for same columns in dataset)"
   ],
   "id": "72ce3e263b662de6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:09.065739Z",
     "start_time": "2025-07-01T08:45:08.561086Z"
    }
   },
   "source": [
    "val decimals: Int = 4\n",
    "val minimumYearDataset = 2024\n",
    "val projectDir: String = \"/Users/luca/Desktop/Luca/Università/Magistrale/Corsi/BigData/Drivers\"\n",
    "val datasetDir = \"dataset\"\n",
    "val outputDir = \"output/firstJobOutputOpt\"\n",
    "val yellowDatasetDir = s\"$datasetDir/yellow_cab\"\n",
    "val greenDatasetDir = s\"$datasetDir/green_cab\"\n",
    "val fhvDatasetDir = s\"$datasetDir/fhv_cab\"\n",
    "val fhvhvDatasetDir = s\"$datasetDir/fhvhv_cab\"\n",
    "val datasetDirMap: Map[String, String] = Map(\"yellow\" -> yellowDatasetDir, \"green\" -> greenDatasetDir, \"fhv\" -> fhvDatasetDir, \"fhvhv\" -> fhvhvDatasetDir)\n",
    "val datasetIterator: Map[String, (String, String)] = Map(\n",
    "  \"yellow\" -> (\"tpep_dropoff_datetime\", \"tpep_pickup_datetime\"),\n",
    "  \"green\" -> (\"lpep_dropoff_datetime\", \"lpep_pickup_datetime\"),\n",
    "  // (\"fhv\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\"),\n",
    "  // (\"fhvhv\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\"),\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decimals: Int = 4\n",
       "minimumYearDataset: Int = 2024\n",
       "datasetDir: String = dataset\n",
       "outputDir: String = output/firstJobOutputOpt\n",
       "yellowDatasetDir: String = dataset/yellow_cab\n",
       "greenDatasetDir: String = dataset/green_cab\n",
       "fhvDatasetDir: String = dataset/fhv_cab\n",
       "fhvhvDatasetDir: String = dataset/fhvhv_cab\n",
       "datasetDirMap: Map[String,String] = Map(yellow -> dataset/yellow_cab, green -> dataset/green_cab, fhv -> dataset/fhv_cab, fhvhv -> dataset/fhvhv_cab)\n",
       "datasetIterator: Map[String,(String, String)] = Map(yellow -> (tpep_dropoff_datetime,tpep_pickup_datetime), green -> (lpep_dropoff_datetime,lpep_pickup_datetime))\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "801ff1e945bd4c8",
   "metadata": {},
   "source": [
    "## Define Columns for analysis\n",
    "- Columns names\n",
    "- Time zones for overprice\n",
    "- Columns used in classification for average price calculation\n",
    "- Columns which values are used in analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "bfdb3f87a1c6a6c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:09.390131Z",
     "start_time": "2025-07-01T08:45:09.072037Z"
    }
   },
   "source": [
    "val colDurationMinutes: String = \"duration_minutes\"\n",
    "val colDurationMinutesBinLabel: String = \"duration_minutes_bin_label\"\n",
    "val colYear: String = \"year\"\n",
    "val colWeekdaySurcharge: String = \"weekday_surcharge\"\n",
    "val colAggregateFee: String = \"fees\"\n",
    "val colAggregateFeeBin: String = \"agg_fee_bin_label\"\n",
    "val colDistanceBin: String = \"distance_bin_label\"\n",
    "val colFareAmount: String = \"fare_amount\"\n",
    "val colPricePerDistance: String = \"cost_per_distance\"\n",
    "val colPricePerTime: String = \"cost_per_time\"\n",
    "val colAvgPricePerDistance: String = \"avg_cost_per_distance\"\n",
    "val colAvgPricePerTime: String = \"avg_cost_per_time\"\n",
    "val colPricePerDistanceDiff: String = \"cost_per_distance_diff\"\n",
    "val colPricePerDistanceDiffPcg: String = \"cost_per_distance_diff_pcg\"\n",
    "val colPricePerTimeDiff: String = \"cost_per_time_diff\"\n",
    "val colPricePerTimeDiffPcg: String = \"cost_per_time_diff_pcg\"\n",
    "val colPricePerDistanceDiffPcgLabel: String = colPricePerDistanceDiffPcg + \"_label\"\n",
    "val colPricePerTimeDiffPcgLabel: String = colPricePerTimeDiffPcg + \"_label\"\n",
    "\n",
    "val timeZoneOver: String = \"overnight\"\n",
    "val timeZones = Map(timeZoneOver -> (20, 6), \"regular\" -> (6, 20))\n",
    "val weekDaySurcharge: Double = 2.5\n",
    "\n",
    "val colDurationOvernightPcg: String = s\"${timeZoneOver}_duration_pcg\"\n",
    "\n",
    "val colToUse: Set[String] = Set(\n",
    "  \"tpep_pickup_datetime\",\n",
    "  \"tpep_dropoff_datetime\",\n",
    "  \"lpep_pickup_datetime\",\n",
    "  \"lpep_dropoff_datetime\",\n",
    "  \"passenger_count\",\n",
    "  \"trip_distance\",\n",
    "  \"ratecodeid\",\n",
    "  \"store_and_fwd_flag\",\n",
    "  \"payment_type\",\n",
    "  \"fare_amount\",\n",
    "  \"extra\",\n",
    "  \"mta_tax\",\n",
    "  \"tip_amount\",\n",
    "  \"tolls_amount\",\n",
    "  \"improvement_surcharge\",\n",
    "  \"total_amount\",\n",
    "  \"congestion_surcharge\",\n",
    "  \"airport_fee\")\n",
    "\n",
    "val colFees: Set[String] = Set(\n",
    "  \"extra\",\n",
    "  \"mta_tax\",\n",
    "  \"improvement_surcharge\",\n",
    "  \"congestion_surcharge\",\n",
    "  \"airport_fee\")\n",
    "\n",
    "val colsForClassification: Seq[String] = Seq(\n",
    "  \"passenger_count\",\n",
    "  \"store_and_fwd_flag\",\n",
    "  \"payment_type\",\n",
    "  colAggregateFeeBin,\n",
    "  colDurationMinutesBinLabel,\n",
    "  colDistanceBin,\n",
    "  colYear,\n",
    "  s\"${colDurationOvernightPcg}_label\",\n",
    "  colPricePerDistanceDiffPcgLabel,\n",
    "  colPricePerTimeDiffPcgLabel\n",
    ")\n",
    "\n",
    "val colsForValuesAnalysis: Seq[String] = Seq(\n",
    "  \"passenger_count\",\n",
    "  \"store_and_fwd_flag\",\n",
    "  \"payment_type\",\n",
    "  colAggregateFeeBin,\n",
    "  colDurationMinutesBinLabel,\n",
    "  colDistanceBin,\n",
    "  colYear,\n",
    "  s\"${colDurationOvernightPcg}_label\",\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colDurationMinutes: String = duration_minutes\n",
       "colDurationMinutesBinLabel: String = duration_minutes_bin_label\n",
       "colYear: String = year\n",
       "colWeekdaySurcharge: String = weekday_surcharge\n",
       "colAggregateFee: String = fees\n",
       "colAggregateFeeBin: String = agg_fee_bin_label\n",
       "colDistanceBin: String = distance_bin_label\n",
       "colFareAmount: String = fare_amount\n",
       "colPricePerDistance: String = cost_per_distance\n",
       "colPricePerTime: String = cost_per_time\n",
       "colAvgPricePerDistance: String = avg_cost_per_distance\n",
       "colAvgPricePerTime: String = avg_cost_per_time\n",
       "colPricePerDistanceDiff: String = cost_per_distance_diff\n",
       "colPricePerDistanceDiffPcg: String = cost_per_distance_diff_pcg\n",
       "colPricePerTimeDiff: String = cost_per_time_diff\n",
       "colPricePerTimeDiffPcg: String = cost_per_time_diff_pcg\n",
       "colPricePerDistanceDiffPcgLabel: String = ...\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "6251e1a01e2d34a6",
   "metadata": {},
   "source": [
    "### Define preprocess rules"
   ]
  },
  {
   "cell_type": "code",
   "id": "f3c66b94d590de92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:09.936123Z",
     "start_time": "2025-07-01T08:45:09.394229Z"
    }
   },
   "source": [
    "import java.time.LocalDateTime\n",
    "\n",
    "val featureFilters: Map[String, Any => Boolean] = Map(\n",
    "  \"passenger_count\" -> {\n",
    "    case i: Int => i > 0\n",
    "    case f: Float => val i = f.toInt; i > 0\n",
    "    case d: Double => val i = d.toInt; i > 0\n",
    "    case _ => false\n",
    "  },\n",
    "  \"trip_distance\" -> {\n",
    "    case i: Int => i > 0\n",
    "    case i: Float => i > 0\n",
    "    case i: Double => i > 0\n",
    "    case _ => false\n",
    "  },\n",
    "  \"ratecodeid\" -> {\n",
    "    case i: Int => (i >= 1 && i <= 6) || i == 99\n",
    "    case f: Float => val i = f.toInt; (i >= 1 && i <= 6) || i == 99\n",
    "    case d: Double => val i = d.toInt; (i >= 1 && i <= 6) || i == 99\n",
    "    case _ => false\n",
    "  },\n",
    "  \"store_and_fwd_flag\" -> {\n",
    "    case i: String => i == \"Y\" || i == \"N\"\n",
    "    case _ => false\n",
    "  },\n",
    "  \"payment_type\" -> {\n",
    "    case i: Int => i >= 1 && i <= 6\n",
    "    case f: Float => val i = f.toInt; i >= 1 && i <= 6\n",
    "    case d: Double => val i = d.toInt; i >= 1 && i <= 6\n",
    "    case _ => false\n",
    "  },\n",
    "  \"fare_amount\" -> {\n",
    "    case i: Int => i > 0\n",
    "    case i: Float => i > 0\n",
    "    case i: Double => i > 0\n",
    "    case _ => false\n",
    "  },\n",
    "  \"tolls_amount\" -> {\n",
    "    case i: Int => i >= 0 && i < 200\n",
    "    case i: Float => i >= 0 && i < 200\n",
    "    case i: Double => i >= 0 && i < 200\n",
    "    case _ => false\n",
    "  }\n",
    ")\n",
    "\n",
    "val taxFilter: Any => Boolean = {\n",
    "  case tax: Int => tax >= 0 && tax < 20\n",
    "  case tax: Float => tax >= 0 && tax < 20\n",
    "  case tax: Double => tax >= 0 && tax < 20\n",
    "  case _ => false\n",
    "}\n",
    "\n",
    "val dateFilter: (Any, Int) => Boolean = {\n",
    "  case (date: LocalDateTime, minimumYearDataset: Int) => val year: Int = date.getYear; year >= minimumYearDataset && year <= LocalDateTime.now().getYear\n",
    "  case _ => false\n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.LocalDateTime\n",
       "featureFilters: Map[String,Any => Boolean] = Map(trip_distance -> $Lambda$5441/0x0000000801c56840@69b42713, tolls_amount -> $Lambda$5446/0x0000000801c61040@347e67e5, payment_type -> $Lambda$5444/0x0000000801c54040@6d871dcc, fare_amount -> $Lambda$5445/0x0000000801c60040@332e40c, passenger_count -> $Lambda$5440/0x0000000801c57040@4bf6c7c6, store_and_fwd_flag -> $Lambda$5443/0x0000000801c55040@cf0d012, ratecodeid -> $Lambda$5442/0x0000000801c55840@594cd53d)\n",
       "taxFilter: Any => Boolean = $Lambda$5447/0x0000000801c61840@4d4a489c\n",
       "dateFilter: (Any, Int) => Boolean = $Lambda$5448/0x0000000801c62840@30ff7e04\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "46150dfaa1006964",
   "metadata": {},
   "source": [
    "### Utils functions for rdd"
   ]
  },
  {
   "cell_type": "code",
   "id": "9aae38423ac3200c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:11.111200Z",
     "start_time": "2025-07-01T08:45:09.940829Z"
    }
   },
   "source": [
    "import java.time.temporal.ChronoUnit\n",
    "import java.time.{DayOfWeek, LocalDate}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Row\n",
    "import scala.math.BigDecimal.RoundingMode\n",
    "\n",
    "def getDatasetPath(localPath: String): String = {\n",
    "  \"file://\" + projectDir + \"/\" + localPath\n",
    "}\n",
    "\n",
    "def binColByStepValue(rdd: RDD[Row], indexOfColToDiscrete: Int, stepValue: Int = 5): RDD[Row] = {\n",
    "  rdd\n",
    "    .map { row =>\n",
    "      val value: Double = row.get(indexOfColToDiscrete) match {\n",
    "        case i: Int => i.toDouble\n",
    "        case d: Double => d\n",
    "        case l: Long => l.toDouble\n",
    "        case s: String => try { s.toDouble } catch { case _: Throwable => Double.NaN}\n",
    "        case _ => Double.NaN\n",
    "      }\n",
    "\n",
    "      val rawBin = (value / stepValue).toInt * stepValue\n",
    "      val binBase = if (value < 0 && value % stepValue == 0) rawBin + stepValue else rawBin\n",
    "      val label = if (value < 0) { s\"[${(binBase - stepValue).toInt}|${binBase.toInt})\" } else { s\"[${binBase.toInt}|${(binBase + stepValue).toInt})\" }\n",
    "\n",
    "      Row.fromSeq(row.toSeq :+ label)\n",
    "    }\n",
    "}\n",
    "\n",
    "val castForFilter: Any => Any = {\n",
    "  case s: String => if (s.matches(\"\"\"^-?\\d+\\.\\d+$\"\"\")) s.toDouble else if (s.matches(\"\"\"^-?\\d+$\"\"\")) s.toInt else s.trim\n",
    "  case d: Double => d\n",
    "  case i: Int => i\n",
    "  case l: Long => l.toDouble\n",
    "  case f: Float => f.toDouble\n",
    "  case b: Boolean => b\n",
    "  case null => null\n",
    "  case other => other.toString.trim\n",
    "}\n",
    "\n",
    "val preciseBucketUDF: (Map[String, (Int, Int)], LocalDateTime, LocalDateTime, Int) => Map[String, Double] = { (timeZones: Map[String, (Int, Int)], start: LocalDateTime, end: LocalDateTime, decimals: Int) =>\n",
    "\n",
    "  val overlap: (LocalDateTime, LocalDateTime, LocalDateTime, LocalDateTime, Int) => Double = { (start1: LocalDateTime, end1: LocalDateTime, start2: LocalDateTime, end2: LocalDateTime, decimals: Int) =>\n",
    "    val overlapStart = if (start1.isAfter(start2)) start1 else start2\n",
    "    val overlapEnd = if (end1.isBefore(end2)) end1 else end2\n",
    "    if (overlapEnd.isAfter(overlapStart)) BigDecimal(ChronoUnit.MILLIS.between(overlapStart, overlapEnd) / 60000.0).setScale(decimals, RoundingMode.HALF_UP).toDouble else 0.0\n",
    "  }\n",
    "\n",
    "  var result = timeZones.keys.map(_ -> 0.0).toMap\n",
    "\n",
    "  if (!(start == null || end == null)) {\n",
    "\n",
    "    if (!end.isBefore(start)) {\n",
    "\n",
    "      var current = start.toLocalDate.atStartOfDay\n",
    "\n",
    "      while (!current.isAfter(end)) {\n",
    "        val nextDay = current.plusDays(1)\n",
    "\n",
    "        timeZones\n",
    "          .foreach {\n",
    "            case (label, (startHour, endHour)) if startHour > endHour => {\n",
    "              val bucketStartBeforeMidnight = current.withHour(startHour).withMinute(0).withSecond(0).withNano(0)\n",
    "              val bucketEndBeforeMidnight = current.withHour(23).withMinute(59).withSecond(59)\n",
    "              val bucketStartAfterMidnight = current.withHour(0).withMinute(0).withSecond(0).withNano(0)\n",
    "              val bucketEndAfterMidnight = current.withHour(endHour).withMinute(0).withSecond(0).withNano(0)\n",
    "\n",
    "              val minutesBeforeMidnight = overlap(start, end, bucketStartBeforeMidnight, bucketEndBeforeMidnight, decimals)\n",
    "              val minutesAfterMidnight = overlap(start, end, bucketStartAfterMidnight, bucketEndAfterMidnight, decimals)\n",
    "\n",
    "              result = result.updated(label, result(label) + minutesBeforeMidnight + minutesAfterMidnight)\n",
    "            }\n",
    "            case (label, (startHour, endHour)) => {\n",
    "              val bucketStart = current.withHour(startHour).withMinute(0).withSecond(0).withNano(0)\n",
    "              val bucketEnd = if (endHour == 24) current.plusDays(1).withHour(0).withMinute(0).withSecond(0).withNano(0) else current.withHour(endHour).withMinute(0).withSecond(0).withNano(0)\n",
    "\n",
    "              val minutes = overlap(start, end, bucketStart, bucketEnd, decimals)\n",
    "\n",
    "              result = result.updated(label, result(label) + minutes)\n",
    "            }\n",
    "          }\n",
    "\n",
    "        current = nextDay\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  result\n",
    "}\n",
    "\n",
    "val isUSHolidayOrWeekend: LocalDate => Boolean = { date =>\n",
    "  val month = date.getMonthValue\n",
    "  val day = date.getDayOfMonth\n",
    "  val dayOfWeek = date.getDayOfWeek\n",
    "\n",
    "  val isIndependenceDay = month == 7 && day == 4\n",
    "  val isChristmas = month == 12 && day == 25\n",
    "  val isNewYear = month == 1 && day == 1\n",
    "  val isLaborDay = month == 9 && dayOfWeek == DayOfWeek.MONDAY && day <= 7\n",
    "\n",
    "  val isThanksgiving = month == 11 && dayOfWeek == DayOfWeek.THURSDAY && day >= 22 && day <= 28 && ((day - 1) / 7 + 1 == 4)\n",
    "\n",
    "  isIndependenceDay || isChristmas || isNewYear || isLaborDay || isThanksgiving || dayOfWeek == DayOfWeek.SATURDAY || dayOfWeek == DayOfWeek.SUNDAY\n",
    "}\n",
    "\n",
    "val selectColumns: (RDD[Row], Seq[String], Set[String]) => RDD[Row] = { (rdd, headers, columnsToKeep) =>\n",
    "  val keepIndexes = headers.zipWithIndex.collect {\n",
    "    case (col, idx) if columnsToKeep.contains(col) => idx\n",
    "  }\n",
    "\n",
    "  rdd\n",
    "    .map { row =>\n",
    "      val selectedValues = keepIndexes.map(row.get)\n",
    "      Row.fromSeq(selectedValues)\n",
    "    }\n",
    "}\n",
    "\n",
    "val removeColumns: (RDD[Row], Seq[String], Set[String]) => RDD[Row] = { (rdd, headers, columnsToRemove) =>\n",
    "  val lowerHeaders = headers.map(_.toLowerCase)\n",
    "  val removeSet = columnsToRemove.map(_.toLowerCase)\n",
    "\n",
    "  val keepIndexes = lowerHeaders.zipWithIndex.collect {\n",
    "    case (col, idx) if !removeSet.contains(col) => idx\n",
    "  }\n",
    "\n",
    "  rdd\n",
    "    .map { row =>\n",
    "      val selectedValues = keepIndexes.map(row.get)\n",
    "      Row.fromSeq(selectedValues)\n",
    "    }\n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.temporal.ChronoUnit\n",
       "import java.time.{DayOfWeek, LocalDate}\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.sql.Row\n",
       "import scala.math.BigDecimal.RoundingMode\n",
       "projectDir: String = /Users/luca/Desktop/Luca/Università/Magistrale/Corsi/BigData/Drivers\n",
       "getDatasetPath: (localPath: String)String\n",
       "binColByStepValue: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], indexOfColToDiscrete: Int, stepValue: Int)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "castForFilter: Any => Any = $Lambda$5449/0x0000000801c63040@1870a20c\n",
       "preciseBucketUDF: (Map[String,(Int, Int)], java.time.LocalDateTime, java.time.LocalDateTime, Int) => Map[String,Double] = $Lambda$5450/0x0000000801c63840@3a73e22f\n",
       "isUSHolidayOrWeekend: java.time.LocalDate => Boolean = $Lambda$5451/0x0000000801...\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "d67c5a94825ecee9",
   "metadata": {},
   "source": [
    "# Actual job\n",
    "\n",
    "1) Select dataset [yellow or green]"
   ]
  },
  {
   "cell_type": "code",
   "id": "60aee06709329346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:11.393375Z",
     "start_time": "2025-07-01T08:45:11.115554Z"
    }
   },
   "source": [
    "val name: String = \"green\"\n",
    "val (dropoff, pickup) = datasetIterator(name)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: String = green\n",
       "dropoff: String = lpep_dropoff_datetime\n",
       "pickup: String = lpep_pickup_datetime\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2) Load dataset",
   "id": "6e3fcaeb36518cd9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:12.140226Z",
     "start_time": "2025-07-01T08:45:11.399260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val startTime = System.currentTimeMillis()\n",
    "\n",
    "val dataset = spark.read.parquet(getDatasetPath(datasetDirMap(name)))\n",
    "var headers: Seq[String] = dataset.columns.map(_.toLowerCase)\n",
    "val indexesToUse: Seq[Int] = headers.zipWithIndex.collect {\n",
    "  case (h, i) if colToUse.contains(h.toLowerCase) => i\n",
    "}\n",
    "headers = headers.filter(head => colToUse.contains(head.toLowerCase))"
   ],
   "id": "a846e646fac3bac4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startTime: Long = 1751359511796\n",
       "dataset: org.apache.spark.sql.DataFrame = [VendorID: int, lpep_pickup_datetime: timestamp_ntz ... 18 more fields]\n",
       "headers: Seq[String] = ArraySeq(lpep_pickup_datetime, lpep_dropoff_datetime, store_and_fwd_flag, ratecodeid, passenger_count, trip_distance, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, payment_type, congestion_surcharge)\n",
       "indexesToUse: Seq[Int] = ArraySeq(1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 19)\n",
       "headers: Seq[String] = ArraySeq(lpep_pickup_datetime, lpep_dropoff_datetime, store_and_fwd_flag, ratecodeid, passenger_count, trip_distance, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, payment_type, congestion_surcharge)\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3) Filter taxes and features based on filter conditions previously defined",
   "id": "df209b8e24cd13e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:12.629948Z",
     "start_time": "2025-07-01T08:45:12.144457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import java.time.format.DateTimeFormatter\n",
    "\n",
    "def transformRDD(dataset: DataFrame, idxs: Seq[Int], castFunc: Any => Any): RDD[Row] = {\n",
    "  dataset.rdd.map(row => Row.fromSeq(idxs.map(row.get).map(castFunc)))\n",
    "}\n",
    "\n",
    "val rdd = transformRDD(dataset, indexesToUse, castForFilter)\n",
    "\n",
    "def applyFilters(rdd: RDD[Row], headers: Seq[String], colOfFees: Set[String], taxFilter: Any => Boolean, featFilter: Map[String, Any => Boolean], dateFilter: (Any, Int) => Boolean, dropoff: String, pickup: String, minimumYearDataset: Int): RDD[Row] = {\n",
    "  rdd\n",
    "    .filter { row =>\n",
    "      val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm[:ss]\")\n",
    "      headers.zip(row.toSeq).forall { case (header: String, value) =>\n",
    "        val taxFilterCondition = if (colOfFees.contains(header.toLowerCase)) taxFilter(value) else true\n",
    "\n",
    "        featFilter.get(header.toLowerCase) match {\n",
    "          case Some(filterFunc) => taxFilterCondition && filterFunc(value)\n",
    "          case None => if (header.equals(pickup) || header.equals(dropoff)) { dateFilter(LocalDateTime.parse(row.getAs[String](headers.indexOf(header)).trim, formatter), minimumYearDataset) && taxFilterCondition } else taxFilterCondition\n",
    "        }\n",
    "\n",
    "      }\n",
    "    }\n",
    "}\n",
    "\n",
    "val rddFiltered = applyFilters(rdd, headers, colFees, taxFilter, featureFilters, dateFilter, dropoff, pickup, minimumYearDataset)"
   ],
   "id": "f8a8a0eaeaa2d294",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.storage.StorageLevel\n",
       "import java.time.format.DateTimeFormatter\n",
       "transformRDD: (dataset: org.apache.spark.sql.DataFrame, idxs: Seq[Int], castFunc: Any => Any)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[81] at map at <console>:65\n",
       "applyFilters: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], headers: Seq[String], colOfFees: Set[String], taxFilter: Any => Boolean, featFilter: Map[String,Any => Boolean], dateFilter: (Any, Int) => Boolean, dropoff: String, pickup: String, minimumYearDataset: Int)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rddFiltered: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[82] at filter a...\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4) Add duration and timezones",
   "id": "523a6cb64b4c03d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:12.986818Z",
     "start_time": "2025-07-01T08:45:12.633952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import java.time.Duration\n",
    "\n",
    "def addDuration(rdd: RDD[Row], headers: Seq[String], pickup: String, dropoff: String, decimals: Int): RDD[Row] = {\n",
    "  rdd\n",
    "    .map {row =>\n",
    "      val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm[:ss]\")\n",
    "\n",
    "      val pickupStr = row.getAs[String](headers.indexOf(pickup)).trim\n",
    "      val dropoffStr = row.getAs[String](headers.indexOf(dropoff)).trim\n",
    "\n",
    "      val pickupTS = LocalDateTime.parse(pickupStr, formatter)\n",
    "      val dropoffTS = LocalDateTime.parse(dropoffStr, formatter)\n",
    "      val durationMillis = Duration.between(pickupTS, dropoffTS).toMillis\n",
    "      val durationMinutes = BigDecimal(durationMillis / 60000.0).setScale(decimals, RoundingMode.HALF_UP).toDouble\n",
    "\n",
    "      val pickupYear = pickupTS.getYear\n",
    "\n",
    "      Row.fromSeq(row.toSeq ++ Seq(durationMinutes, pickupYear))\n",
    "    }\n",
    "    .filter {\n",
    "      row => row.getAs[Double](row.toSeq.length - 2) > 0.0\n",
    "    }\n",
    "}\n",
    "\n",
    "val rddDuration = addDuration(rddFiltered, headers, pickup, dropoff, decimals)\n",
    "headers = headers ++ Seq(colDurationMinutes, colYear)\n",
    "\n",
    "val rddDurationBin = binColByStepValue(rddDuration, headers.indexOf(colDurationMinutes), 5)\n",
    "headers = headers :+ colDurationMinutesBinLabel\n",
    "\n",
    "def addTimeZones(rdd: RDD[Row], headers: Seq[String], timezones: Map[String, (Int, Int)], weekDaySurcharge: Double, colDuration: String, pickup: String, dropoff: String, decimals: Int, preciseBucketUDF: (Map[String, (Int, Int)], LocalDateTime, LocalDateTime, Int) => Map[String, Double], isUSHolidayOrWeekendTZ: LocalDate => Boolean): RDD[Row] = {\n",
    "  rdd\n",
    "    .map { row =>\n",
    "      val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm[:ss]\")\n",
    "\n",
    "      val timeZonesDuration: Map[String, Double] = preciseBucketUDF(timezones, LocalDateTime.parse(row.getAs[String](headers.indexOf(pickup)).trim, formatter), LocalDateTime.parse(row.getAs[String](headers.indexOf(dropoff)).trim, formatter), decimals)\n",
    "\n",
    "      val weekday_surcharge: Double = if (isUSHolidayOrWeekendTZ(LocalDateTime.parse(row.getAs[String](headers.indexOf(pickup)).trim, formatter).toLocalDate)) 0 else weekDaySurcharge\n",
    "      val colsToAdd: Seq[Double] = timezones.keys.toSeq.flatMap { tz =>\n",
    "        val duration = timeZonesDuration.getOrElse(tz, 0.0)\n",
    "        val totalDuration = row.getAs[Double](headers.indexOf(colDuration))\n",
    "        Seq(duration, BigDecimal(duration * 100 / totalDuration).setScale(decimals, RoundingMode.HALF_UP).toDouble)\n",
    "      }\n",
    "      Row.fromSeq((row.toSeq ++ colsToAdd) :+ weekday_surcharge)\n",
    "    }\n",
    "}\n",
    "\n",
    "val rddTimeZones = addTimeZones(rddDurationBin, headers, timeZones, weekDaySurcharge, colDurationMinutes, pickup, dropoff, decimals, preciseBucketUDF, isUSHolidayOrWeekend)\n",
    "\n",
    "val headersToAdd: Seq[String] = timeZones.keys.toSeq.flatMap { tz =>\n",
    "  Seq(tz + \"_duration\", tz + \"_duration_pcg\")\n",
    "} :+ colWeekdaySurcharge\n",
    "\n",
    "headers = headers ++ headersToAdd"
   ],
   "id": "9f3672bebd356114",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.Duration\n",
       "addDuration: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], headers: Seq[String], pickup: String, dropoff: String, decimals: Int)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rddDuration: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[84] at filter at <console>:89\n",
       "headers: Seq[String] = ArraySeq(lpep_pickup_datetime, lpep_dropoff_datetime, store_and_fwd_flag, ratecodeid, passenger_count, trip_distance, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, payment_type, congestion_surcharge, duration_minutes, year, duration_minutes_bin_label, overnight_duration, overnight_duration_pcg, regular_duration, regular_duration_pcg, weekday_surcharge)\n",
       "rddDurationBin: org.apache.spark.rdd.RDD[or...\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:13.219299Z",
     "start_time": "2025-07-01T08:45:12.991017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val colToRemoveTimeZones = Set(pickup, dropoff, \"overnight_duration\", \"regular_duration\", \"regular_duration_pcg\", \"ratecodeid\", \"tip_amount\", \"tolls_amount\", \"total_amount\")\n",
    "\n",
    "val rddTimeZonesOpt = removeColumns(rddTimeZones, headers, colToRemoveTimeZones)\n",
    "var headersTimeZonesOpt = headers.filterNot(col => colToRemoveTimeZones.contains(col.toLowerCase))"
   ],
   "id": "a03dd43acdc10696",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colToRemoveTimeZones: scala.collection.immutable.Set[String] = Set(lpep_dropoff_datetime, regular_duration, overnight_duration, regular_duration_pcg, tolls_amount, tip_amount, lpep_pickup_datetime, total_amount, ratecodeid)\n",
       "rddTimeZonesOpt: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[87] at map at <console>:173\n",
       "headersTimeZonesOpt: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, trip_distance, fare_amount, extra, mta_tax, improvement_surcharge, payment_type, congestion_surcharge, duration_minutes, year, duration_minutes_bin_label, overnight_duration_pcg, weekday_surcharge)\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5) Add Aggregate fees and bins",
   "id": "135d87a329e10208"
  },
  {
   "cell_type": "code",
   "id": "cfb99aef020692d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:13.523586Z",
     "start_time": "2025-07-01T08:45:13.223242Z"
    }
   },
   "source": [
    "def addAggregateFees(rdd: RDD[Row], headers: Seq[String], colOfFees: Set[String]): RDD[Row] = {\n",
    "  rdd\n",
    "    .map { row =>\n",
    "      val fees = colOfFees\n",
    "        .filter(col => headers.contains(col.toLowerCase))\n",
    "        .map(col => row.getAs[Double](headers.indexOf(col.toLowerCase))).sum\n",
    "\n",
    "      Row.fromSeq(row.toSeq :+ fees)\n",
    "    }\n",
    "}\n",
    "\n",
    "val rddAggFees = addAggregateFees(rddTimeZonesOpt, headersTimeZonesOpt, colFees)\n",
    "headersTimeZonesOpt = headersTimeZonesOpt :+ colAggregateFee\n",
    "\n",
    "val rddAggFeesBin = binColByStepValue(rddAggFees, headersTimeZonesOpt.indexOf(colAggregateFee), 2)\n",
    "headersTimeZonesOpt = headersTimeZonesOpt :+ colAggregateFeeBin"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addAggregateFees: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], headers: Seq[String], colOfFees: Set[String])org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rddAggFees: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[88] at map at <console>:62\n",
       "headersTimeZonesOpt: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, trip_distance, fare_amount, extra, mta_tax, improvement_surcharge, payment_type, congestion_surcharge, duration_minutes, year, duration_minutes_bin_label, overnight_duration_pcg, weekday_surcharge, fees, agg_fee_bin_label)\n",
       "rddAggFeesBin: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[89] at map at <console>:60\n",
       "headersTimeZonesOpt: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, trip_distance, fare...\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:13.751069Z",
     "start_time": "2025-07-01T08:45:13.527436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val colToRemoveAggFees = colFees ++ Set(colAggregateFee)\n",
    "\n",
    "val rddAggFeesOpt = removeColumns(rddAggFeesBin, headersTimeZonesOpt, colToRemoveAggFees)\n",
    "var headersAggFeesOpt = headersTimeZonesOpt.filterNot(col => colToRemoveAggFees.contains(col.toLowerCase))"
   ],
   "id": "e6282efb09847386",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colToRemoveAggFees: scala.collection.immutable.Set[String] = Set(improvement_surcharge, fees, extra, airport_fee, congestion_surcharge, mta_tax)\n",
       "rddAggFeesOpt: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[90] at map at <console>:173\n",
       "headersAggFeesOpt: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, trip_distance, fare_amount, payment_type, duration_minutes, year, duration_minutes_bin_label, overnight_duration_pcg, weekday_surcharge, agg_fee_bin_label)\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6) Add price per mile and minute",
   "id": "2612d914aaba250d"
  },
  {
   "cell_type": "code",
   "id": "7a05a60d734daf56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:14.025252Z",
     "start_time": "2025-07-01T08:45:13.755168Z"
    }
   },
   "source": [
    "def addPricePerDistanceAndTime(rdd: RDD[Row], headers: Seq[String], colFareAmount: String, colDuration: String, colDistance: String): RDD[Row] = {\n",
    "  rdd\n",
    "    .map { row =>\n",
    "      val pricePerTime = Math.round(row.getAs[Double](headers.indexOf(colFareAmount)) / row.getAs[Double](headers.indexOf(colDuration)) * 100) / 100.0\n",
    "      val pricePerDistance = Math.round(row.getAs[Double](headers.indexOf(colFareAmount)) / row.getAs[Double](headers.indexOf(colDistance)) * 100) / 100.0\n",
    "\n",
    "      Row.fromSeq(row.toSeq ++ Seq(pricePerTime, pricePerDistance))\n",
    "    }\n",
    "}\n",
    "\n",
    "val rddPriced = addPricePerDistanceAndTime(rddAggFeesOpt, headersAggFeesOpt, colFareAmount, colDurationMinutes, \"trip_distance\")\n",
    "headersAggFeesOpt = headersAggFeesOpt ++ Seq(colPricePerTime, colPricePerDistance)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addPricePerDistanceAndTime: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], headers: Seq[String], colFareAmount: String, colDuration: String, colDistance: String)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rddPriced: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[91] at map at <console>:61\n",
       "headersAggFeesOpt: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, trip_distance, fare_amount, payment_type, duration_minutes, year, duration_minutes_bin_label, overnight_duration_pcg, weekday_surcharge, agg_fee_bin_label, cost_per_time, cost_per_distance)\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:14.502742Z",
     "start_time": "2025-07-01T08:45:14.039218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val colToRemovePricePerDistanceAndTime = Set(colFareAmount, colDurationMinutes)\n",
    "\n",
    "val rddPricePerDistanceAndTimeOpt = removeColumns(rddPriced, headersAggFeesOpt, colToRemovePricePerDistanceAndTime)\n",
    "var headersPricePerDistanceAndTimeOpt = headersAggFeesOpt.filterNot(col => colToRemovePricePerDistanceAndTime.contains(col.toLowerCase))"
   ],
   "id": "249ca87bec9ec878",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colToRemovePricePerDistanceAndTime: scala.collection.immutable.Set[String] = Set(fare_amount, duration_minutes)\n",
       "rddPricePerDistanceAndTimeOpt: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[92] at map at <console>:173\n",
       "headersPricePerDistanceAndTimeOpt: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, trip_distance, payment_type, year, duration_minutes_bin_label, overnight_duration_pcg, weekday_surcharge, agg_fee_bin_label, cost_per_time, cost_per_distance)\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "7) Add distance bin and duration in overnight time zone",
   "id": "86d18a3f7203c64e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:14.735732Z",
     "start_time": "2025-07-01T08:45:14.506691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val rddDistBin = binColByStepValue(rddPricePerDistanceAndTimeOpt, headersPricePerDistanceAndTimeOpt.indexOf(\"trip_distance\"), 5)\n",
    "headersPricePerDistanceAndTimeOpt = headersPricePerDistanceAndTimeOpt :+ colDistanceBin\n",
    "\n",
    "val rddOvernightBin = binColByStepValue(rddDistBin, headersPricePerDistanceAndTimeOpt.indexOf(colDurationOvernightPcg), 5)\n",
    "headersPricePerDistanceAndTimeOpt = headersPricePerDistanceAndTimeOpt :+ (colDurationOvernightPcg + \"_label\")"
   ],
   "id": "d8c2b74844f233ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddDistBin: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[93] at map at <console>:60\n",
       "headersPricePerDistanceAndTimeOpt: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, trip_distance, payment_type, year, duration_minutes_bin_label, overnight_duration_pcg, weekday_surcharge, agg_fee_bin_label, cost_per_time, cost_per_distance, distance_bin_label, overnight_duration_pcg_label)\n",
       "rddOvernightBin: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[94] at map at <console>:60\n",
       "headersPricePerDistanceAndTimeOpt: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, trip_distance, payment_type, year, duration_minutes_bin_label, overnight_duration_pcg, weekday_surcharge, agg_fee_bin_label, cost_per_time, cost_per_distance, distance_bin_lab...\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:14.883522Z",
     "start_time": "2025-07-01T08:45:14.740026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val colToRemoveOvernightBin = Set(\"trip_distance\", colDurationOvernightPcg)\n",
    "\n",
    "val rddOvernightBinOpt = removeColumns(rddOvernightBin, headersPricePerDistanceAndTimeOpt, colToRemoveOvernightBin)\n",
    "var headersOvernightBinOpt = headersPricePerDistanceAndTimeOpt.filterNot(col => colToRemoveOvernightBin.contains(col.toLowerCase))"
   ],
   "id": "3895474ebdec5875",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colToRemoveOvernightBin: scala.collection.immutable.Set[String] = Set(trip_distance, overnight_duration_pcg)\n",
       "rddOvernightBinOpt: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[95] at map at <console>:173\n",
       "headersOvernightBinOpt: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, payment_type, year, duration_minutes_bin_label, weekday_surcharge, agg_fee_bin_label, cost_per_time, cost_per_distance, distance_bin_label, overnight_duration_pcg_label)\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "8) Add key for average calculation based on columns for classification",
   "id": "6419c663ec4fef40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:15.114259Z",
     "start_time": "2025-07-01T08:45:14.887937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "val numPartitions = spark.sparkContext.defaultParallelism\n",
    "val partitioner = new HashPartitioner(numPartitions)\n",
    "\n",
    "val actualHeader = headersOvernightBinOpt\n",
    "def addKey(rdd: RDD[Row], colsClassification: Seq[String], headers: Seq[String]): RDD[(String, Row)] = {\n",
    "  rdd\n",
    "    .map { row =>\n",
    "      val key = colsClassification.filter(col => headers.contains(col.toLowerCase))\n",
    "      .map(col => row.get(headers.indexOf(col.toLowerCase)))\n",
    "      .mkString(\"_\")\n",
    "      (key, row)\n",
    "    }\n",
    "}\n",
    "\n",
    "val rddWithKey = addKey(rddOvernightBinOpt, colsForClassification, actualHeader).partitionBy(partitioner).persist(StorageLevel.MEMORY_ONLY)"
   ],
   "id": "3b88a93a6da25df1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\n",
       "numPartitions: Int = 12\n",
       "partitioner: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@c\n",
       "actualHeader: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, payment_type, year, duration_minutes_bin_label, weekday_surcharge, agg_fee_bin_label, cost_per_time, cost_per_distance, distance_bin_label, overnight_duration_pcg_label)\n",
       "addKey: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], colsClassification: Seq[String], headers: Seq[String])org.apache.spark.rdd.RDD[(String, org.apache.spark.sql.Row)]\n",
       "rddWithKey: org.apache.spark.rdd.RDD[(String, org.apache.spark.sql.Row)] = ShuffledRDD[97] at partitionBy at <console>:76\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "9) Calculate prices per distance and time",
   "id": "d200896a8f8a49d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:15.299204Z",
     "start_time": "2025-07-01T08:45:15.118088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculatePrices(rdd: RDD[(String, Row)], headers: Seq[String], colPriceDistance: String, colPriceTime: String): RDD[(String, (Double, Double, Long))] = {\n",
    "  rdd\n",
    "    .mapValues { row =>\n",
    "      val costPerDistance = row.getAs[Double](headers.indexOf(colPriceDistance))\n",
    "      val costPerTime = row.getAs[Double](headers.indexOf(colPriceTime))\n",
    "      (costPerDistance, costPerTime, 1L)\n",
    "    }\n",
    "}\n",
    "\n",
    "val rddForAvg = calculatePrices(rddWithKey, headersOvernightBinOpt, colPricePerDistance, colPricePerTime)"
   ],
   "id": "85e4e86bc9992218",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculatePrices: (rdd: org.apache.spark.rdd.RDD[(String, org.apache.spark.sql.Row)], headers: Seq[String], colPriceDistance: String, colPriceTime: String)org.apache.spark.rdd.RDD[(String, (Double, Double, Long))]\n",
       "rddForAvg: org.apache.spark.rdd.RDD[(String, (Double, Double, Long))] = MapPartitionsRDD[98] at mapValues at <console>:60\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "10) Calculate average prices per distance and time",
   "id": "ab90fed37146ba50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:15.512566Z",
     "start_time": "2025-07-01T08:45:15.302902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculateAvgPrices(rdd: RDD[(String, (Double, Double, Long))], decimals: Int): RDD[(String, (Double, Double))] = {\n",
    "  rdd\n",
    "    .reduceByKey {\n",
    "      case ((d1, t1, c1), (d2, t2, c2)) => (d1 + d2, t1 + t2, c1 + c2)\n",
    "    }\n",
    "    .mapValues {\n",
    "      case (sumDist, sumTime, count) =>\n",
    "        val avgDist = BigDecimal(sumDist / count).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "        val avgTime = BigDecimal(sumTime / count).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "        (avgDist, avgTime)\n",
    "    }\n",
    "    .filter {\n",
    "      case (_, (dist, time)) => dist > 0.0 && time > 0.0\n",
    "    }\n",
    "}\n",
    "\n",
    "val rddWithAvgPrices = calculateAvgPrices(rddForAvg, decimals)"
   ],
   "id": "6c3896dc99dbd31b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculateAvgPrices: (rdd: org.apache.spark.rdd.RDD[(String, (Double, Double, Long))], decimals: Int)org.apache.spark.rdd.RDD[(String, (Double, Double))]\n",
       "rddWithAvgPrices: org.apache.spark.rdd.RDD[(String, (Double, Double))] = MapPartitionsRDD[101] at filter at <console>:66\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "11) Join average prices to previous rdd",
   "id": "2b46c67228dd6352"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:26.880479Z",
     "start_time": "2025-07-01T08:45:15.516234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.broadcast.Broadcast\n",
    "\n",
    "val broadcastAvgPrices: Broadcast[Map[String, (Double, Double)]] = spark.sparkContext.broadcast(rddWithAvgPrices.collectAsMap().toMap)\n",
    "\n",
    "def applyJoin(rdd: RDD[(String, Row)], broadcastMap: Broadcast[Map[String, (Double, Double)]]): RDD[Row] = {\n",
    "  rdd\n",
    "    .flatMap {\n",
    "      case (key, originalRow) => broadcastMap.value.get(key)\n",
    "        .map {\n",
    "          case (avgCostPerDistance, avgCostPerTime) => Row.fromSeq(originalRow.toSeq ++ Seq(avgCostPerDistance, avgCostPerTime))\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "val rddJoined = applyJoin(rddWithKey, broadcastAvgPrices)\n",
    "\n",
    "rddWithKey.unpersist()\n",
    "\n",
    "headersOvernightBinOpt = headersOvernightBinOpt ++ Seq(colAvgPricePerDistance, colAvgPricePerTime)"
   ],
   "id": "c7da5320936ec0fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.broadcast.Broadcast\n",
       "broadcastAvgPrices: org.apache.spark.broadcast.Broadcast[Map[String,(Double, Double)]] = Broadcast(21)\n",
       "applyJoin: (rdd: org.apache.spark.rdd.RDD[(String, org.apache.spark.sql.Row)], broadcastMap: org.apache.spark.broadcast.Broadcast[Map[String,(Double, Double)]])org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rddJoined: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[102] at flatMap at <console>:66\n",
       "headersOvernightBinOpt: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, payment_type, year, duration_minutes_bin_label, weekday_surcharge, agg_fee_bin_label, cost_per_time, cost_per_distance, distance_bin_label, overnight_duration_pcg_label, avg_cost_per_distance, avg_cost_per_time)\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:28.550525Z",
     "start_time": "2025-07-01T08:45:26.886032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val colToRemoveJoin = Set(\"trip_distance\", colDurationOvernightPcg)\n",
    "\n",
    "val rddJoinOpt = removeColumns(rddJoined, headersOvernightBinOpt, colToRemovePricePerDistanceAndTime)\n",
    "var headersJoinOpt = headersOvernightBinOpt.filterNot(col => colToRemoveJoin.contains(col.toLowerCase))"
   ],
   "id": "a218ec951ce87cce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colToRemoveJoin: scala.collection.immutable.Set[String] = Set(trip_distance, overnight_duration_pcg)\n",
       "rddJoinOpt: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[103] at map at <console>:173\n",
       "headersJoinOpt: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, payment_type, year, duration_minutes_bin_label, weekday_surcharge, agg_fee_bin_label, cost_per_time, cost_per_distance, distance_bin_label, overnight_duration_pcg_label, avg_cost_per_distance, avg_cost_per_time)\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "12) Add price comparison w.r.t. average price and actual price difference",
   "id": "b97ec9d1cf2b66bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:28.987264Z",
     "start_time": "2025-07-01T08:45:28.554666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def addPriceComparison(rdd: RDD[Row], headers: Seq[String], colPriceDistance: String, colAvgPriceDistance: String, colPriceTime: String, colAvgPriceTime: String, decimals: Int) = {\n",
    "  rdd.map { row =>\n",
    "    val priceColsToAdd: Seq[Double] = Seq((colPriceDistance, colAvgPriceDistance), (colPriceTime, colAvgPriceTime))\n",
    "      .flatMap { case (colPrice, colAvgPrice) =>\n",
    "        val price = row.getAs[Double](headers.indexOf(colPrice))\n",
    "        val priceAvg = row.getAs[Double](headers.indexOf(colAvgPrice))\n",
    "        val priceDiff = BigDecimal(price - priceAvg).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "        val priceDiffPcg = BigDecimal(priceDiff / priceAvg * 100).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "\n",
    "        Seq(priceDiff, priceDiffPcg)\n",
    "      }\n",
    "    Row.fromSeq(row.toSeq ++ priceColsToAdd)\n",
    "  }\n",
    "}\n",
    "\n",
    "val rddPriceComparison = addPriceComparison(rddJoinOpt, headersJoinOpt, colPricePerDistance, colAvgPricePerDistance, colPricePerTime, colAvgPricePerTime, decimals)\n",
    "headersJoinOpt = headersJoinOpt ++ Seq(colPricePerDistanceDiff, colPricePerDistanceDiffPcg, colPricePerTimeDiff, colPricePerTimeDiffPcg)"
   ],
   "id": "794982d2cb593fd2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addPriceComparison: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], headers: Seq[String], colPriceDistance: String, colAvgPriceDistance: String, colPriceTime: String, colAvgPriceTime: String, decimals: Int)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rddPriceComparison: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[104] at map at <console>:67\n",
       "headersJoinOpt: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, payment_type, year, duration_minutes_bin_label, weekday_surcharge, agg_fee_bin_label, cost_per_time, cost_per_distance, distance_bin_label, overnight_duration_pcg_label, avg_cost_per_distance, avg_cost_per_time, cost_per_distance_diff, cost_per_distance_diff_pcg, cost_per_time_diff, cost_per_time_diff_pcg)\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "13) Bin price difference per time and distance",
   "id": "988b3766e90e9789"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:29.287206Z",
     "start_time": "2025-07-01T08:45:28.991300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val rddPriceDistBin = binColByStepValue(rddPriceComparison,headersJoinOpt.indexOf(colPricePerDistanceDiffPcg), 5)\n",
    "val rddPriceDistTimeBin = binColByStepValue(rddPriceDistBin, headersJoinOpt.indexOf(colPricePerTimeDiffPcg), 5)\n",
    "\n",
    "headersJoinOpt = headersJoinOpt ++ Seq(colPricePerDistanceDiffPcgLabel, colPricePerTimeDiffPcgLabel)"
   ],
   "id": "d43c4991694d18f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddPriceDistBin: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[105] at map at <console>:60\n",
       "rddPriceDistTimeBin: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[106] at map at <console>:60\n",
       "headersJoinOpt: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, payment_type, year, duration_minutes_bin_label, weekday_surcharge, agg_fee_bin_label, cost_per_time, cost_per_distance, distance_bin_label, overnight_duration_pcg_label, avg_cost_per_distance, avg_cost_per_time, cost_per_distance_diff, cost_per_distance_diff_pcg, cost_per_time_diff, cost_per_time_diff_pcg, cost_per_distance_diff_pcg_label, cost_per_time_diff_pcg_label)\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:29.657096Z",
     "start_time": "2025-07-01T08:45:29.291557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val colToRemovePriceDistTimeBin = Set(colPricePerDistanceDiff, colPricePerDistanceDiffPcg, colPricePerTimeDiff, colPricePerTimeDiffPcg, colWeekdaySurcharge, colPricePerTime, colPricePerDistance, colAvgPricePerDistance, colAvgPricePerTime)\n",
    "\n",
    "val rddPriceDistTimeBinOpt = removeColumns(rddPriceDistTimeBin, headersJoinOpt, colToRemovePriceDistTimeBin)\n",
    "var headersPriceDistTimeBinOpt = headersJoinOpt.filterNot(col => colToRemovePriceDistTimeBin.contains(col.toLowerCase))"
   ],
   "id": "e3576a957119ffeb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colToRemovePriceDistTimeBin: scala.collection.immutable.Set[String] = Set(avg_cost_per_distance, cost_per_distance, cost_per_distance_diff_pcg, avg_cost_per_time, cost_per_time_diff, weekday_surcharge, cost_per_time_diff_pcg, cost_per_distance_diff, cost_per_time)\n",
       "rddPriceDistTimeBinOpt: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[107] at map at <console>:173\n",
       "headersPriceDistTimeBinOpt: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, payment_type, year, duration_minutes_bin_label, agg_fee_bin_label, distance_bin_label, overnight_duration_pcg_label, cost_per_distance_diff_pcg_label, cost_per_time_diff_pcg_label)\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "14) Reduce to analysis columns only",
   "id": "d55dcfcdf1b91809"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:32.621941Z",
     "start_time": "2025-07-01T08:45:29.661218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val headersForAnalysis = headersPriceDistTimeBinOpt.zipWithIndex.filter(head => colsForClassification.contains(head._1.toLowerCase))\n",
    "\n",
    "val headersForAnalysisIdxs = headersForAnalysis.map(_._2)\n",
    "val headersForAnalysisCols = headersForAnalysis.map(_._1)\n",
    "\n",
    "def reduceToAnalysis(rdd: RDD[Row], idxs: Seq[Int]): RDD[Row] = {\n",
    "  rdd.map { row => Row.fromSeq(idxs.map(row.get)) }\n",
    "}\n",
    "\n",
    "val rddAnalysis = reduceToAnalysis(rddPriceDistTimeBinOpt, headersForAnalysisIdxs)\n",
    "\n",
    "val totalCount = rddAnalysis.count()"
   ],
   "id": "c56a14f69be905a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headersForAnalysis: Seq[(String, Int)] = ArraySeq((store_and_fwd_flag,0), (passenger_count,1), (payment_type,2), (year,3), (duration_minutes_bin_label,4), (agg_fee_bin_label,5), (distance_bin_label,6), (overnight_duration_pcg_label,7), (cost_per_distance_diff_pcg_label,8), (cost_per_time_diff_pcg_label,9))\n",
       "headersForAnalysisIdxs: Seq[Int] = ArraySeq(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
       "headersForAnalysisCols: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, payment_type, year, duration_minutes_bin_label, agg_fee_bin_label, distance_bin_label, overnight_duration_pcg_label, cost_per_distance_diff_pcg_label, cost_per_time_diff_pcg_label)\n",
       "reduceToAnalysis: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], idxs: Seq[Int])org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rddAnaly...\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "15) Group by feature value",
   "id": "9f6cdebc4d24c9de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:34.925083Z",
     "start_time": "2025-07-01T08:45:32.626043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def groupByFeatures(rdd: RDD[Row], colForValuesAnalysis: Seq[String], colPriceDistanceDiffPcgLabel: String, colPriceTimeDiffPcgLabel: String, headersAnalysis: Seq[String], decimals: Int, totalCount: Long): Seq[RDD[Row]] = {\n",
    "  colForValuesAnalysis\n",
    "    .map { colName =>\n",
    "      val groupCols = Seq(colPriceDistanceDiffPcgLabel, colPriceTimeDiffPcgLabel):+ colName\n",
    "      val grouped = rdd\n",
    "        .map { row =>\n",
    "          val key = groupCols.map(col => row.get(headersAnalysis.indexOf(col.toLowerCase)))\n",
    "          (key, 1)\n",
    "        }\n",
    "        .reduceByKey(_ + _).map { case (keySeq, count) =>\n",
    "          val value = keySeq.last.toString\n",
    "          val costDistLabel = keySeq(0).toString\n",
    "          val costTimeLabel = keySeq(1).toString\n",
    "          val pcg = BigDecimal(count.toDouble / totalCount * 100).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "          Row.fromSeq(Seq(colName, value, count, pcg, costDistLabel, costTimeLabel))\n",
    "        }\n",
    "      grouped\n",
    "    }\n",
    "}\n",
    "\n",
    "val rddFeatures = groupByFeatures(rddAnalysis, colsForValuesAnalysis, colPricePerDistanceDiffPcgLabel, colPricePerTimeDiffPcgLabel, headersForAnalysisCols, decimals, totalCount)"
   ],
   "id": "298184e06179fe88",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "groupByFeatures: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], colForValuesAnalysis: Seq[String], colPriceDistanceDiffPcgLabel: String, colPriceTimeDiffPcgLabel: String, headersAnalysis: Seq[String], decimals: Int, totalCount: Long)Seq[org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]]\n",
       "rddFeatures: Seq[org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]] = List(MapPartitionsRDD[111] at map at <console>:70, MapPartitionsRDD[114] at map at <console>:70, MapPartitionsRDD[117] at map at <console>:70, MapPartitionsRDD[120] at map at <console>:70, MapPartitionsRDD[123] at map at <console>:70, MapPartitionsRDD[126] at map at <console>:70, MapPartitionsRDD[129] at map at <console>:70, MapPartitionsRDD[132] at map at <console>:70)\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "16) Reduce to single rdd and write output",
   "id": "795fbb90ba3dc7a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:45:50.835768Z",
     "start_time": "2025-07-01T08:45:34.929472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val headersForSchema = Seq(\n",
    "  StructField(\"feature\", StringType),\n",
    "  StructField(\"value\", StringType),\n",
    "  StructField(\"count\", IntegerType),\n",
    "  StructField(\"pcg\", DoubleType),\n",
    "  StructField(\"cost_distance_label\", StringType),\n",
    "  StructField(\"cost_time_label\", StringType)\n",
    ")\n",
    "\n",
    "val schema = StructType(headersForSchema)\n",
    "\n",
    "val dfForAnalysis = spark.createDataFrame(rddFeatures.reduce(_ union _).coalesce(1), schema)\n",
    "\n",
    "dfForAnalysis.show(1)\n",
    "val endTime = System.currentTimeMillis()\n",
    "val durationMs = endTime - startTime\n",
    "\n",
    "println(s\"Job $name-dataset optimized executed in $durationMs ms\")\n",
    "\n",
    "dfForAnalysis.write.mode(\"overwrite\").parquet(getDatasetPath(outputDir + f\"/$name\"))"
   ],
   "id": "f147c54fa9f01d8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-----+------+-------------------+---------------+\n",
      "|        feature|value|count|   pcg|cost_distance_label|cost_time_label|\n",
      "+---------------+-----+-----+------+-------------------+---------------+\n",
      "|passenger_count|  1.0|   72|0.0088|          [-55|-50)|        [30|35)|\n",
      "+---------------+-----+-----+------+-------------------+---------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Job green-dataset optimized executed in 35797 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "headersForSchema: Seq[org.apache.spark.sql.types.StructField] = List(StructField(feature,StringType,true), StructField(value,StringType,true), StructField(count,IntegerType,true), StructField(pcg,DoubleType,true), StructField(cost_distance_label,StringType,true), StructField(cost_time_label,StringType,true))\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(feature,StringType,true),StructField(value,StringType,true),StructField(count,IntegerType,true),StructField(pcg,DoubleType,true),StructField(cost_distance_label,StringType,true),StructField(cost_time_label,StringType,true))\n",
       "dfForAnalysis: org.apache.spark.sql.DataFrame = [feature: string, value: string ... 4 more fields]\n",
       "endTime: Long = 1751359547593\n",
       "durationMs: Long = 35797\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
