{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cefe988a9aff7538",
   "metadata": {},
   "source": [
    "# Big Data project A.Y. 2024-2025 - First Job\n",
    "\n",
    "## Members\n",
    "\n",
    "- Giovanni Antonioni\n",
    "- Luca Rubboli - 0001083742"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce3e263b662de6",
   "metadata": {},
   "source": [
    "### Define useful parameters\n",
    "\n",
    "- Dataset location\n",
    "- Iterator (defined like this to overcome different names for same columns in dataset)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c9c769832012f1b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:31:43.677353Z",
     "start_time": "2025-05-12T08:31:26.568484Z"
    }
   },
   "source": [
    "import org.apache.spark\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val\n",
    "spark = SparkSession.builder\n",
    ".appName(\"First job\")\n",
    ".getOrCreate()\n",
    "\n",
    "val\n",
    "sc = spark.sparkContext"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://mac.fritz.box:4040\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1747038691597)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\n",
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6940eef8\n",
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@14d69012\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:58:59.570832Z",
     "start_time": "2025-05-11T16:58:57.269662Z"
    }
   },
   "source": [
    "val\n",
    "decimals: Int = 4\n",
    "val\n",
    "datasetDir = \"dataset\"\n",
    "val\n",
    "outputDir = \"output/firstJobOutput\"\n",
    "val\n",
    "yellowDatasetDir = s\n",
    "\"$datasetDir/yellow_cab\"\n",
    "val\n",
    "greenDatasetDir = s\n",
    "\"$datasetDir/green_cab\"\n",
    "val\n",
    "fhvDatasetDir = s\n",
    "\"$datasetDir/fhv_cab\"\n",
    "val\n",
    "fhvhvDatasetDir = s\n",
    "\"$datasetDir/fhvhv_cab\"\n",
    "val\n",
    "datasetDirMap: Map[String, String] = Map(\"yellow\" -> yellowDatasetDir, \"green\" -> greenDatasetDir,\n",
    "\"fhv\" -> fhvDatasetDir, \"fhvhv\" -> fhvhvDatasetDir)\n",
    "val\n",
    "datasetIterator: Iterable[(String, String, String)] = Seq(\n",
    "  (\"yellow\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\"),\n",
    "  (\"green\", \"lpep_dropoff_datetime\", \"lpep_pickup_datetime\"),\n",
    "                                                      // (\"fhv\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\"),\n",
    "// (\"fhvhv\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\"),\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decimals: Int = 4\n",
       "datasetDir: String = dataset\n",
       "outputDir: String = output/firstJobOutput\n",
       "yellowDatasetDir: String = dataset/yellow_cab\n",
       "greenDatasetDir: String = dataset/green_cab\n",
       "fhvDatasetDir: String = dataset/fhv_cab\n",
       "fhvhvDatasetDir: String = dataset/fhvhv_cab\n",
       "datasetDirMap: Map[String,String] = Map(yellow -> dataset/yellow_cab, green -> dataset/green_cab, fhv -> dataset/fhv_cab, fhvhv -> dataset/fhvhv_cab)\n",
       "datasetIterator: Iterable[(String, String, String)] = List((yellow,tpep_dropoff_datetime,tpep_pickup_datetime), (green,lpep_dropoff_datetime,lpep_pickup_datetime))\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "801ff1e945bd4c8",
   "metadata": {},
   "source": [
    "## Define Columns for analysis\n",
    "- Columns names\n",
    "- Time zones for overprice\n",
    "- Columns used in classification for average price calculation\n",
    "- Columns which values are used in analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "bfdb3f87a1c6a6c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:59:00.680091Z",
     "start_time": "2025-05-11T16:58:59.575217Z"
    }
   },
   "source": [
    "val\n",
    "colDurationMinutes: String = \"duration_minutes\"\n",
    "val\n",
    "colDurationMinutesBinLabel: String = \"duration_minutes_bin_label\"\n",
    "val\n",
    "colYear: String = \"year\"\n",
    "val\n",
    "colWeekdaySurcharge: String = \"weekday_surcharge\"\n",
    "val\n",
    "colAggregateFee: String = \"fees\"\n",
    "val\n",
    "colAggregateFeeBin: String = \"agg_fee_bin_label\"\n",
    "val\n",
    "colDistanceBin: String = \"distance_bin_label\"\n",
    "val\n",
    "colFareAmount: String = \"fare_amount\"\n",
    "val\n",
    "colPricePerDistance: String = \"cost_per_distance\"\n",
    "val\n",
    "colPricePerTime: String = \"cost_per_time\"\n",
    "val\n",
    "colAvgPricePerDistance: String = \"avg_cost_per_distance\"\n",
    "val\n",
    "colAvgPricePerTime: String = \"avg_cost_per_time\"\n",
    "val\n",
    "colPricePerDistanceDiff: String = \"cost_per_distance_diff\"\n",
    "val\n",
    "colPricePerDistanceDiffPcg: String = \"cost_per_distance_diff_pcg\"\n",
    "val\n",
    "colPricePerTimeDiff: String = \"cost_per_time_diff\"\n",
    "val\n",
    "colPricePerTimeDiffPcg: String = \"cost_per_time_diff_pcg\"\n",
    "val\n",
    "colPricePerDistanceDiffPcgLabel: String = colPricePerDistanceDiffPcg + \"_label\"\n",
    "val\n",
    "colPricePerTimeDiffPcgLabel: String = colPricePerTimeDiffPcg + \"_label\"\n",
    "\n",
    "val\n",
    "timeZoneOver: String = \"overnight\"\n",
    "val\n",
    "timeZones = Map(timeZoneOver -> (20, 6), \"regular\" -> (6, 20))\n",
    "val\n",
    "weekDaySurcharge: Double = 2.5\n",
    "\n",
    "val\n",
    "colDurationOvernightPcg: String = s\n",
    "\"${timeZoneOver}_duration_pcg\"\n",
    "\n",
    "val\n",
    "colToUse: Set[String] = Set(\n",
    "  \"tpep_pickup_datetime\",\n",
    "  \"tpep_dropoff_datetime\",\n",
    "  \"lpep_pickup_datetime\",\n",
    "  \"lpep_dropoff_datetime\",\n",
    "  \"passenger_count\",\n",
    "  \"trip_distance\",\n",
    "  \"ratecodeid\",\n",
    "  \"store_and_fwd_flag\",\n",
    "  \"payment_type\",\n",
    "  \"fare_amount\",\n",
    "  \"extra\",\n",
    "  \"mta_tax\",\n",
    "  \"tip_amount\",\n",
    "  \"tolls_amount\",\n",
    "  \"improvement_surcharge\",\n",
    "  \"total_amount\",\n",
    "  \"congestion_surcharge\",\n",
    "  \"airport_fee\")\n",
    "\n",
    "val\n",
    "colFees: Set[String] = Set(\n",
    "  \"extra\",\n",
    "  \"mta_tax\",\n",
    "  \"improvement_surcharge\",\n",
    "  \"congestion_surcharge\",\n",
    "  \"airport_fee\")\n",
    "\n",
    "val\n",
    "colsForClassification: Seq[String] = Seq(\n",
    "  \"passenger_count\",\n",
    "  \"store_and_fwd_flag\",\n",
    "  \"payment_type\",\n",
    "  colAggregateFeeBin,\n",
    "  colDurationMinutesBinLabel,\n",
    "  colDistanceBin,\n",
    "  colYear,\n",
    "  s\n",
    "\"${colDurationOvernightPcg}_label\",\n",
    "colPricePerDistanceDiffPcgLabel,\n",
    "colPricePerTimeDiffPcgLabel\n",
    ")\n",
    "\n",
    "val\n",
    "colsForValuesAnalysis: Seq[String] = Seq(\n",
    "  \"passenger_count\",\n",
    "  \"store_and_fwd_flag\",\n",
    "  \"payment_type\",\n",
    "  colAggregateFeeBin,\n",
    "  colDurationMinutesBinLabel,\n",
    "  colDistanceBin,\n",
    "  colYear,\n",
    "  s\n",
    "\"${colDurationOvernightPcg}_label\",\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colDurationMinutes: String = duration_minutes\n",
       "colDurationMinutesBinLabel: String = duration_minutes_bin_label\n",
       "colYear: String = year\n",
       "colWeekdaySurcharge: String = weekday_surcharge\n",
       "colAggregateFee: String = fees\n",
       "colAggregateFeeBin: String = agg_fee_bin_label\n",
       "colDistanceBin: String = distance_bin_label\n",
       "colFareAmount: String = fare_amount\n",
       "colPricePerDistance: String = cost_per_distance\n",
       "colPricePerTime: String = cost_per_time\n",
       "colAvgPricePerDistance: String = avg_cost_per_distance\n",
       "colAvgPricePerTime: String = avg_cost_per_time\n",
       "colPricePerDistanceDiff: String = cost_per_distance_diff\n",
       "colPricePerDistanceDiffPcg: String = cost_per_distance_diff_pcg\n",
       "colPricePerTimeDiff: String = cost_per_time_diff\n",
       "colPricePerTimeDiffPcg: String = cost_per_time_diff_pcg\n",
       "colPricePerDistanceDiffPcgLabel: String = ...\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "6251e1a01e2d34a6",
   "metadata": {},
   "source": [
    "### Define preprocess rules"
   ]
  },
  {
   "cell_type": "code",
   "id": "f3c66b94d590de92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:59:01.712901Z",
     "start_time": "2025-05-11T16:59:00.684421Z"
    }
   },
   "source": [
    "val\n",
    "featureFilters: Map[String, Any = > Boolean] = Map(\n",
    "  \"passenger_count\" -> {\n",
    "  case\n",
    "i: Int = > i > 0\n",
    "case\n",
    "f: Float = > val\n",
    "i = f.toInt;\n",
    "i > 0\n",
    "case\n",
    "d: Double = > val\n",
    "i = d.toInt;\n",
    "i > 0\n",
    "case\n",
    "_ = > false\n",
    "},\n",
    "\"trip_distance\" -> {\n",
    "case i: Int = > i > 0\n",
    "case i: Float = > i > 0\n",
    "case i: Double = > i > 0\n",
    "case _ = > false\n",
    "},\n",
    "\"ratecodeid\" -> {\n",
    "case i: Int = > (i >= 1 & & i <= 6) | | i == 99\n",
    "case f: Float = > val i = f.toInt; (i >= 1 & & i <= 6) | | i == 99\n",
    "case d: Double = > val i = d.toInt; (i >= 1 & & i <= 6) | | i == 99\n",
    "case _ = > false\n",
    "},\n",
    "\"store_and_fwd_flag\" -> {\n",
    "case i: String = > i == \"Y\" | | i == \"N\"\n",
    "case _ = > false\n",
    "},\n",
    "\"payment_type\" -> {\n",
    "case i: Int = > i >= 1 & & i <= 6\n",
    "case f: Float = > val i = f.toInt; i >= 1 & & i <= 6\n",
    "case d: Double = > val i = d.toInt; i >= 1 & & i <= 6\n",
    "case _ = > false\n",
    "},\n",
    "\"fare_amount\" -> {\n",
    "case i: Int = > i > 0\n",
    "case i: Float = > i > 0\n",
    "case i: Double = > i > 0\n",
    "case _ = > false\n",
    "},\n",
    "\"tolls_amount\" -> {\n",
    "case i: Int = > i >= 0 & & i < 200\n",
    "case i: Float = > i >= 0 & & i < 200\n",
    "case i: Double = > i >= 0 & & i < 200\n",
    "case _ = > false\n",
    "}\n",
    ")\n",
    "\n",
    "def taxFilter(tax: Any): Boolean = {\n",
    "  tax\n",
    "\n",
    "\n",
    "match\n",
    "{\n",
    "  case\n",
    "tax: Int = > tax >= 0 & & tax < 20\n",
    "case\n",
    "tax: Float = > tax >= 0 & & tax < 20\n",
    "case\n",
    "tax: Double = > tax >= 0 & & tax < 20\n",
    "case\n",
    "_ = > false\n",
    "}\n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureFilters: Map[String,Any => Boolean] = Map(trip_distance -> $Lambda$2372/0x0000000801088040@378b1c26, tolls_amount -> $Lambda$2377/0x000000080108c040@18befbd8, payment_type -> $Lambda$2375/0x000000080108a840@5b3cd415, fare_amount -> $Lambda$2376/0x000000080108b040@6c86b738, passenger_count -> $Lambda$2371/0x0000000801087840@383eca2, store_and_fwd_flag -> $Lambda$2374/0x0000000801089840@6c0ca731, ratecodeid -> $Lambda$2373/0x0000000801089040@51ad4ad9)\n",
       "taxFilter: (tax: Any)Boolean\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "46150dfaa1006964",
   "metadata": {},
   "source": [
    "### Utils functions for rdd"
   ]
  },
  {
   "cell_type": "code",
   "id": "9aae38423ac3200c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:59:03.590585Z",
     "start_time": "2025-05-11T16:59:01.716922Z"
    }
   },
   "source": [
    "import java.time.temporal.ChronoUnit\n",
    "import java.time.\n",
    "\n",
    "{DayOfWeek, LocalDate, LocalDateTime}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Row\n",
    "import scala.math.BigDecimal.RoundingMode\n",
    "\n",
    "\n",
    "def binColByStepValue(rdd: RDD[Row], indexOfColToDiscrete: Int, stepValue: Int = 5): RDD[Row] = {\n",
    "  rdd.map\n",
    "\n",
    "\n",
    "{row = >\n",
    "val\n",
    "value: Double = row.get(indexOfColToDiscrete)\n",
    "match\n",
    "{\n",
    "  case\n",
    "i: Int = > i.toDouble\n",
    "case\n",
    "d: Double = > d\n",
    "case\n",
    "l: Long = > l.toDouble\n",
    "case\n",
    "s: String = > try {s.toDouble} catch {case _: Throwable = > Double.NaN}\n",
    "case\n",
    "_ = > Double.NaN\n",
    "}\n",
    "\n",
    "val\n",
    "rawBin = (value / stepValue).toInt * stepValue\n",
    "\n",
    "val\n",
    "binBase =\n",
    "if (value < 0 & & value % stepValue == 0) rawBin + stepValue else rawBin\n",
    "\n",
    "val label = if (value < 0) {\n",
    "s\n",
    "\"[${(binBase - stepValue).toInt}|${binBase.toInt})\"\n",
    "} else {\n",
    "s\n",
    "\"[${binBase.toInt}|${(binBase + stepValue).toInt})\"\n",
    "}\n",
    "\n",
    "Row.fromSeq(row.toSeq: + label)\n",
    "}\n",
    "}\n",
    "\n",
    "val\n",
    "castForFilter: Any = > Any = {\n",
    "  case\n",
    "s: String = > if (s.matches(\"\"\"^-?\\d+\\.\\d+$\"\"\")) s.toDouble else if (s.matches(\"\"\"^-?\\d+$\"\"\")) s.toInt else s.trim\n",
    "case d: Double = > d\n",
    "case\n",
    "i: Int = > i\n",
    "case\n",
    "l: Long = > l.toDouble\n",
    "case\n",
    "f: Float = > f.toDouble\n",
    "case\n",
    "b: Boolean = > b\n",
    "case\n",
    "null = > null\n",
    "case\n",
    "other = > other.toString.trim\n",
    "}\n",
    "\n",
    "def overlap(start1: LocalDateTime, end1: LocalDateTime,\n",
    "            start2: LocalDateTime, end2: LocalDateTime): Double = {\n",
    "  val\n",
    "\n",
    "\n",
    "overlapStart =\n",
    "if (start1.isAfter(start2)) start1 else start2\n",
    "val overlapEnd = if (end1.isBefore(end2)) end1 else end2\n",
    "if (overlapEnd.isAfter(overlapStart)) BigDecimal(ChronoUnit.MILLIS.between(overlapStart, overlapEnd) / 60000.0).setScale(decimals, RoundingMode.HALF_UP).toDouble else 0\n",
    "}\n",
    "\n",
    "\n",
    "def preciseBucketUDF(timeZones: Map[String, (Int, Int)], start: LocalDateTime, end: LocalDateTime): Map[\n",
    "  String, Double] = {\n",
    "  var\n",
    "\n",
    "\n",
    "result = timeZones.keys.map(_ -> 0.0).toMap\n",
    "\n",
    "if (!(start == null || end == null)) {\n",
    "\n",
    "    if (!end.isBefore(start)) {\n",
    "\n",
    "    var current = start.toLocalDate.atStartOfDay\n",
    "\n",
    "while (!current.isAfter(end)) {\n",
    "val nextDay = current.plusDays(1)\n",
    "\n",
    "timeZones.foreach {\n",
    "case (label, (startHour, endHour)) if startHour > endHour = >\n",
    "val bucketStartBeforeMidnight = current.withHour(startHour).withMinute(0).withSecond(0).withNano(0)\n",
    "val bucketEndBeforeMidnight = current.withHour(23).withMinute(59).withSecond(59)\n",
    "val bucketStartAfterMidnight = current.withHour(0).withMinute(0).withSecond(0).withNano(0)\n",
    "val bucketEndAfterMidnight = current.withHour(endHour).withMinute(0).withSecond(0).withNano(0)\n",
    "\n",
    "val minutesBeforeMidnight = overlap(start, end, bucketStartBeforeMidnight, bucketEndBeforeMidnight)\n",
    "val minutesAfterMidnight = overlap(start, end, bucketStartAfterMidnight, bucketEndAfterMidnight)\n",
    "\n",
    "result = result.updated(label, result(label) + minutesBeforeMidnight + minutesAfterMidnight)\n",
    "\n",
    "case (label, (startHour, endHour)) = >\n",
    "val bucketStart = current.withHour(startHour).withMinute(0).withSecond(0).withNano(0)\n",
    "val bucketEnd = if (endHour == 24) current.plusDays(1).withHour(0).withMinute(0).withSecond(0).withNano(0) else current.withHour(endHour).withMinute(0).withSecond(0).withNano(0)\n",
    "\n",
    "val minutes = overlap(start, end, bucketStart, bucketEnd)\n",
    "\n",
    "result = result.updated(label, result(label) + minutes)\n",
    "}\n",
    "current = nextDay\n",
    "}\n",
    "}\n",
    "}\n",
    "result\n",
    "}\n",
    "\n",
    "def isUSHolidayOrWeekend(date: LocalDate):\n",
    "  Boolean = {\n",
    "    val month = date.getMonthValue\n",
    "val day = date.getDayOfMonth\n",
    "val dayOfWeek = date.getDayOfWeek\n",
    "\n",
    "if (month == 7 & & day == 4) return true\n",
    "\n",
    "if (month == 12 & & day == 25) return true\n",
    "\n",
    "if (month == 1 & & day == 1) return true\n",
    "\n",
    "if (month == 9 & & dayOfWeek == DayOfWeek.MONDAY & & day <= 7) return true\n",
    "\n",
    "if (month == 11 & & dayOfWeek == DayOfWeek.THURSDAY & & (day >= 22 & & day <= 28)) {\n",
    "val weekOfMonth = (day - 1) / 7 + 1\n",
    "if (weekOfMonth == 4) return true\n",
    "}\n",
    "\n",
    "dayOfWeek == DayOfWeek.SATURDAY | | dayOfWeek == DayOfWeek.SUNDAY\n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.temporal.ChronoUnit\n",
       "import java.time.{DayOfWeek, LocalDate, LocalDateTime}\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.sql.Row\n",
       "import scala.math.BigDecimal.RoundingMode\n",
       "binColByStepValue: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], indexOfColToDiscrete: Int, stepValue: Int)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "castForFilter: Any => Any with Serializable = <function1>\n",
       "overlap: (start1: java.time.LocalDateTime, end1: java.time.LocalDateTime, start2: java.time.LocalDateTime, end2: java.time.LocalDateTime)Double\n",
       "preciseBucketUDF: (timeZones: Map[String,(Int, Int)], start: java.time.LocalDateTime, end: java.time.LocalDateTime)Map[String,Double]\n",
       "isUSHolidayOrWeekend: (date: java.time.LocalDate)Boolean\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "57c2f73d1187fa8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:59:03.835316Z",
     "start_time": "2025-05-11T16:59:03.594580Z"
    }
   },
   "source": [
    "val projectDir: String = \"/Users/luca/Desktop/Luca/Università/Magistrale/Corsi/BigData/Drivers\"\n",
    "\n",
    "\n",
    "def getDatasetPath(localPath: String): String = {\n",
    "  \"file://\" + projectDir + \"/\" + localPath\n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "projectDir: String = /Users/luca/Desktop/Luca/Università/Magistrale/Corsi/BigData/Drivers\n",
       "getDatasetPath: (localPath: String)String\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "d67c5a94825ecee9",
   "metadata": {},
   "source": [
    "# Actual job\n",
    "\n",
    "1) Select dataset, dropoff and pickup columns"
   ]
  },
  {
   "cell_type": "code",
   "id": "60aee06709329346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:59:04.003663Z",
     "start_time": "2025-05-11T16:59:03.839498Z"
    }
   },
   "source": [
    "val\n",
    "name: String = \"yellow\"\n",
    "val\n",
    "dropoff: String = \"tpep_dropoff_datetime\"\n",
    "val\n",
    "pickup: String = \"tpep_pickup_datetime\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: String = yellow\n",
       "dropoff: String = tpep_dropoff_datetime\n",
       "pickup: String = tpep_pickup_datetime\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2) Load dataset",
   "id": "6e3fcaeb36518cd9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:59:07.347304Z",
     "start_time": "2025-05-11T16:59:04.007815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val\n",
    "dataset = spark.read.parquet(getDatasetPath(datasetDirMap(name)))\n",
    "var\n",
    "headers: Seq[String] = dataset.columns.map(_.toLowerCase)\n",
    "val\n",
    "indexesToUse: Seq[Int] = headers.zipWithIndex.collect\n",
    "{\n",
    "  case(h, i) if colToUse.contains(h.toLowerCase) = > i\n",
    "}"
   ],
   "id": "a846e646fac3bac4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: timestamp_ntz ... 17 more fields]\n",
       "headers: Seq[String] = ArraySeq(vendorid, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, ratecodeid, store_and_fwd_flag, pulocationid, dolocationid, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge, airport_fee)\n",
       "indexesToUse: Seq[Int] = ArraySeq(1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T17:02:07.417330Z",
     "start_time": "2025-05-11T17:02:06.803264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import java.io._\n",
    "\n",
    "\n",
    "def isSerializable(obj: Any): Boolean = {\n",
    "\n",
    "\n",
    "try {\n",
    "val bos = new ByteArrayOutputStream()\n",
    "val out = new ObjectOutputStream(bos)\n",
    "out.writeObject(obj)\n",
    "out.close()\n",
    "true\n",
    "} catch {\n",
    "case e: Exception = >\n",
    "println(s\n",
    "\"Serialization failed: ${e}\")\n",
    "e.printStackTrace()\n",
    "false\n",
    "}\n",
    "}"
   ],
   "id": "1a97369378a2afe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import java.io._\n",
       "isSerializable: (obj: Any)Boolean\n",
       "defined object MyFilterUtils\n",
       "func: org.apache.spark.sql.Row => Any with java.io.Serializable = <function1>\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filter taxes and features based on filter conditions previously defined",
   "id": "df209b8e24cd13e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T21:47:12.393427Z",
     "start_time": "2025-05-11T21:34:28.164006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "\n",
    "def transformRDD(dataset: DataFrame, headers: Seq[String], idxs: Seq[Int], castFunc: Any = > Any): RDD[Row] = {\n",
    "  dataset.rdd.map(row= > Row.fromSeq(idxs.map(row.get).map(castFunc))\n",
    "\n",
    ")\n",
    "}\n",
    "\n",
    "var\n",
    "rdd = transformRDD(dataset, headers, indexesToUse, castForFilter)\n",
    "headers = headers.filter(head= > colToUse.contains(head.toLowerCase))\n",
    "\n",
    "def applyFilters(rdd: RDD[Row], headers: Seq[String], colOfFees: Set[String], taxFilter: Any = > Boolean,\n",
    "                 featFilter: Map[String, Any = > Boolean]): RDD[Row] = {\n",
    "  rdd.filter\n",
    "\n",
    "\n",
    "{row = >\n",
    "headers.zip(row.toSeq).forall\n",
    "{case(header: String, value) = >\n",
    "val\n",
    "taxFilterCondition =\n",
    "if (colOfFees.contains(header.toLowerCase)) taxFilter(value) else true\n",
    "featFilter.get(header.toLowerCase) match {\n",
    "case Some(filterFunc) = > taxFilterCondition & & filterFunc(value)\n",
    "case None = > taxFilterCondition // no filter defined for this column, so accept it\n",
    "}\n",
    "}\n",
    "}\n",
    "}\n",
    "\n",
    "rdd = applyFilters(rdd, headers, colFees, taxFilter, featureFilters)\n",
    "\n",
    "rdd.take(5).foreach(println)"
   ],
   "id": "f8a8a0eaeaa2d294",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Add duration and timezones",
   "id": "523a6cb64b4c03d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def addDuration(rdd: RDD[Row], decimals: Int): RDD[Row] = {\n",
    "  rdd.map\n",
    "\n",
    "\n",
    "{row = >\n",
    "val\n",
    "formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm[:ss]\")\n",
    "\n",
    "val\n",
    "pickupStr = row.getAs[String](headers.indexOf(pickup)).trim\n",
    "val\n",
    "dropoffStr = row.getAs[String](headers.indexOf(dropoff)).trim\n",
    "\n",
    "val\n",
    "pickupTS = LocalDateTime.parse(pickupStr, formatter)\n",
    "val\n",
    "dropoffTS = LocalDateTime.parse(dropoffStr, formatter)\n",
    "val\n",
    "durationMillis = Duration.between(pickupTS, dropoffTS).toMillis\n",
    "val\n",
    "durationMinutes = BigDecimal(durationMillis / 60000.0)\n",
    ".setScale(decimals, RoundingMode.HALF_UP)\n",
    ".toDouble\n",
    "\n",
    "val\n",
    "pickupYear = pickupTS.getYear\n",
    "\n",
    "Row.fromSeq(row.toSeq + + Seq(durationMinutes, pickupYear))\n",
    "}.filter\n",
    "{row = >\n",
    "row.getAs[Double](row.toSeq.length - 2) > 0.0\n",
    "}\n",
    "}\n",
    "\n",
    "rdd = addDuration(rdd, decimals)\n",
    "headers = headers + + Seq(colDurationMinutes, colYear)\n",
    "\n",
    "\n",
    "def addTimeZones(rdd: RDD[Row], timezones: Map[String, (Int, Int)], weekDaySurcharge: Double, colDuration: String,\n",
    "                 decimals: Int): RDD[Row] = {\n",
    "  rdd.map\n",
    "\n",
    "\n",
    "{row = >\n",
    "val\n",
    "formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm[:ss]\")\n",
    "\n",
    "val\n",
    "timeZonesDuration: Map[String, Double] = preciseBucketUDF(timeZones,\n",
    "                                                          LocalDateTime.parse(\n",
    "                                                            row.getAs[String](headers.indexOf(pickup)).trim, formatter),\n",
    "                                                          LocalDateTime.parse(\n",
    "                                                            row.getAs[String](headers.indexOf(dropoff)).trim, formatter)\n",
    "                                                          )\n",
    "\n",
    "val\n",
    "weekday_surcharge: Double =\n",
    "if (isUSHolidayOrWeekend(\n",
    "  LocalDateTime.parse(row.getAs[String](headers.indexOf(pickup)).trim, formatter).toLocalDate)) 0 else weekDaySurcharge\n",
    "\n",
    "val colsToAdd: Seq[Double] = timezones.keys.toSeq.flatMap\n",
    "{tz = >\n",
    "val\n",
    "duration = timeZonesDuration.getOrElse(tz, 0.0)\n",
    "val\n",
    "totalDuration = row.getAs[Double](headers.indexOf(colDuration))\n",
    "Seq(duration, BigDecimal(duration * 100 / totalDuration).setScale(decimals, RoundingMode.HALF_UP).toDouble)\n",
    "}\n",
    "\n",
    "Row.fromSeq((row.toSeq + + colsToAdd): + weekday_surcharge)\n",
    "}\n",
    "}\n",
    "\n",
    "rdd = addTimeZones(rdd, timeZones, weekDaySurcharge, colDurationMinutes, decimals)\n",
    "\n",
    "val\n",
    "headersToAdd: Seq[String] = timeZones.keys.toSeq.flatMap\n",
    "{tz = >\n",
    "Seq(tz + \"_duration\", tz + \"_duration_pcg\")\n",
    "}:+ colWeekdaySurcharge\n",
    "\n",
    "headers = headers + + headersToAdd"
   ],
   "id": "9f3672bebd356114"
  },
  {
   "cell_type": "code",
   "id": "cfb99aef020692d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:43:00.672156Z",
     "start_time": "2025-05-09T14:42:52.971563Z"
    }
   },
   "source": [
    "import java.time.format.DateTimeFormatter\n",
    "import java.time.Duration\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.HashPartitioner\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "rdd.map {\n",
    "\n",
    "}\n",
    "rdd = rdd.map\n",
    "{row = >\n",
    "valformatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm[:ss]\")\n",
    "\n",
    "val\n",
    "pickupStr = row.getAs[String](headers.indexOf(pickup)).trim\n",
    "val\n",
    "dropoffStr = row.getAs[String](headers.indexOf(dropoff)).trim\n",
    "\n",
    "val\n",
    "pickupTS = LocalDateTime.parse(pickupStr, formatter)\n",
    "val\n",
    "dropoffTS = LocalDateTime.parse(dropoffStr, formatter)\n",
    "val\n",
    "durationMillis = Duration.between(pickupTS, dropoffTS).toMillis\n",
    "val\n",
    "durationMinutes = BigDecimal(durationMillis / 60000.0)\n",
    ".setScale(decimals, RoundingMode.HALF_UP)\n",
    ".toDouble\n",
    "\n",
    "val\n",
    "pickupYear = pickupTS.getYear\n",
    "\n",
    "Row.fromSeq(row.toSeq + + Seq(durationMinutes, pickupYear))\n",
    "}.filter\n",
    "{row = >\n",
    "row.getAs[Double](row.toSeq.length - 2) > 0.0\n",
    "}\n",
    "headers = headers + + Seq(colDurationMinutes, colYear)\n",
    "\n",
    "rdd = binColByStepValue(rdd, headers.indexOf(colDurationMinutes), 5)\n",
    "headers = headers:+ colDurationMinutesBinLabel\n",
    "\n",
    "rdd = rdd.map\n",
    "{row = >\n",
    "val\n",
    "formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm[:ss]\")\n",
    "\n",
    "val\n",
    "timeZonesDuration: Map[String, Double] = preciseBucketUDF(timeZones,\n",
    "                                                          LocalDateTime.parse(\n",
    "                                                              row.getAs[String](headers.indexOf(pickup)).trim,\n",
    "                                                              formatter),\n",
    "                                                          LocalDateTime.parse(\n",
    "                                                              row.getAs[String](headers.indexOf(dropoff)).trim,\n",
    "                                                              formatter)\n",
    "                                                          )\n",
    "\n",
    "val\n",
    "weekday_surcharge: Double =\n",
    "if (isUSHolidayOrWeekend(LocalDateTime.parse(row.getAs[String](headers.indexOf(pickup)).trim, formatter).toLocalDate)) 0\n",
    "else weekDaySurcharge\n",
    "\n",
    "val colsToAdd: Seq[Double] = timeZones.keys.toSeq.flatMap\n",
    "{tz = >\n",
    "val\n",
    "duration = timeZonesDuration.getOrElse(tz, 0.0)\n",
    "val\n",
    "totalDuration = row.getAs[Double](headers.indexOf(colDurationMinutes))\n",
    "Seq(duration, BigDecimal(duration * 100 / totalDuration).setScale(decimals, RoundingMode.HALF_UP).toDouble)\n",
    "}\n",
    "\n",
    "Row.fromSeq((row.toSeq + + colsToAdd): + weekday_surcharge)\n",
    "}\n",
    "\n",
    "val\n",
    "headersToAdd: Seq[String] = timeZones.keys.toSeq.flatMap\n",
    "{tz = >\n",
    "Seq(tz + \"_duration\", tz + \"_duration_pcg\")\n",
    "}:+ colWeekdaySurcharge\n",
    "headers = headers + + headersToAdd\n",
    "\n",
    "rdd = rdd.map\n",
    "{row = >\n",
    "val\n",
    "fees = colFees.filter(col= >\n",
    "       headers.contains(col.toLowerCase)).map(col= > row.getAs[Double](headers.indexOf(col.toLowerCase))).sum\n",
    "\n",
    "Row.fromSeq(row.toSeq: + fees)\n",
    "}\n",
    "headers = headers:+ colAggregateFee\n",
    "\n",
    "rdd = binColByStepValue(rdd, headers.indexOf(colAggregateFee), 2)\n",
    "headers = headers:+ colAggregateFeeBin\n",
    "\n",
    "rdd = rdd.map\n",
    "{row = >\n",
    "val\n",
    "pricePerTime = Math.round(row.getAs[Double](headers.indexOf(colFareAmount)) / row.getAs[Double](\n",
    "    headers.indexOf(colDurationMinutes)) * 100) / 100.0\n",
    "val\n",
    "pricePerDistance = Math.round(row.getAs[Double](headers.indexOf(colFareAmount)) / row.getAs[Double](\n",
    "    headers.indexOf(\"trip_distance\")) * 100) / 100.0\n",
    "\n",
    "Row.fromSeq(row.toSeq + + Seq(pricePerTime, pricePerDistance))\n",
    "}\n",
    "headers = headers + + Seq(colPricePerTime, colPricePerDistance)\n",
    "\n",
    "rdd = binColByStepValue(rdd, headers.indexOf(\"trip_distance\"), 5)\n",
    "headers = headers:+ colDistanceBin\n",
    "\n",
    "rdd = binColByStepValue(rdd, headers.indexOf(colDurationOvernightPcg), 5)\n",
    "headers = headers:+ (colDurationOvernightPcg + \"_label\")\n",
    "\n",
    "val\n",
    "actualHeader = headers\n",
    "val\n",
    "rddWithKey = rdd.map\n",
    "{row = >\n",
    "val\n",
    "key = colsForClassification\n",
    "      .filter(col= > actualHeader.contains(col.toLowerCase))\n",
    ".map(col= > row.get(actualHeader.indexOf(col.toLowerCase)))\n",
    ".mkString(\"_\")\n",
    "(key, row)\n",
    "}\n",
    "\n",
    "val\n",
    "numPartitions = spark.sparkContext.defaultParallelism\n",
    "val\n",
    "partitioner = new\n",
    "HashPartitioner(numPartitions)\n",
    "\n",
    "val\n",
    "rddWithKeyForAvg = rddWithKey\n",
    ".mapValues\n",
    "{row = >\n",
    "val\n",
    "costPerDistance = row.getAs[Double](headers.indexOf(colPricePerDistance))\n",
    "val\n",
    "costPerTime = row.getAs[Double](headers.indexOf(colPricePerTime))\n",
    "(costPerDistance, costPerTime, 1L)\n",
    "}\n",
    ".partitionBy(partitioner)\n",
    ".persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "val\n",
    "rddAvgPrices = rddWithKeyForAvg\n",
    ".aggregateByKey((0.0, 0.0, 0L))(\n",
    "    (acc, v) = > (acc._1 + v._1, acc._2 + v._2, acc._3 + v._3),\n",
    "(a, b) = > (a._1 + b._1, a._2 + b._2, a._3 + b._3)\n",
    ")\n",
    ".mapValues\n",
    "{case(sumDist, sumTime, count) = >\n",
    "val\n",
    "avgDist = BigDecimal(sumDist / count).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "val\n",
    "avgTime = BigDecimal(sumTime / count).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "(avgDist, avgTime)\n",
    "}\n",
    ".filter\n",
    "{case(_, (dist, time)) = > dist > 0.0 & & time > 0.0}\n",
    "\n",
    "rddWithKeyForAvg.unpersist()\n",
    "\n",
    "val\n",
    "broadcastAvgPrices: Broadcast[Map[String, (Double, Double)]] = spark.sparkContext.broadcast(\n",
    "    rddAvgPrices.collectAsMap().toMap)\n",
    "\n",
    "rdd = rddWithKey.flatMap\n",
    "{case(key, originalRow) = >\n",
    "broadcastAvgPrices.value.get(key).map\n",
    "{case(avgCostPerDistance, avgCostPerTime) = >\n",
    "Row.fromSeq(originalRow.toSeq + + Seq(avgCostPerDistance, avgCostPerTime))\n",
    "}\n",
    "}\n",
    "headers = headers + + Seq(colAvgPricePerDistance, colAvgPricePerTime)\n",
    "\n",
    "rdd = rdd.map\n",
    "{row = >\n",
    "val\n",
    "priceColsToAdd: Seq[Double] =\n",
    "Seq((colPricePerDistance, colAvgPricePerDistance), (colPricePerTime, colAvgPricePerTime))\n",
    ".flatMap\n",
    "{case(colPrice, colAvgPrice) = >\n",
    "val\n",
    "price = row.getAs[Double](headers.indexOf(colPrice))\n",
    "val\n",
    "priceAvg = row.getAs[Double](headers.indexOf(colAvgPrice))\n",
    "val\n",
    "priceDiff = BigDecimal(price - priceAvg).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "val\n",
    "priceDiffPcg = BigDecimal(priceDiff / priceAvg * 100).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "\n",
    "Seq(priceDiff, priceDiffPcg)\n",
    "}\n",
    "\n",
    "Row.fromSeq(row.toSeq + + priceColsToAdd)\n",
    "}\n",
    "headers = headers + + Seq(colPricePerDistanceDiff, colPricePerDistanceDiffPcg, colPricePerTimeDiff,\n",
    "                          colPricePerTimeDiffPcg)\n",
    "\n",
    "rdd = binColByStepValue(\n",
    "    binColByStepValue(\n",
    "        rdd,\n",
    "        headers.indexOf(colPricePerDistanceDiffPcg), 5),\n",
    "    headers.indexOf(colPricePerTimeDiffPcg), 5\n",
    ")\n",
    "\n",
    "headers = headers + + Seq(colPricePerDistanceDiffPcgLabel, colPricePerTimeDiffPcgLabel)\n",
    "\n",
    "val\n",
    "headersForAnalysis = headers.filter(head= > colsForClassification.contains(head.toLowerCase))\n",
    "val\n",
    "rddForAnalysis = rdd.map\n",
    "{row = >\n",
    "Row.fromSeq(headersForAnalysis.map(head= > row.get(headers.indexOf(head))))\n",
    "}\n",
    "val\n",
    "totalCount = rddForAnalysis.count()\n",
    "\n",
    "val\n",
    "rddFeatures = colsForValuesAnalysis.map\n",
    "{colName = >\n",
    "val\n",
    "groupCols = Seq(colPricePerDistanceDiffPcgLabel, colPricePerTimeDiffPcgLabel):+ colName\n",
    "\n",
    "val\n",
    "grouped = rddForAnalysis\n",
    ".map\n",
    "{row = >\n",
    "val\n",
    "key = groupCols.map(col= > {row.get(headersForAnalysis.indexOf(col.toLowerCase))})\n",
    "(key, 1)\n",
    "}\n",
    ".reduceByKey(_ + _)\n",
    ".map\n",
    "{case(keySeq, count) = >\n",
    "val\n",
    "value = keySeq.last.toString\n",
    "val\n",
    "costDistLabel = keySeq(0).toString\n",
    "val\n",
    "costTimeLabel = keySeq(1).toString\n",
    "val\n",
    "pcg = BigDecimal(count.toDouble / totalCount * 100).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "Row.fromSeq(Seq(colName, value, count, pcg, costDistLabel, costTimeLabel))\n",
    "}\n",
    "grouped\n",
    "}\n",
    "\n",
    "val\n",
    "headersForSchema = Seq(\n",
    "    StructField(\"feature\", StringType),\n",
    "    StructField(\"value\", StringType),\n",
    "    StructField(\"count\", IntegerType),\n",
    "    StructField(\"pcg\", DoubleType),\n",
    "    StructField(\"cost_distance_label\", StringType),\n",
    "    StructField(\"cost_time_label\", StringType)\n",
    ")\n",
    "\n",
    "val\n",
    "schema = StructType(headersForSchema)\n",
    "\n",
    "val\n",
    "dfForAnalysis = spark.createDataFrame(rddFeatures.reduce(_\n",
    "union\n",
    "_), schema)\n",
    "\n",
    "dfForAnalysis.write.mode(\"overwrite\").parquet(getDatasetPath(outputDir + f\"/$name\"))"
   ],
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Task not serializable",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Task not serializable",
      "  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:444)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:416)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2674)",
      "  at org.apache.spark.rdd.RDD.$anonfun$map$1(RDD.scala:418)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)",
      "  at org.apache.spark.rdd.RDD.map(RDD.scala:417)",
      "  ... 39 elided",
      "Caused by: java.io.NotSerializableException: org.apache.spark.SparkContext",
      "Serialization stack:",
      "\t- object not serializable (class: org.apache.spark.SparkContext, value: org.apache.spark.SparkContext@5e3776b8)",
      "\t- field (class: $iw, name: sc, type: class org.apache.spark.SparkContext)",
      "\t- object (class $iw, $iw@4c530d21)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@496e969)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@69029c44)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@11f79585)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@53c1327e)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@1bb9874e)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@3bb055f9)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@14a8e287)",
      "\t- field (class: $read, name: $iw, type: class $iw)",
      "\t- object (class $read, $read@2cc4aa03)",
      "\t- field (class: $iw, name: read, type: class $read)",
      "\t- object (class $iw, $iw@67cb0481)",
      "\t- field (class: $iw, name: $outer, type: class $iw)",
      "\t- object (class $iw, $iw@4dae3e21)",
      "\t- element of array (index: 0)",
      "\t- array (class [Ljava.lang.Object;, size 1)",
      "\t- field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;)",
      "\t- object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class $iw, functionalInterfaceMethod=scala/Function1.apply:(Ljava/lang/Object;)Ljava/lang/Object;, implementation=invokeStatic $anonfun$rdd$1:(L$iw;Lorg/apache/spark/sql/Row;)Lorg/apache/spark/sql/Row;, instantiatedMethodType=(Lorg/apache/spark/sql/Row;)Lorg/apache/spark/sql/Row;, numCaptured=1])",
      "\t- writeReplace data (class: java.lang.invoke.SerializedLambda)",
      "\t- object (class $Lambda$4096/0x0000000801870840, $Lambda$4096/0x0000000801870840@74e9f7d0)",
      "  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)",
      "  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:49)",
      "  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)",
      "  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:441)",
      "  ... 47 more",
      ""
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a05a60d734daf56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
