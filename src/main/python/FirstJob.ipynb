{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cefe988a9aff7538",
   "metadata": {},
   "source": [
    "# Big Data project A.Y. 2024-2025 - First Job\n",
    "\n",
    "## Members\n",
    "\n",
    "- Giovanni Antonioni\n",
    "- Luca Rubboli - 0001083742"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce3e263b662de6",
   "metadata": {},
   "source": [
    "### Define useful parameters\n",
    "\n",
    "- Dataset location\n",
    "- Iterator (defined like this to overcome different names for same columns in dataset)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c9c769832012f1b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:02.182862Z",
     "start_time": "2025-05-13T16:55:46.283979Z"
    }
   },
   "source": [
    "import org.apache.spark\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder\n",
    "  .appName(\"First job\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val sc = spark.sparkContext"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.201.106.166:4040\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1747155350627)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\n",
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5e2b1d9a\n",
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@2d11f91c\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:04.567548Z",
     "start_time": "2025-05-13T16:56:02.192439Z"
    }
   },
   "source": [
    "val decimals: Int = 4\n",
    "val datasetDir = \"dataset\"\n",
    "val outputDir = \"output/firstJobOutput\"\n",
    "val yellowDatasetDir = s\"$datasetDir/yellow_cab\"\n",
    "val greenDatasetDir = s\"$datasetDir/green_cab\"\n",
    "val fhvDatasetDir = s\"$datasetDir/fhv_cab\"\n",
    "val fhvhvDatasetDir = s\"$datasetDir/fhvhv_cab\"\n",
    "val datasetDirMap: Map[String, String] = Map(\"yellow\" -> yellowDatasetDir, \"green\" -> greenDatasetDir, \"fhv\" -> fhvDatasetDir, \"fhvhv\" -> fhvhvDatasetDir)\n",
    "val datasetIterator: Iterable[(String, String, String)] = Seq(\n",
    "  (\"yellow\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\"),\n",
    "  (\"green\", \"lpep_dropoff_datetime\", \"lpep_pickup_datetime\"),\n",
    "  // (\"fhv\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\"),\n",
    "  // (\"fhvhv\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\"),\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decimals: Int = 4\n",
       "datasetDir: String = dataset\n",
       "outputDir: String = output/firstJobOutput\n",
       "yellowDatasetDir: String = dataset/yellow_cab\n",
       "greenDatasetDir: String = dataset/green_cab\n",
       "fhvDatasetDir: String = dataset/fhv_cab\n",
       "fhvhvDatasetDir: String = dataset/fhvhv_cab\n",
       "datasetDirMap: Map[String,String] = Map(yellow -> dataset/yellow_cab, green -> dataset/green_cab, fhv -> dataset/fhv_cab, fhvhv -> dataset/fhvhv_cab)\n",
       "datasetIterator: Iterable[(String, String, String)] = List((yellow,tpep_dropoff_datetime,tpep_pickup_datetime), (green,lpep_dropoff_datetime,lpep_pickup_datetime))\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "801ff1e945bd4c8",
   "metadata": {},
   "source": [
    "## Define Columns for analysis\n",
    "- Columns names\n",
    "- Time zones for overprice\n",
    "- Columns used in classification for average price calculation\n",
    "- Columns which values are used in analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "bfdb3f87a1c6a6c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:05.712186Z",
     "start_time": "2025-05-13T16:56:04.572211Z"
    }
   },
   "source": [
    "val colDurationMinutes: String = \"duration_minutes\"\n",
    "val colDurationMinutesBinLabel: String = \"duration_minutes_bin_label\"\n",
    "val colYear: String = \"year\"\n",
    "val colWeekdaySurcharge: String = \"weekday_surcharge\"\n",
    "val colAggregateFee: String = \"fees\"\n",
    "val colAggregateFeeBin: String = \"agg_fee_bin_label\"\n",
    "val colDistanceBin: String = \"distance_bin_label\"\n",
    "val colFareAmount: String = \"fare_amount\"\n",
    "val colPricePerDistance: String = \"cost_per_distance\"\n",
    "val colPricePerTime: String = \"cost_per_time\"\n",
    "val colAvgPricePerDistance: String = \"avg_cost_per_distance\"\n",
    "val colAvgPricePerTime: String = \"avg_cost_per_time\"\n",
    "val colPricePerDistanceDiff: String = \"cost_per_distance_diff\"\n",
    "val colPricePerDistanceDiffPcg: String = \"cost_per_distance_diff_pcg\"\n",
    "val colPricePerTimeDiff: String = \"cost_per_time_diff\"\n",
    "val colPricePerTimeDiffPcg: String = \"cost_per_time_diff_pcg\"\n",
    "val colPricePerDistanceDiffPcgLabel: String = colPricePerDistanceDiffPcg + \"_label\"\n",
    "val colPricePerTimeDiffPcgLabel: String = colPricePerTimeDiffPcg + \"_label\"\n",
    "\n",
    "val timeZoneOver: String = \"overnight\"\n",
    "val timeZones = Map(timeZoneOver -> (20, 6), \"regular\" -> (6, 20))\n",
    "val weekDaySurcharge: Double = 2.5\n",
    "\n",
    "val colDurationOvernightPcg: String = s\"${timeZoneOver}_duration_pcg\"\n",
    "\n",
    "val colToUse: Set[String] = Set(\n",
    "  \"tpep_pickup_datetime\",\n",
    "  \"tpep_dropoff_datetime\",\n",
    "  \"lpep_pickup_datetime\",\n",
    "  \"lpep_dropoff_datetime\",\n",
    "  \"passenger_count\",\n",
    "  \"trip_distance\",\n",
    "  \"ratecodeid\",\n",
    "  \"store_and_fwd_flag\",\n",
    "  \"payment_type\",\n",
    "  \"fare_amount\",\n",
    "  \"extra\",\n",
    "  \"mta_tax\",\n",
    "  \"tip_amount\",\n",
    "  \"tolls_amount\",\n",
    "  \"improvement_surcharge\",\n",
    "  \"total_amount\",\n",
    "  \"congestion_surcharge\",\n",
    "  \"airport_fee\")\n",
    "\n",
    "val colFees: Set[String] = Set(\n",
    "  \"extra\",\n",
    "  \"mta_tax\",\n",
    "  \"improvement_surcharge\",\n",
    "  \"congestion_surcharge\",\n",
    "  \"airport_fee\")\n",
    "\n",
    "val colsForClassification: Seq[String] = Seq(\n",
    "  \"passenger_count\",\n",
    "  \"store_and_fwd_flag\",\n",
    "  \"payment_type\",\n",
    "  colAggregateFeeBin,\n",
    "  colDurationMinutesBinLabel,\n",
    "  colDistanceBin,\n",
    "  colYear,\n",
    "  s\"${colDurationOvernightPcg}_label\",\n",
    "  colPricePerDistanceDiffPcgLabel,\n",
    "  colPricePerTimeDiffPcgLabel\n",
    ")\n",
    "\n",
    "val colsForValuesAnalysis: Seq[String] = Seq(\n",
    "  \"passenger_count\",\n",
    "  \"store_and_fwd_flag\",\n",
    "  \"payment_type\",\n",
    "  colAggregateFeeBin,\n",
    "  colDurationMinutesBinLabel,\n",
    "  colDistanceBin,\n",
    "  colYear,\n",
    "  s\"${colDurationOvernightPcg}_label\",\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colDurationMinutes: String = duration_minutes\n",
       "colDurationMinutesBinLabel: String = duration_minutes_bin_label\n",
       "colYear: String = year\n",
       "colWeekdaySurcharge: String = weekday_surcharge\n",
       "colAggregateFee: String = fees\n",
       "colAggregateFeeBin: String = agg_fee_bin_label\n",
       "colDistanceBin: String = distance_bin_label\n",
       "colFareAmount: String = fare_amount\n",
       "colPricePerDistance: String = cost_per_distance\n",
       "colPricePerTime: String = cost_per_time\n",
       "colAvgPricePerDistance: String = avg_cost_per_distance\n",
       "colAvgPricePerTime: String = avg_cost_per_time\n",
       "colPricePerDistanceDiff: String = cost_per_distance_diff\n",
       "colPricePerDistanceDiffPcg: String = cost_per_distance_diff_pcg\n",
       "colPricePerTimeDiff: String = cost_per_time_diff\n",
       "colPricePerTimeDiffPcg: String = cost_per_time_diff_pcg\n",
       "colPricePerDistanceDiffPcgLabel: String = ...\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "6251e1a01e2d34a6",
   "metadata": {},
   "source": [
    "### Define preprocess rules"
   ]
  },
  {
   "cell_type": "code",
   "id": "f3c66b94d590de92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:06.784071Z",
     "start_time": "2025-05-13T16:56:05.716529Z"
    }
   },
   "source": [
    "val featureFilters: Map[String, Any => Boolean] = Map(\n",
    "  \"passenger_count\" -> {\n",
    "    case i: Int => i > 0\n",
    "    case f: Float => val i = f.toInt; i > 0\n",
    "    case d: Double => val i = d.toInt; i > 0\n",
    "    case _ => false\n",
    "  },\n",
    "  \"trip_distance\" -> {\n",
    "    case i: Int => i > 0\n",
    "    case i: Float => i > 0\n",
    "    case i: Double => i > 0\n",
    "    case _ => false\n",
    "  },\n",
    "  \"ratecodeid\" -> {\n",
    "    case i: Int => (i >= 1 && i <= 6) || i == 99\n",
    "    case f: Float => val i = f.toInt; (i >= 1 && i <= 6) || i == 99\n",
    "    case d: Double => val i = d.toInt; (i >= 1 && i <= 6) || i == 99\n",
    "    case _ => false\n",
    "  },\n",
    "  \"store_and_fwd_flag\" -> {\n",
    "    case i: String => i == \"Y\" || i == \"N\"\n",
    "    case _ => false\n",
    "  },\n",
    "  \"payment_type\" -> {\n",
    "    case i: Int => i >= 1 && i <= 6\n",
    "    case f: Float => val i = f.toInt; i >= 1 && i <= 6\n",
    "    case d: Double => val i = d.toInt; i >= 1 && i <= 6\n",
    "    case _ => false\n",
    "  },\n",
    "  \"fare_amount\" -> {\n",
    "    case i: Int => i > 0\n",
    "    case i: Float => i > 0\n",
    "    case i: Double => i > 0\n",
    "    case _ => false\n",
    "  },\n",
    "  \"tolls_amount\" -> {\n",
    "    case i: Int => i >= 0 && i < 200\n",
    "    case i: Float => i >= 0 && i < 200\n",
    "    case i: Double => i >= 0 && i < 200\n",
    "    case _ => false\n",
    "  }\n",
    ")\n",
    "\n",
    "val taxFilter: Any => Boolean = {\n",
    "  case tax: Int => tax >= 0 && tax < 20\n",
    "  case tax: Float => tax >= 0 && tax < 20\n",
    "  case tax: Double => tax >= 0 && tax < 20\n",
    "  case _ => false\n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureFilters: Map[String,Any => Boolean] = Map(trip_distance -> $Lambda$2372/0x0000000801086040@6445100d, tolls_amount -> $Lambda$2377/0x000000080108a040@666ac400, payment_type -> $Lambda$2375/0x0000000801088840@6ae6cbce, fare_amount -> $Lambda$2376/0x0000000801089040@51b2ae58, passenger_count -> $Lambda$2371/0x0000000801085840@1dbb191b, store_and_fwd_flag -> $Lambda$2374/0x0000000801087840@2dc18468, ratecodeid -> $Lambda$2373/0x0000000801087040@71a592a5)\n",
       "taxFilter: Any => Boolean = $Lambda$2378/0x000000080108a840@35a132a7\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "46150dfaa1006964",
   "metadata": {},
   "source": [
    "### Utils functions for rdd"
   ]
  },
  {
   "cell_type": "code",
   "id": "9aae38423ac3200c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:08.643734Z",
     "start_time": "2025-05-13T16:56:06.788163Z"
    }
   },
   "source": [
    "import java.time.temporal.ChronoUnit\n",
    "import java.time.{DayOfWeek, LocalDate, LocalDateTime}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Row\n",
    "import scala.math.BigDecimal.RoundingMode\n",
    "\n",
    "\n",
    "def binColByStepValue(rdd: RDD[Row], indexOfColToDiscrete: Int, stepValue: Int = 5): RDD[Row] = {\n",
    "  rdd.map { row =>\n",
    "    val value: Double = row.get(indexOfColToDiscrete) match {\n",
    "      case i: Int => i.toDouble\n",
    "      case d: Double => d\n",
    "      case l: Long => l.toDouble\n",
    "      case s: String => try { s.toDouble } catch { case _: Throwable => Double.NaN}\n",
    "      case _ => Double.NaN\n",
    "    }\n",
    "\n",
    "    val rawBin = (value / stepValue).toInt * stepValue\n",
    "    val binBase = if (value < 0 && value % stepValue == 0) rawBin + stepValue else rawBin\n",
    "    val label = if (value < 0) { s\"[${(binBase - stepValue).toInt}|${binBase.toInt})\" } else { s\"[${binBase.toInt}|${(binBase + stepValue).toInt})\" }\n",
    "\n",
    "    Row.fromSeq(row.toSeq :+ label)\n",
    "  }\n",
    "}\n",
    "\n",
    "val castForFilter: Any => Any = {\n",
    "  case s: String => if (s.matches(\"\"\"^-?\\d+\\.\\d+$\"\"\")) s.toDouble else if (s.matches(\"\"\"^-?\\d+$\"\"\")) s.toInt else s.trim\n",
    "  case d: Double => d\n",
    "  case i: Int => i\n",
    "  case l: Long => l.toDouble\n",
    "  case f: Float => f.toDouble\n",
    "  case b: Boolean => b\n",
    "  case null => null\n",
    "  case other => other.toString.trim\n",
    "}\n",
    "\n",
    "val preciseBucketUDF: (Map[String, (Int, Int)], LocalDateTime, LocalDateTime, Int) => Map[String, Double] = { (timeZones: Map[String, (Int, Int)], start: LocalDateTime, end: LocalDateTime, decimals: Int) =>\n",
    "\n",
    "  val overlap: (LocalDateTime, LocalDateTime, LocalDateTime, LocalDateTime, Int) => Double = { (start1: LocalDateTime, end1: LocalDateTime, start2: LocalDateTime, end2: LocalDateTime, decimals: Int) =>\n",
    "    val overlapStart = if (start1.isAfter(start2)) start1 else start2\n",
    "    val overlapEnd = if (end1.isBefore(end2)) end1 else end2\n",
    "    if (overlapEnd.isAfter(overlapStart)) BigDecimal(ChronoUnit.MILLIS.between(overlapStart, overlapEnd) / 60000.0).setScale(decimals, RoundingMode.HALF_UP).toDouble else 0.0\n",
    "  }\n",
    "\n",
    "  var result = timeZones.keys.map(_ -> 0.0).toMap\n",
    "\n",
    "  if (!(start == null || end == null)) {\n",
    "\n",
    "    if (!end.isBefore(start)) {\n",
    "\n",
    "      var current = start.toLocalDate.atStartOfDay\n",
    "\n",
    "      while (!current.isAfter(end)) {\n",
    "        val nextDay = current.plusDays(1)\n",
    "\n",
    "        timeZones.foreach {\n",
    "          case (label, (startHour, endHour)) if startHour > endHour =>\n",
    "            val bucketStartBeforeMidnight = current.withHour(startHour).withMinute(0).withSecond(0).withNano(0)\n",
    "            val bucketEndBeforeMidnight = current.withHour(23).withMinute(59).withSecond(59)\n",
    "            val bucketStartAfterMidnight = current.withHour(0).withMinute(0).withSecond(0).withNano(0)\n",
    "            val bucketEndAfterMidnight = current.withHour(endHour).withMinute(0).withSecond(0).withNano(0)\n",
    "\n",
    "            val minutesBeforeMidnight = overlap(start, end, bucketStartBeforeMidnight, bucketEndBeforeMidnight, decimals)\n",
    "            val minutesAfterMidnight = overlap(start, end, bucketStartAfterMidnight, bucketEndAfterMidnight, decimals)\n",
    "\n",
    "            result = result.updated(label, result(label) + minutesBeforeMidnight + minutesAfterMidnight)\n",
    "\n",
    "          case (label, (startHour, endHour)) =>\n",
    "            val bucketStart = current.withHour(startHour).withMinute(0).withSecond(0).withNano(0)\n",
    "            val bucketEnd = if (endHour == 24) current.plusDays(1).withHour(0).withMinute(0).withSecond(0).withNano(0) else current.withHour(endHour).withMinute(0).withSecond(0).withNano(0)\n",
    "\n",
    "            val minutes = overlap(start, end, bucketStart, bucketEnd, decimals)\n",
    "\n",
    "            result = result.updated(label, result(label) + minutes)\n",
    "        }\n",
    "\n",
    "        current = nextDay\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  result\n",
    "}\n",
    "\n",
    "val isUSHolidayOrWeekend: LocalDate => Boolean = { date =>\n",
    "  val month = date.getMonthValue\n",
    "  val day = date.getDayOfMonth\n",
    "  val dayOfWeek = date.getDayOfWeek\n",
    "\n",
    "  val isIndependenceDay = month == 7 && day == 4\n",
    "  val isChristmas = month == 12 && day == 25\n",
    "  val isNewYear = month == 1 && day == 1\n",
    "  val isLaborDay = month == 9 && dayOfWeek == DayOfWeek.MONDAY && day <= 7\n",
    "\n",
    "  val isThanksgiving = month == 11 && dayOfWeek == DayOfWeek.THURSDAY && day >= 22 && day <= 28 && ((day - 1) / 7 + 1 == 4)\n",
    "\n",
    "  isIndependenceDay || isChristmas || isNewYear || isLaborDay || isThanksgiving || dayOfWeek == DayOfWeek.SATURDAY || dayOfWeek == DayOfWeek.SUNDAY\n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.temporal.ChronoUnit\n",
       "import java.time.{DayOfWeek, LocalDate, LocalDateTime}\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.sql.Row\n",
       "import scala.math.BigDecimal.RoundingMode\n",
       "binColByStepValue: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], indexOfColToDiscrete: Int, stepValue: Int)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "castForFilter: Any => Any = $Lambda$2515/0x00000008010f6840@3e1ad086\n",
       "preciseBucketUDF: (Map[String,(Int, Int)], java.time.LocalDateTime, java.time.LocalDateTime, Int) => Map[String,Double] = $Lambda$2516/0x00000008010f7040@60345872\n",
       "isUSHolidayOrWeekend: java.time.LocalDate => Boolean = $Lambda$2517/0x00000008010f7840@1555ca19\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "57c2f73d1187fa8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:08.922145Z",
     "start_time": "2025-05-13T16:56:08.648231Z"
    }
   },
   "source": [
    "val projectDir: String = \"/Users/luca/Desktop/Luca/Università/Magistrale/Corsi/BigData/Drivers\"\n",
    "\n",
    "def getDatasetPath(localPath: String): String = {\n",
    "  \"file://\" + projectDir + \"/\" + localPath\n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "projectDir: String = /Users/luca/Desktop/Luca/Università/Magistrale/Corsi/BigData/Drivers\n",
       "getDatasetPath: (localPath: String)String\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "d67c5a94825ecee9",
   "metadata": {},
   "source": [
    "# Actual job\n",
    "\n",
    "1) Select dataset, dropoff and pickup columns"
   ]
  },
  {
   "cell_type": "code",
   "id": "60aee06709329346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:09.131933Z",
     "start_time": "2025-05-13T16:56:08.926177Z"
    }
   },
   "source": [
    "val name: String = \"green\"\n",
    "val dropoff: String = \"lpep_dropoff_datetime\"\n",
    "val pickup: String = \"lpep_pickup_datetime\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: String = green\n",
       "dropoff: String = lpep_dropoff_datetime\n",
       "pickup: String = lpep_pickup_datetime\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2) Load dataset",
   "id": "6e3fcaeb36518cd9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:12.759329Z",
     "start_time": "2025-05-13T16:56:09.135909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val dataset = spark.read.parquet(getDatasetPath(datasetDirMap(name)))\n",
    "var headers: Seq[String] = dataset.columns.map(_.toLowerCase)\n",
    "val indexesToUse: Seq[Int] = headers.zipWithIndex.collect {\n",
    "  case(h, i) if colToUse.contains(h.toLowerCase) => i\n",
    "}"
   ],
   "id": "a846e646fac3bac4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset: org.apache.spark.sql.DataFrame = [VendorID: int, lpep_pickup_datetime: timestamp_ntz ... 18 more fields]\n",
       "headers: Seq[String] = ArraySeq(vendorid, lpep_pickup_datetime, lpep_dropoff_datetime, store_and_fwd_flag, ratecodeid, pulocationid, dolocationid, passenger_count, trip_distance, fare_amount, extra, mta_tax, tip_amount, tolls_amount, ehail_fee, improvement_surcharge, total_amount, payment_type, trip_type, congestion_surcharge)\n",
       "indexesToUse: Seq[Int] = ArraySeq(1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 19)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:14.534492Z",
     "start_time": "2025-05-13T16:56:12.763451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import java.io._\n",
    "\n",
    "def isSerializable(obj: Any): Boolean = {\n",
    "  try {\n",
    "    val bos = new ByteArrayOutputStream()\n",
    "    val out = new ObjectOutputStream(bos)\n",
    "    out.writeObject(obj)\n",
    "    out.close()\n",
    "    true\n",
    "  } catch {\n",
    "    case e: Exception =>\n",
    "    println(s\"Serialization failed: ${e}\")\n",
    "    e.printStackTrace()\n",
    "    false\n",
    "  }\n",
    "}"
   ],
   "id": "1a97369378a2afe5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.io._\n",
       "isSerializable: (obj: Any)Boolean\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filter taxes and features based on filter conditions previously defined",
   "id": "df209b8e24cd13e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:16.471382Z",
     "start_time": "2025-05-13T16:56:14.538481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def transformRDD(dataset: DataFrame, headers: Seq[String], idxs: Seq[Int], castFunc: Any => Any): RDD[Row] = {\n",
    "  dataset.rdd.map(row => Row.fromSeq(idxs.map(row.get).map(castFunc)))\n",
    "}\n",
    "\n",
    "var rdd = transformRDD(dataset, headers, indexesToUse, castForFilter)\n",
    "headers = headers.filter(head => colToUse.contains(head.toLowerCase))\n",
    "\n",
    "def applyFilters(rdd: RDD[Row], headers: Seq[String], colOfFees: Set[String], taxFilter: Any => Boolean, featFilter: Map[String, Any => Boolean]): RDD[Row] = {\n",
    "  rdd.filter { row =>\n",
    "    headers.zip(row.toSeq).forall {case(header: String, value) =>\n",
    "      val taxFilterCondition = if (colOfFees.contains(header.toLowerCase)) taxFilter(value) else true\n",
    "      featFilter.get(header.toLowerCase) match {\n",
    "        case Some(filterFunc) => taxFilterCondition && filterFunc(value)\n",
    "        case None => taxFilterCondition // no filter defined for this column, so accept it\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "rdd = applyFilters(rdd, headers, colFees, taxFilter, featureFilters)"
   ],
   "id": "f8a8a0eaeaa2d294",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "transformRDD: (dataset: org.apache.spark.sql.DataFrame, headers: Seq[String], idxs: Seq[Int], castFunc: Any => Any)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[9] at filter at <console>:51\n",
       "headers: Seq[String] = ArraySeq(lpep_pickup_datetime, lpep_dropoff_datetime, store_and_fwd_flag, ratecodeid, passenger_count, trip_distance, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, payment_type, congestion_surcharge)\n",
       "applyFilters: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], headers: Seq[String], colOfFees: Set[String], taxFilter: Any => Boolean, featFilter: Map[String,Any => Boolean])org.apache.spark.rdd.RDD[org.apache....\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Add duration and timezones",
   "id": "523a6cb64b4c03d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:19.649548Z",
     "start_time": "2025-05-13T16:56:16.475853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import java.time.format.DateTimeFormatter\n",
    "import java.time.Duration\n",
    "\n",
    "def addDuration(rdd: RDD[Row], headers: Seq[String], pickup: String, dropoff: String, decimals: Int): RDD[Row] = {\n",
    "  rdd.map {row =>\n",
    "    val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm[:ss]\")\n",
    "\n",
    "    val pickupStr = row.getAs[String](headers.indexOf(pickup)).trim\n",
    "    val dropoffStr = row.getAs[String](headers.indexOf(dropoff)).trim\n",
    "\n",
    "    val pickupTS = LocalDateTime.parse(pickupStr, formatter)\n",
    "    val dropoffTS = LocalDateTime.parse(dropoffStr, formatter)\n",
    "    val durationMillis = Duration.between(pickupTS, dropoffTS).toMillis\n",
    "    val durationMinutes = BigDecimal(durationMillis / 60000.0).setScale(decimals, RoundingMode.HALF_UP).toDouble\n",
    "\n",
    "    val pickupYear = pickupTS.getYear\n",
    "\n",
    "    Row.fromSeq(row.toSeq ++ Seq(durationMinutes, pickupYear))\n",
    "  }.filter { row => row.getAs[Double](row.toSeq.length - 2) > 0.0 }\n",
    "}\n",
    "\n",
    "rdd = addDuration(rdd, headers, pickup, dropoff, decimals)\n",
    "headers = headers ++ Seq(colDurationMinutes, colYear)\n",
    "\n",
    "rdd = binColByStepValue(rdd, headers.indexOf(colDurationMinutes), 5)\n",
    "headers = headers :+ colDurationMinutesBinLabel\n",
    "\n",
    "def addTimeZones(rdd: RDD[Row], headers: Seq[String], timezones: Map[String, (Int, Int)], weekDaySurcharge: Double, colDuration: String, pickup: String, dropoff: String, decimals: Int, preciseBucketUDF: (Map[String, (Int, Int)], LocalDateTime, LocalDateTime, Int) => Map[String, Double], isUSHolidayOrWeekendTZ: LocalDate => Boolean): RDD[Row] = {\n",
    "  rdd.map { row =>\n",
    "    val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm[:ss]\")\n",
    "\n",
    "    val timeZonesDuration: Map[String, Double] = preciseBucketUDF(timezones, LocalDateTime.parse(row.getAs[String](headers.indexOf(pickup)).trim, formatter), LocalDateTime.parse(row.getAs[String](headers.indexOf(dropoff)).trim, formatter), decimals)\n",
    "\n",
    "    val weekday_surcharge: Double = if (isUSHolidayOrWeekendTZ(LocalDateTime.parse(row.getAs[String](headers.indexOf(pickup)).trim, formatter).toLocalDate)) 0 else weekDaySurcharge\n",
    "    val colsToAdd: Seq[Double] = timezones.keys.toSeq.flatMap { tz =>\n",
    "      val duration = timeZonesDuration.getOrElse(tz, 0.0)\n",
    "      val totalDuration = row.getAs[Double](headers.indexOf(colDuration))\n",
    "      Seq(duration, BigDecimal(duration * 100 / totalDuration).setScale(decimals, RoundingMode.HALF_UP).toDouble)\n",
    "    }\n",
    "    Row.fromSeq((row.toSeq ++ colsToAdd) :+ weekday_surcharge)\n",
    "  }\n",
    "}\n",
    "\n",
    "rdd = addTimeZones(rdd, headers, timeZones, weekDaySurcharge, colDurationMinutes, pickup, dropoff, decimals, preciseBucketUDF, isUSHolidayOrWeekend)\n",
    "\n",
    "val headersToAdd: Seq[String] = timeZones.keys.toSeq.flatMap { tz =>\n",
    "  Seq(tz + \"_duration\", tz + \"_duration_pcg\")\n",
    "} :+ colWeekdaySurcharge\n",
    "\n",
    "headers = headers ++ headersToAdd\n",
    "\n",
    "rdd.take(5).foreach(println)"
   ],
   "id": "9f3672bebd356114",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-01T00:07:08,2024-05-01T00:15:03,N,1.0,1.0,1.24,9.3,1.0,0.5,2.0,0.0,1.0,13.8,1.0,0.0,7.9167,2024.0,[5|10),7.9167,100.0,0.0,0.0,2.5]\n",
      "[2024-05-01T00:30:48,2024-05-01T00:35:49,N,1.0,1.0,0.94,7.2,1.0,0.5,1.94,0.0,1.0,11.64,1.0,0.0,5.0167,2024.0,[5|10),5.0167,100.0,0.0,0.0,2.5]\n",
      "[2024-05-01T00:34:13,2024-05-01T00:38:07,N,1.0,1.0,0.84,6.5,1.0,0.5,0.0,0.0,1.0,9.0,2.0,0.0,3.9,2024.0,[0|5),3.9,100.0,0.0,0.0,2.5]\n",
      "[2024-05-01T00:58:01,2024-05-01T01:14:41,N,1.0,1.0,6.07,25.4,1.0,0.5,5.0,0.0,1.0,32.9,1.0,0.0,16.6667,2024.0,[15|20),16.6667,100.0,0.0,0.0,2.5]\n",
      "[2024-05-01T00:11:45,2024-05-01T00:20:38,N,1.0,2.0,2.06,12.1,1.0,0.5,2.92,0.0,1.0,17.52,1.0,0.0,8.8833,2024.0,[5|10),8.8833,100.0,0.0,0.0,2.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import java.time.format.DateTimeFormatter\n",
       "import java.time.Duration\n",
       "addDuration: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], headers: Seq[String], pickup: String, dropoff: String, decimals: Int)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[13] at map at <console>:76\n",
       "headers: Seq[String] = ArraySeq(lpep_pickup_datetime, lpep_dropoff_datetime, store_and_fwd_flag, ratecodeid, passenger_count, trip_distance, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, payment_type, congestion_surcharge, duration_minutes, year, duration_minutes_bin_label, overnight_duration, overnight_duration_pcg, regular_duration, regular_duration_pcg, weekday_surcharge)\n",
       "rdd: org.apa...\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Add Aggregate fees and bins",
   "id": "135d87a329e10208"
  },
  {
   "cell_type": "code",
   "id": "cfb99aef020692d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:20.380950Z",
     "start_time": "2025-05-13T16:56:19.657215Z"
    }
   },
   "source": [
    "def addAggregateFees(rdd: RDD[Row], headers: Seq[String], colOfFees: Set[String]): RDD[Row] = {\n",
    "  rdd.map { row =>\n",
    "    val fees = colOfFees\n",
    "      .filter(col => headers.contains(col.toLowerCase))\n",
    "      .map(col => row.getAs[Double](headers.indexOf(col.toLowerCase))).sum\n",
    "\n",
    "    Row.fromSeq(row.toSeq :+ fees)\n",
    "  }\n",
    "}\n",
    "\n",
    "rdd = addAggregateFees(rdd, headers, colFees)\n",
    "headers = headers :+ colAggregateFee\n",
    "\n",
    "rdd = binColByStepValue(rdd, headers.indexOf(colAggregateFee), 2)\n",
    "headers = headers :+ colAggregateFeeBin"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addAggregateFees: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], headers: Seq[String], colOfFees: Set[String])org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[15] at map at <console>:35\n",
       "headers: Seq[String] = ArraySeq(lpep_pickup_datetime, lpep_dropoff_datetime, store_and_fwd_flag, ratecodeid, passenger_count, trip_distance, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, payment_type, congestion_surcharge, duration_minutes, year, duration_minutes_bin_label, overnight_duration, overnight_duration_pcg, regular_duration, regular_duration_pcg, weekday_surcharge, fees, agg_fee_bin_label)\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD...\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Add price per mile and minute",
   "id": "2612d914aaba250d"
  },
  {
   "cell_type": "code",
   "id": "7a05a60d734daf56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:20.740422Z",
     "start_time": "2025-05-13T16:56:20.386092Z"
    }
   },
   "source": [
    "def addPricePerDistanceAndTime(rdd: RDD[Row], headers: Seq[String], colFareAmount: String, colDuration: String, colDistance: String): RDD[Row] = {\n",
    "  rdd.map { row =>\n",
    "    val pricePerTime = Math.round(row.getAs[Double](headers.indexOf(colFareAmount)) / row.getAs[Double](headers.indexOf(colDuration)) * 100) / 100.0\n",
    "    val pricePerDistance = Math.round(row.getAs[Double](headers.indexOf(colFareAmount)) / row.getAs[Double](headers.indexOf(colDistance)) * 100) / 100.0\n",
    "\n",
    "    Row.fromSeq(row.toSeq ++ Seq(pricePerTime, pricePerDistance))\n",
    "  }\n",
    "}\n",
    "\n",
    "rdd = addPricePerDistanceAndTime(rdd, headers, colFareAmount, colDurationMinutes, \"trip_distance\")\n",
    "headers = headers ++ Seq(colPricePerTime, colPricePerDistance)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addPricePerDistanceAndTime: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], headers: Seq[String], colFareAmount: String, colDuration: String, colDistance: String)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[16] at map at <console>:43\n",
       "headers: Seq[String] = ArraySeq(lpep_pickup_datetime, lpep_dropoff_datetime, store_and_fwd_flag, ratecodeid, passenger_count, trip_distance, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, payment_type, congestion_surcharge, duration_minutes, year, duration_minutes_bin_label, overnight_duration, overnight_duration_pcg, regular_duration, regular_duration_pcg, weekday_surcharge, fees, agg_fee_bin_label, cost_per_time, cost_per...\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Add distance bin and duration in overnight time zone",
   "id": "86d18a3f7203c64e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:20.983008Z",
     "start_time": "2025-05-13T16:56:20.744406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rdd = binColByStepValue(rdd, headers.indexOf(\"trip_distance\"), 5)\n",
    "headers = headers :+ colDistanceBin\n",
    "\n",
    "rdd = binColByStepValue(rdd, headers.indexOf(colDurationOvernightPcg), 5)\n",
    "headers = headers :+ (colDurationOvernightPcg + \"_label\")"
   ],
   "id": "d8c2b74844f233ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[18] at map at <console>:35\n",
       "headers: Seq[String] = ArraySeq(lpep_pickup_datetime, lpep_dropoff_datetime, store_and_fwd_flag, ratecodeid, passenger_count, trip_distance, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, payment_type, congestion_surcharge, duration_minutes, year, duration_minutes_bin_label, overnight_duration, overnight_duration_pcg, regular_duration, regular_duration_pcg, weekday_surcharge, fees, agg_fee_bin_label, cost_per_time, cost_per_distance, distance_bin_label, overnight_duration_pcg_label)\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[18] at map at <console>:35\n",
       "headers: Seq[String] = ArraySeq(lpep_pickup_datetime, lpep_d...\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Add key for average calculation based on columns for classification",
   "id": "6419c663ec4fef40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:21.748964Z",
     "start_time": "2025-05-13T16:56:20.988919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val actualHeader = headers\n",
    "def addKey(rdd: RDD[Row], colsClassification: Seq[String], headers: Seq[String]): RDD[(String, Row)] = {\n",
    "  rdd.map { row =>\n",
    "    val key = colsClassification.filter(col => headers.contains(col.toLowerCase))\n",
    "    .map(col => row.get(headers.indexOf(col.toLowerCase)))\n",
    "    .mkString(\"_\")\n",
    "    (key, row)\n",
    "  }\n",
    "}\n",
    "\n",
    "val rddWithKey = addKey(rdd, colsForClassification, actualHeader)"
   ],
   "id": "3b88a93a6da25df1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actualHeader: Seq[String] = ArraySeq(lpep_pickup_datetime, lpep_dropoff_datetime, store_and_fwd_flag, ratecodeid, passenger_count, trip_distance, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, payment_type, congestion_surcharge, duration_minutes, year, duration_minutes_bin_label, overnight_duration, overnight_duration_pcg, regular_duration, regular_duration_pcg, weekday_surcharge, fees, agg_fee_bin_label, cost_per_time, cost_per_distance, distance_bin_label, overnight_duration_pcg_label)\n",
       "addKey: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], colsClassification: Seq[String], headers: Seq[String])org.apache.spark.rdd.RDD[(String, org.apache.spark.sql.Row)]\n",
       "rddWithKey: org.apache.spark.rdd.RDD[(String, org.apache.spark.sql.Row)] = Map...\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Calculate prices per distance and time",
   "id": "d200896a8f8a49d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:22.257198Z",
     "start_time": "2025-05-13T16:56:21.753003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "val numPartitions = spark.sparkContext.defaultParallelism\n",
    "val partitioner = new HashPartitioner(numPartitions)\n",
    "\n",
    "def calculatePrices(rdd: RDD[(String, Row)], headers: Seq[String], colPriceDistance: String, colPriceTime: String): RDD[(String, (Double, Double, Long))] = {\n",
    "  rdd.mapValues { row =>\n",
    "    val costPerDistance = row.getAs[Double](headers.indexOf(colPriceDistance))\n",
    "    val costPerTime = row.getAs[Double](headers.indexOf(colPriceTime))\n",
    "    (costPerDistance, costPerTime, 1L)\n",
    "  }\n",
    "}\n",
    "\n",
    "val rddForAvg = calculatePrices(rddWithKey, headers, colPricePerDistance, colPricePerTime).partitionBy(partitioner).persist(StorageLevel.MEMORY_AND_DISK)"
   ],
   "id": "85e4e86bc9992218",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\n",
       "import org.apache.spark.storage.StorageLevel\n",
       "numPartitions: Int = 12\n",
       "partitioner: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@c\n",
       "calculatePrices: (rdd: org.apache.spark.rdd.RDD[(String, org.apache.spark.sql.Row)], headers: Seq[String], colPriceDistance: String, colPriceTime: String)org.apache.spark.rdd.RDD[(String, (Double, Double, Long))]\n",
       "rddForAvg: org.apache.spark.rdd.RDD[(String, (Double, Double, Long))] = ShuffledRDD[21] at partitionBy at <console>:57\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Calculate average prices per distance and time",
   "id": "ab90fed37146ba50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:23.125358Z",
     "start_time": "2025-05-13T16:56:22.261547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculateAvgPrices(rdd: RDD[(String, (Double, Double, Long))], decimals: Int): RDD[(String, (Double, Double))] = {\n",
    "  rdd.aggregateByKey((0.0, 0.0, 0L))((acc, v) => (acc._1 + v._1, acc._2 + v._2, acc._3 + v._3), (a, b) => (a._1 + b._1, a._2 + b._2, a._3 + b._3)).mapValues {\n",
    "    case(sumDist, sumTime, count) =>\n",
    "      val avgDist = BigDecimal(sumDist / count).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "      val avgTime = BigDecimal(sumTime / count).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "      (avgDist, avgTime)\n",
    "  }.filter { case(_, (dist, time)) => dist > 0.0 && time > 0.0 }\n",
    "}\n",
    "\n",
    "val rddWithAvgPrices = calculateAvgPrices(rddForAvg, decimals)\n",
    "rddForAvg.unpersist()"
   ],
   "id": "6c3896dc99dbd31b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculateAvgPrices: (rdd: org.apache.spark.rdd.RDD[(String, (Double, Double, Long))], decimals: Int)org.apache.spark.rdd.RDD[(String, (Double, Double))]\n",
       "rddWithAvgPrices: org.apache.spark.rdd.RDD[(String, (Double, Double))] = MapPartitionsRDD[24] at filter at <console>:47\n",
       "res1: rddForAvg.type = ShuffledRDD[21] at partitionBy at <console>:57\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Join average prices to previous rdd",
   "id": "2b46c67228dd6352"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:35.123626Z",
     "start_time": "2025-05-13T16:56:23.129476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.broadcast.Broadcast\n",
    "\n",
    "val broadcastAvgPrices: Broadcast[Map[String, (Double, Double)]] = spark.sparkContext.broadcast(rddWithAvgPrices.collectAsMap().toMap)\n",
    "\n",
    "def applyJoin(rdd: RDD[(String, Row)], broadcastMap: Broadcast[Map[String, (Double, Double)]]): RDD[Row] = {\n",
    "  rddWithKey.flatMap { case (key, originalRow) =>\n",
    "    broadcastMap.value.get(key).map { case(avgCostPerDistance, avgCostPerTime) =>\n",
    "      Row.fromSeq(originalRow.toSeq ++ Seq(avgCostPerDistance, avgCostPerTime))\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "rdd = applyJoin(rddWithKey, broadcastAvgPrices)\n",
    "headers = headers ++ Seq(colAvgPricePerDistance, colAvgPricePerTime)\n",
    "\n",
    "rdd.take(100).foreach(println)"
   ],
   "id": "c7da5320936ec0fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-01T00:07:08,2024-05-01T00:15:03,N,1.0,1.0,1.24,9.3,1.0,0.5,2.0,0.0,1.0,13.8,1.0,0.0,7.9167,2024.0,[5|10),7.9167,100.0,0.0,0.0,2.5,1.5,[0|2),1.17,7.5,[0|5),[100|105),8.3097,1.3835]\n",
      "[2024-05-01T00:30:48,2024-05-01T00:35:49,N,1.0,1.0,0.94,7.2,1.0,0.5,1.94,0.0,1.0,11.64,1.0,0.0,5.0167,2024.0,[5|10),5.0167,100.0,0.0,0.0,2.5,1.5,[0|2),1.44,7.66,[0|5),[100|105),8.3097,1.3835]\n",
      "[2024-05-01T00:34:13,2024-05-01T00:38:07,N,1.0,1.0,0.84,6.5,1.0,0.5,0.0,0.0,1.0,9.0,2.0,0.0,3.9,2024.0,[0|5),3.9,100.0,0.0,0.0,2.5,1.5,[0|2),1.67,7.74,[0|5),[100|105),79.1046,14.1859]\n",
      "[2024-05-01T00:58:01,2024-05-01T01:14:41,N,1.0,1.0,6.07,25.4,1.0,0.5,5.0,0.0,1.0,32.9,1.0,0.0,16.6667,2024.0,[15|20),16.6667,100.0,0.0,0.0,2.5,1.5,[0|2),1.52,4.18,[5|10),[100|105),4.9056,1.8955]\n",
      "[2024-05-01T00:11:45,2024-05-01T00:20:38,N,1.0,2.0,2.06,12.1,1.0,0.5,2.92,0.0,1.0,17.52,1.0,0.0,8.8833,2024.0,[5|10),8.8833,100.0,0.0,0.0,2.5,1.5,[0|2),1.36,5.87,[0|5),[100|105),9.4582,1.5518]\n",
      "[2024-05-01T00:29:06,2024-05-01T00:36:03,N,1.0,1.0,1.3,9.3,1.0,1.5,1.0,0.0,1.0,12.8,1.0,0.0,6.95,2024.0,[5|10),6.95,100.0,0.0,0.0,2.5,2.5,[2|4),1.34,7.15,[0|5),[100|105),7.375,1.3162]\n",
      "[2024-05-01T00:06:23,2024-05-01T00:18:18,N,1.0,5.0,4.35,19.8,1.0,0.5,3.0,0.0,1.0,28.05,1.0,2.75,11.9167,2024.0,[10|15),11.9167,100.0,0.0,0.0,2.5,4.25,[4|6),1.66,4.55,[0|5),[100|105),5.8138,1.2292]\n",
      "[2024-05-01T00:06:36,2024-05-01T00:18:03,N,1.0,1.0,2.02,13.5,1.0,0.5,0.0,0.0,1.0,16.0,2.0,0.0,11.45,2024.0,[10|15),11.45,100.0,0.0,0.0,2.5,1.5,[0|2),1.18,6.68,[0|5),[100|105),9.5061,1.1794]\n",
      "[2024-05-01T00:58:01,2024-05-01T01:07:35,N,1.0,1.0,2.35,12.8,1.0,0.5,3.0,0.0,1.0,21.05,1.0,2.75,9.5667,2024.0,[5|10),9.5667,100.0,0.0,0.0,2.5,4.25,[4|6),1.34,5.45,[0|5),[100|105),6.8234,1.3422]\n",
      "[2024-05-01T00:54:12,2024-05-01T00:58:51,N,5.0,1.0,1.3,8.0,0.0,0.0,0.0,0.0,1.0,9.0,1.0,0.0,4.65,2024.0,[0|5),4.65,100.0,0.0,0.0,2.5,1.0,[0|2),1.72,6.15,[0|5),[100|105),95.0093,43.3716]\n",
      "[2024-05-01T00:38:09,2024-05-01T00:45:10,N,1.0,1.0,1.69,10.0,1.0,0.5,0.0,0.0,1.0,15.0,2.0,2.5,7.0167,2024.0,[5|10),7.0167,100.0,0.0,0.0,2.5,4.0,[4|6),1.43,5.92,[0|5),[100|105),7.9375,1.3479]\n",
      "[2024-05-01T00:21:21,2024-05-01T00:39:26,N,1.0,1.0,3.37,20.5,1.0,0.5,3.0,0.0,1.0,26.0,1.0,0.0,18.0833,2024.0,[15|20),18.0833,100.0,0.0,0.0,2.5,1.5,[0|2),1.13,6.08,[0|5),[100|105),10.2852,1.1739]\n",
      "[2024-05-01T00:03:24,2024-05-01T00:06:23,N,1.0,2.0,0.9,6.5,1.0,0.5,0.0,0.0,1.0,9.0,1.0,0.0,2.9833,2024.0,[0|5),2.9833,100.0,0.0,0.0,2.5,1.5,[0|2),2.18,7.22,[0|5),[100|105),284.4757,81.723]\n",
      "[2024-05-01T00:32:43,2024-05-01T00:40:43,N,1.0,4.0,1.87,10.7,1.0,0.5,2.64,0.0,1.0,15.84,1.0,0.0,8.0,2024.0,[5|10),8.0,100.0,0.0,0.0,2.5,1.5,[0|2),1.34,5.72,[0|5),[100|105),11.703,2.1437]\n",
      "[2024-05-01T00:03:09,2024-05-01T00:04:40,N,1.0,5.0,0.33,4.4,1.0,0.5,0.0,0.0,1.0,6.9,2.0,0.0,1.5167,2024.0,[0|5),1.5167,100.0,0.0,0.0,2.5,1.5,[0|2),2.9,13.33,[0|5),[100|105),61.5134,3.1324]\n",
      "[2024-05-01T00:33:26,2024-05-01T00:42:38,N,1.0,1.0,1.55,11.4,1.0,0.5,2.78,0.0,1.0,16.68,1.0,0.0,9.2,2024.0,[5|10),9.2,100.0,0.0,0.0,2.5,1.5,[0|2),1.24,7.35,[0|5),[100|105),8.3097,1.3835]\n",
      "[2024-05-01T00:25:03,2024-05-01T00:33:56,N,1.0,1.0,1.19,10.0,1.0,0.5,0.0,0.0,1.0,12.5,1.0,0.0,8.8833,2024.0,[5|10),8.8833,100.0,0.0,0.0,2.5,1.5,[0|2),1.13,8.4,[0|5),[100|105),8.3097,1.3835]\n",
      "[2024-05-01T00:05:02,2024-05-01T00:23:51,N,1.0,1.0,4.49,23.3,1.0,0.5,0.0,0.0,1.0,25.8,2.0,0.0,18.8167,2024.0,[15|20),18.8167,100.0,0.0,0.0,2.5,1.5,[0|2),1.24,5.19,[0|5),[100|105),8.8787,1.064]\n",
      "[2024-05-01T00:39:26,2024-05-01T00:50:54,N,5.0,1.0,3.4,15.0,0.0,0.0,0.0,0.0,1.0,16.0,2.0,0.0,11.4667,2024.0,[10|15),11.4667,100.0,0.0,0.0,2.5,1.0,[0|2),1.31,4.41,[0|5),[100|105),9.5061,1.1794]\n",
      "[2024-05-01T01:04:29,2024-05-01T01:23,N,1.0,1.0,4.25,21.9,1.0,0.5,0.0,0.0,1.0,24.4,2.0,0.0,18.5167,2024.0,[15|20),18.5167,100.0,0.0,0.0,2.5,1.5,[0|2),1.18,5.15,[0|5),[100|105),8.8787,1.064]\n",
      "[2024-05-01T01:40,2024-05-01T01:44:59,N,1.0,1.0,0.8,6.5,1.0,0.5,0.0,0.0,1.0,9.0,2.0,0.0,4.9833,2024.0,[0|5),4.9833,100.0,0.0,0.0,2.5,1.5,[0|2),1.3,8.13,[0|5),[100|105),79.1046,14.1859]\n",
      "[2024-05-01T01:29:39,2024-05-01T01:37:57,N,1.0,1.0,0.84,8.6,1.0,0.5,0.0,0.0,1.0,11.1,2.0,0.0,8.3,2024.0,[5|10),8.3,100.0,0.0,0.0,2.5,1.5,[0|2),1.04,10.24,[0|5),[100|105),14.181,1.3387]\n",
      "[2024-05-01T01:25:03,2024-05-01T01:33:26,N,1.0,2.0,1.29,10.0,1.0,0.5,0.0,0.0,1.0,12.5,2.0,0.0,8.3833,2024.0,[5|10),8.3833,100.0,0.0,0.0,2.5,1.5,[0|2),1.19,7.75,[0|5),[100|105),9.9863,1.4388]\n",
      "[2024-05-01T01:55:33,2024-05-01T02:09:10,N,1.0,3.0,2.25,15.6,1.0,0.5,0.0,0.0,1.0,18.1,2.0,0.0,13.6167,2024.0,[10|15),13.6167,100.0,0.0,0.0,2.5,1.5,[0|2),1.15,6.93,[0|5),[100|105),6.9643,1.4111]\n",
      "[2024-05-01T01:35:10,2024-05-01T01:46:14,N,1.0,2.0,2.24,13.5,1.0,0.5,3.2,0.0,1.0,19.2,1.0,0.0,11.0667,2024.0,[10|15),11.0667,100.0,0.0,0.0,2.5,1.5,[0|2),1.22,6.03,[0|5),[100|105),9.5305,1.395]\n",
      "[2024-05-01T01:26:36,2024-05-01T01:35:32,N,1.0,1.0,1.93,11.4,1.0,0.5,4.17,0.0,1.0,18.07,1.0,0.0,8.9333,2024.0,[5|10),8.9333,100.0,0.0,0.0,2.5,1.5,[0|2),1.28,5.91,[0|5),[100|105),8.3097,1.3835]\n",
      "[2024-05-01T01:05:17,2024-05-01T01:15:56,N,1.0,1.0,2.46,14.2,1.0,0.5,0.0,0.0,1.0,16.7,2.0,0.0,10.65,2024.0,[10|15),10.65,100.0,0.0,0.0,2.5,1.5,[0|2),1.33,5.77,[0|5),[100|105),9.5061,1.1794]\n",
      "[2024-05-01T01:27:32,2024-05-01T01:39:35,N,1.0,3.0,2.72,14.9,1.0,0.5,3.48,0.0,1.0,20.88,1.0,0.0,12.05,2024.0,[10|15),12.05,100.0,0.0,0.0,2.5,1.5,[0|2),1.24,5.48,[0|5),[100|105),7.8039,1.6149]\n",
      "[2024-05-01T01:50:53,2024-05-01T02:03:36,N,1.0,5.0,5.56,24.0,1.0,0.5,7.95,0.0,1.0,34.45,1.0,0.0,12.7167,2024.0,[10|15),12.7167,100.0,0.0,0.0,2.5,1.5,[0|2),1.89,4.32,[5|10),[100|105),4.8011,2.2293]\n",
      "[2024-05-01T01:25:46,2024-05-01T01:29:13,N,5.0,1.0,0.84,9.0,0.0,0.0,0.0,0.0,1.0,10.0,2.0,0.0,3.45,2024.0,[0|5),3.45,100.0,0.0,0.0,2.5,1.0,[0|2),2.61,10.71,[0|5),[100|105),79.1046,14.1859]\n",
      "[2024-05-01T01:42:51,2024-05-01T01:54:24,N,5.0,1.0,3.21,14.0,0.0,0.0,0.0,0.0,1.0,15.0,2.0,0.0,11.55,2024.0,[10|15),11.55,100.0,0.0,0.0,2.5,1.0,[0|2),1.21,4.36,[0|5),[100|105),9.5061,1.1794]\n",
      "[2024-05-01T02:32:06,2024-05-01T02:47:50,N,1.0,1.0,2.34,16.3,1.0,0.5,3.76,0.0,1.0,22.56,1.0,0.0,15.7333,2024.0,[15|20),15.7333,100.0,0.0,0.0,2.5,1.5,[0|2),1.04,6.97,[0|5),[100|105),10.2852,1.1739]\n",
      "[2024-05-01T02:23:57,2024-05-01T02:37:01,N,1.0,1.0,1.74,14.2,1.0,0.5,0.0,0.0,1.0,16.7,2.0,0.0,13.0667,2024.0,[10|15),13.0667,100.0,0.0,0.0,2.5,1.5,[0|2),1.09,8.16,[0|5),[100|105),9.5061,1.1794]\n",
      "[2024-05-01T02:52:23,2024-05-01T03:01:37,N,1.0,1.0,3.34,15.6,1.0,0.5,3.62,0.0,1.0,21.72,1.0,0.0,9.2333,2024.0,[5|10),9.2333,100.0,0.0,0.0,2.5,1.5,[0|2),1.69,4.67,[0|5),[100|105),8.3097,1.3835]\n",
      "[2024-05-01T02:16:12,2024-05-01T02:52:25,N,1.0,1.0,7.04,37.3,1.0,0.5,8.51,0.0,1.0,51.06,1.0,2.75,36.2167,2024.0,[35|40),36.2167,100.0,0.0,0.0,2.5,4.25,[4|6),1.03,5.3,[5|10),[100|105),5.3805,1.094]\n",
      "[2024-05-01T02:10:08,2024-05-01T02:25:48,N,1.0,1.0,4.64,21.9,1.0,0.5,4.0,0.0,1.0,28.4,1.0,0.0,15.6667,2024.0,[15|20),15.6667,100.0,0.0,0.0,2.5,1.5,[0|2),1.4,4.72,[0|5),[100|105),10.2852,1.1739]\n",
      "[2024-05-01T02:52:32,2024-05-01T03:33:40,N,1.0,1.0,7.44,40.1,1.0,0.5,0.0,6.94,1.0,52.29,2.0,2.75,41.1333,2024.0,[40|45),41.1333,100.0,0.0,0.0,2.5,4.25,[4|6),0.97,5.39,[5|10),[100|105),5.7229,1.0343]\n",
      "[2024-05-01T02:48:25,2024-05-01T03:53:26,N,1.0,2.0,15.8,78.6,1.0,0.5,0.0,0.0,1.0,81.1,2.0,0.0,65.0167,2024.0,[65|70),65.0167,100.0,0.0,0.0,2.5,1.5,[0|2),1.21,4.97,[15|20),[100|105),4.97,1.21]\n",
      "[2024-05-01T02:09:20,2024-05-01T02:13:45,N,1.0,1.0,1.09,7.2,1.0,0.5,0.0,0.0,1.0,9.7,2.0,0.0,4.4167,2024.0,[0|5),4.4167,100.0,0.0,0.0,2.5,1.5,[0|2),1.63,6.61,[0|5),[100|105),79.1046,14.1859]\n",
      "[2024-05-01T02:28:01,2024-05-01T02:33:17,N,1.0,1.0,0.95,7.2,1.0,0.5,1.94,0.0,1.0,11.64,1.0,0.0,5.2667,2024.0,[5|10),5.2667,100.0,0.0,0.0,2.5,1.5,[0|2),1.37,7.58,[0|5),[100|105),8.3097,1.3835]\n",
      "[2024-05-01T02:50:25,2024-05-01T02:56:55,N,1.0,1.0,1.85,10.0,1.0,0.5,2.0,0.0,1.0,14.5,1.0,0.0,6.5,2024.0,[5|10),6.5,100.0,0.0,0.0,2.5,1.5,[0|2),1.54,5.41,[0|5),[100|105),8.3097,1.3835]\n",
      "[2024-05-01T02:31:52,2024-05-01T02:38:59,N,5.0,1.0,1.14,9.0,0.0,0.0,0.0,0.0,1.0,10.0,2.0,0.0,7.1167,2024.0,[5|10),7.1167,100.0,0.0,0.0,2.5,1.0,[0|2),1.26,7.89,[0|5),[100|105),14.181,1.3387]\n",
      "[2024-05-01T03:37:50,2024-05-01T03:48:42,N,1.0,1.0,5.64,24.0,1.0,0.5,0.0,0.0,1.0,26.5,1.0,0.0,10.8667,2024.0,[10|15),10.8667,100.0,0.0,0.0,2.5,1.5,[0|2),2.21,4.26,[5|10),[100|105),4.9387,2.307]\n",
      "[2024-05-01T03:42:49,2024-05-02T02:09:10,N,1.0,1.0,3.04,15.6,1.0,0.5,0.0,0.0,1.0,18.1,2.0,0.0,1346.35,2024.0,[1345|1350),506.3333,37.6079,840.0,62.3909,2.5,1.5,[0|2),0.01,5.13,[0|5),[35|40),6.225,0.01]\n",
      "[2024-05-01T03:12:33,2024-05-01T03:23:25,N,1.0,1.0,2.53,14.2,1.0,0.5,0.0,0.0,1.0,19.45,2.0,2.75,10.8667,2024.0,[10|15),10.8667,100.0,0.0,0.0,2.5,4.25,[4|6),1.31,5.61,[0|5),[100|105),6.7705,1.2218]\n",
      "[2024-05-01T03:18:08,2024-05-01T03:43:45,N,2.0,1.0,14.11,70.0,0.0,0.5,15.69,6.94,1.0,94.13,1.0,0.0,25.6167,2024.0,[25|30),25.6167,100.0,0.0,0.0,2.5,1.5,[0|2),2.73,4.96,[10|15),[100|105),4.6659,2.0597]\n",
      "[2024-05-01T03:29:31,2024-05-01T03:31:52,N,1.0,1.0,0.3,4.4,1.0,0.5,0.0,0.0,1.0,6.9,3.0,0.0,2.35,2024.0,[0|5),2.35,100.0,0.0,0.0,2.5,1.5,[0|2),1.87,14.67,[0|5),[100|105),286.9399,23.9112]\n",
      "[2024-05-01T04:37:07,2024-05-01T04:47:31,N,1.0,1.0,2.62,14.9,1.0,0.5,4.03,0.0,1.0,24.18,1.0,2.75,10.4,2024.0,[10|15),10.4,100.0,0.0,0.0,2.5,4.25,[4|6),1.43,5.69,[0|5),[100|105),6.2146,1.2337]\n",
      "[2024-05-01T04:40:17,2024-05-01T04:57:26,N,1.0,1.0,7.35,31.0,1.0,0.5,8.38,0.0,1.0,41.88,1.0,0.0,17.15,2024.0,[15|20),17.15,100.0,0.0,0.0,2.5,1.5,[0|2),1.81,4.22,[5|10),[100|105),4.9056,1.8955]\n",
      "[2024-05-01T04:35:35,2024-05-01T04:39:55,N,1.0,1.0,1.58,8.6,1.0,0.5,2.22,0.0,1.0,13.32,1.0,0.0,4.3333,2024.0,[0|5),4.3333,100.0,0.0,0.0,2.5,1.5,[0|2),1.98,5.44,[0|5),[100|105),95.0093,43.3716]\n",
      "[2024-05-01T05:33:57,2024-05-01T06:00:17,N,1.0,1.0,6.12,31.0,1.0,0.5,9.06,0.0,1.0,45.31,1.0,2.75,26.3333,2024.0,[25|30),26.05,98.9242,0.2833,1.0758,2.5,4.25,[4|6),1.18,5.07,[5|10),[95|100),4.8824,1.27]\n",
      "[2024-05-01T05:25:46,2024-05-01T05:41:05,N,1.0,1.0,8.3,33.1,6.0,0.5,7.13,6.94,1.0,54.67,1.0,0.0,15.3167,2024.0,[15|20),15.3167,100.0,0.0,0.0,2.5,7.5,[6|8),2.16,3.99,[5|10),[100|105),4.8342,2.0003]\n",
      "[2024-05-01T05:59:07,2024-05-01T06:08:02,N,1.0,1.0,1.67,10.7,1.0,0.5,2.64,0.0,1.0,15.84,1.0,0.0,8.9167,2024.0,[5|10),0.8833,9.9061,8.0333,90.0927,2.5,1.5,[0|2),1.2,6.41,[0|5),[5|10),12.2691,2.0773]\n",
      "[2024-05-01T05:43:34,2024-05-01T05:59:10,N,1.0,1.0,3.29,19.1,1.0,0.5,4.87,0.0,1.0,29.22,1.0,2.75,15.6,2024.0,[15|20),15.6,100.0,0.0,0.0,2.5,4.25,[4|6),1.22,5.81,[0|5),[100|105),5.9418,1.1369]\n",
      "[2024-05-01T05:05:20,2024-05-01T05:07:08,N,1.0,1.0,0.16,3.7,1.0,0.5,0.0,0.0,1.0,6.2,1.0,0.0,1.8,2024.0,[0|5),1.8,100.0,0.0,0.0,2.5,1.5,[0|2),2.06,23.13,[0|5),[100|105),95.0093,43.3716]\n",
      "[2024-05-01T05:40:19,2024-05-01T05:52:31,N,1.0,1.0,2.06,13.5,1.0,0.5,3.2,0.0,1.0,19.2,1.0,0.0,12.2,2024.0,[10|15),12.2,100.0,0.0,0.0,2.5,1.5,[0|2),1.11,6.55,[0|5),[100|105),7.3653,1.2484]\n",
      "[2024-05-01T05:57:02,2024-05-01T06:04:25,N,1.0,1.0,1.31,9.3,1.0,0.5,0.0,0.0,1.0,11.8,1.0,0.0,7.3833,2024.0,[5|10),2.9667,40.1812,4.4167,59.8201,2.5,1.5,[0|2),1.26,7.1,[0|5),[40|45),6.9737,1.34]\n",
      "[2024-05-01T05:19:07,2024-05-01T05:24:19,N,1.0,2.0,1.27,8.6,1.0,0.5,0.0,0.0,1.0,11.1,2.0,0.0,5.2,2024.0,[5|10),5.2,100.0,0.0,0.0,2.5,1.5,[0|2),1.65,6.77,[0|5),[100|105),9.9863,1.4388]\n",
      "[2024-05-01T05:29:28,2024-05-01T05:42:58,N,1.0,1.0,3.91,19.1,1.0,0.5,2.0,0.0,1.0,26.35,1.0,2.75,13.5,2024.0,[10|15),13.5,100.0,0.0,0.0,2.5,4.25,[4|6),1.41,4.88,[0|5),[100|105),6.2146,1.2337]\n",
      "[2024-05-01T05:23:05,2024-05-01T05:24:35,N,5.0,4.0,0.09,20.0,0.0,0.0,0.0,0.0,1.0,21.0,2.0,0.0,1.5,2024.0,[0|5),1.5,100.0,0.0,0.0,2.5,1.0,[0|2),13.33,222.22,[0|5),[100|105),286.1059,40.4294]\n",
      "[2024-05-01T05:48:44,2024-05-01T07:28:17,N,1.0,1.0,15.38,68.1,1.0,0.5,18.34,0.0,1.0,91.69,1.0,2.75,99.55,2024.0,[95|100),11.2667,11.3176,88.2833,88.6824,2.5,4.25,[4|6),0.68,4.43,[15|20),[10|15),4.44,0.69]\n",
      "[2024-05-01T05:35:20,2024-05-01T05:42:06,N,1.0,2.0,1.78,10.0,1.0,0.5,3.05,0.0,1.0,18.3,1.0,2.75,6.7667,2024.0,[5|10),6.7667,100.0,0.0,0.0,2.5,4.25,[4|6),1.48,5.62,[0|5),[100|105),6.5372,1.3689]\n",
      "[2024-05-01T05:59:56,2024-05-01T06:11:04,N,1.0,2.0,3.35,15.6,1.0,0.5,3.13,0.0,1.0,23.98,1.0,2.75,11.1333,2024.0,[10|15),0.0667,0.5991,11.0667,99.4018,2.5,4.25,[4|6),1.4,4.66,[0|5),[0|5),7.4516,1.0994]\n",
      "[2024-05-01T05:35:43,2024-05-01T05:41:18,N,1.0,1.0,1.87,10.0,1.0,0.5,2.5,0.0,1.0,15.0,1.0,0.0,5.5833,2024.0,[5|10),5.5833,100.0,0.0,0.0,2.5,1.5,[0|2),1.79,5.35,[0|5),[100|105),8.3097,1.3835]\n",
      "[2024-05-01T05:13:05,2024-05-01T05:19:53,N,1.0,1.0,4.4,18.4,1.0,0.5,0.0,0.0,1.0,20.9,2.0,0.0,6.8,2024.0,[5|10),6.8,100.0,0.0,0.0,2.5,1.5,[0|2),2.71,4.18,[0|5),[100|105),14.181,1.3387]\n",
      "[2024-05-01T06:41:28,2024-05-01T07:17:18,N,1.0,1.0,13.49,57.6,0.0,0.5,0.0,6.94,1.0,68.79,1.0,2.75,35.8333,2024.0,[35|40),0.0,0.0,35.8333,100.0,2.5,4.25,[4|6),1.61,4.27,[10|15),[0|5),4.4182,1.377]\n",
      "[2024-05-01T06:59:36,2024-05-01T07:10:10,N,1.0,1.0,1.51,11.4,0.0,0.5,2.58,0.0,1.0,15.48,1.0,0.0,10.5667,2024.0,[10|15),0.0,0.0,10.5667,100.0,2.5,1.5,[0|2),1.08,7.55,[0|5),[0|5),7.9745,1.1101]\n",
      "[2024-05-01T06:14:34,2024-05-01T06:21:13,N,1.0,1.0,1.8,9.3,2.75,1.5,2.7,0.0,1.0,16.25,1.0,2.75,6.65,2024.0,[5|10),0.0,0.0,6.65,100.0,2.5,5.25,[4|6),1.4,5.17,[0|5),[0|5),8.0143,1.264]\n",
      "[2024-05-01T06:47:15,2024-05-01T06:59:49,N,1.0,1.0,1.9,12.1,0.0,1.5,1.0,0.0,1.0,14.6,1.0,0.0,12.5667,2024.0,[10|15),0.0,0.0,12.5667,100.0,2.5,2.5,[2|4),0.96,6.37,[0|5),[0|5),7.3211,1.0919]\n",
      "[2024-05-01T06:39:39,2024-05-01T06:44:04,N,1.0,1.0,0.79,6.5,0.0,0.5,0.0,0.0,1.0,8.0,2.0,0.0,4.4167,2024.0,[0|5),0.0,0.0,4.4167,100.0,2.5,1.5,[0|2),1.47,8.23,[0|5),[0|5),109.9904,19.4764]\n",
      "[2024-05-01T06:58:23,2024-05-01T07:13:54,N,1.0,1.0,3.57,19.1,0.0,0.5,4.67,0.0,1.0,28.02,1.0,2.75,15.5167,2024.0,[15|20),0.0,0.0,15.5167,100.0,2.5,4.25,[4|6),1.23,5.35,[0|5),[0|5),7.5576,1.0393]\n",
      "[2024-05-01T06:58:51,2024-05-01T07:02:35,N,1.0,1.0,0.69,5.8,0.0,0.5,1.46,0.0,1.0,8.76,1.0,0.0,3.7333,2024.0,[0|5),0.0,0.0,3.7333,100.0,2.5,1.5,[0|2),1.55,8.41,[0|5),[0|5),87.6965,37.7609]\n",
      "[2024-05-01T05:58:32,2024-05-01T06:02:46,N,1.0,1.0,1.16,7.2,0.0,0.5,1.74,0.0,1.0,10.44,1.0,0.0,4.2333,2024.0,[0|5),1.4667,34.6467,2.7667,65.3556,2.5,1.5,[0|2),1.7,6.21,[0|5),[30|35),7.0167,1.62]\n",
      "[2024-05-01T06:18:41,2024-05-01T06:32:27,N,1.0,1.0,1.22,12.8,0.0,0.5,4.29,0.0,1.0,18.59,1.0,0.0,13.7667,2024.0,[10|15),0.0,0.0,13.7667,100.0,2.5,1.5,[0|2),0.93,10.49,[0|5),[0|5),7.9745,1.1101]\n",
      "[2024-05-01T06:49:33,2024-05-01T06:57:49,N,1.0,1.0,1.15,9.3,0.0,0.5,0.0,0.0,1.0,10.8,2.0,0.0,8.2667,2024.0,[5|10),0.0,0.0,8.2667,100.0,2.5,1.5,[0|2),1.12,8.09,[0|5),[0|5),9.6764,1.2689]\n",
      "[2024-05-01T06:33:03,2024-05-01T06:55:23,N,1.0,1.0,4.09,23.3,0.0,0.5,5.51,0.0,1.0,33.06,1.0,2.75,22.3333,2024.0,[20|25),0.0,0.0,22.3333,100.0,2.5,4.25,[4|6),1.04,5.7,[0|5),[0|5),7.7265,0.9726]\n",
      "[2024-05-01T06:47:48,2024-05-01T06:52:31,N,1.0,1.0,0.2,4.5,0.0,0.5,0.0,0.0,0.3,5.3,2.0,0.0,4.7167,2024.0,[0|5),0.0,0.0,4.7167,100.0,2.5,0.8,[0|2),0.95,22.5,[0|5),[0|5),109.9904,19.4764]\n",
      "[2024-05-01T06:01:52,2024-05-01T06:19:48,N,1.0,1.0,7.45,31.7,5.0,0.5,9.03,6.94,1.0,54.17,1.0,0.0,17.9333,2024.0,[15|20),0.0,0.0,17.9333,100.0,2.5,6.5,[6|8),1.77,4.26,[5|10),[0|5),4.3946,1.6955]\n",
      "[2024-05-01T06:52:43,2024-05-01T07:01:43,N,1.0,1.0,1.69,10.7,0.0,0.5,0.0,0.0,1.0,12.2,2.0,0.0,9.0,2024.0,[5|10),0.0,0.0,9.0,100.0,2.5,1.5,[0|2),1.19,6.33,[0|5),[0|5),9.6764,1.2689]\n",
      "[2024-05-01T06:45:19,2024-05-01T06:46:46,N,1.0,1.0,0.67,5.1,0.0,0.5,1.32,0.0,1.0,7.92,1.0,0.0,1.45,2024.0,[0|5),0.0,0.0,1.45,100.0,2.5,1.5,[0|2),3.52,7.61,[0|5),[0|5),87.6965,37.7609]\n",
      "[2024-05-01T06:53:46,2024-05-01T07:06:58,N,1.0,1.0,2.2,14.9,0.0,0.5,1.5,0.0,1.0,17.9,1.0,0.0,13.2,2024.0,[10|15),0.0,0.0,13.2,100.0,2.5,1.5,[0|2),1.13,6.77,[0|5),[0|5),7.9745,1.1101]\n",
      "[2024-05-01T06:26:47,2024-05-01T06:34:17,N,1.0,1.0,1.55,10.0,0.0,0.5,2.3,0.0,1.0,13.8,1.0,0.0,7.5,2024.0,[5|10),0.0,0.0,7.5,100.0,2.5,1.5,[0|2),1.33,6.45,[0|5),[0|5),8.382,1.2754]\n",
      "[2024-05-01T06:57:32,2024-05-01T07:03:15,N,1.0,1.0,1.2,7.9,0.0,1.5,1.88,0.0,1.0,11.28,1.0,0.0,5.7167,2024.0,[5|10),0.0,0.0,5.7167,100.0,2.5,2.5,[2|4),1.38,6.58,[0|5),[0|5),7.6275,1.2405]\n",
      "[2024-05-01T06:21:04,2024-05-01T06:27:27,N,1.0,1.0,1.6,10.0,2.75,1.5,3.55,0.0,1.0,17.8,1.0,2.75,6.3833,2024.0,[5|10),0.0,0.0,6.3833,100.0,2.5,5.25,[4|6),1.57,6.25,[0|5),[0|5),8.0143,1.264]\n",
      "[2024-05-01T06:18,2024-05-01T06:30:47,N,1.0,1.0,1.86,13.5,0.0,0.5,3.0,0.0,1.0,18.0,1.0,0.0,12.7833,2024.0,[10|15),0.0,0.0,12.7833,100.0,2.5,1.5,[0|2),1.06,7.26,[0|5),[0|5),7.9745,1.1101]\n",
      "[2024-05-01T06:06,2024-05-01T06:19:46,N,1.0,1.0,8.29,33.1,5.0,0.5,11.64,6.94,1.0,58.18,1.0,0.0,13.7667,2024.0,[10|15),0.0,0.0,13.7667,100.0,2.5,6.5,[6|8),2.4,3.99,[5|10),[0|5),4.2268,2.1498]\n",
      "[2024-05-01T06:48:59,2024-05-01T06:53:15,N,1.0,1.0,0.9,7.2,0.0,1.5,1.74,0.0,1.0,10.44,1.0,0.0,4.2667,2024.0,[0|5),0.0,0.0,4.2667,100.0,2.5,2.5,[2|4),1.69,8.0,[0|5),[0|5),10.9277,3.622]\n",
      "[2024-05-01T06:21:22,2024-05-01T06:31:27,N,1.0,1.0,2.1,12.1,0.0,0.5,3.27,0.0,1.0,19.62,1.0,2.75,10.0833,2024.0,[10|15),0.0,0.0,10.0833,100.0,2.5,4.25,[4|6),1.2,5.76,[0|5),[0|5),7.4269,1.1227]\n",
      "[2024-05-01T06:50:30,2024-05-01T06:57:53,N,1.0,1.0,1.27,9.3,0.0,0.5,2.16,0.0,1.0,12.96,1.0,0.0,7.3833,2024.0,[5|10),0.0,0.0,7.3833,100.0,2.5,1.5,[0|2),1.26,7.32,[0|5),[0|5),8.382,1.2754]\n",
      "[2024-05-01T06:34:32,2024-05-01T06:38:30,N,1.0,1.0,0.83,6.5,0.0,0.5,0.0,0.0,1.0,8.0,2.0,0.0,3.9667,2024.0,[0|5),0.0,0.0,3.9667,100.0,2.5,1.5,[0|2),1.64,7.83,[0|5),[0|5),109.9904,19.4764]\n",
      "[2024-05-01T06:34:43,2024-05-01T06:49:45,N,1.0,2.0,7.8,31.7,5.0,0.5,0.0,6.94,1.0,45.14,2.0,0.0,15.0333,2024.0,[15|20),0.0,0.0,15.0333,100.0,2.5,6.5,[6|8),2.11,4.06,[5|10),[0|5),4.33,1.7305]\n",
      "[2024-05-01T06:35:56,2024-05-01T06:46:13,N,1.0,2.0,1.69,12.1,0.0,0.5,2.72,0.0,1.0,16.32,1.0,0.0,10.2833,2024.0,[10|15),0.0,0.0,10.2833,100.0,2.5,1.5,[0|2),1.18,7.16,[0|5),[0|5),7.8069,1.1118]\n",
      "[2024-05-01T06:23:36,2024-05-01T06:34:07,N,1.0,1.0,1.93,12.1,0.0,0.5,0.0,0.0,1.0,13.6,2.0,0.0,10.5167,2024.0,[10|15),0.0,0.0,10.5167,100.0,2.5,1.5,[0|2),1.15,6.27,[0|5),[0|5),9.3926,1.1155]\n",
      "[2024-05-01T06:53:19,2024-05-01T07:02:24,N,1.0,1.0,1.72,11.4,0.0,0.5,3.87,0.0,1.0,16.77,1.0,0.0,9.0833,2024.0,[5|10),0.0,0.0,9.0833,100.0,2.5,1.5,[0|2),1.26,6.63,[0|5),[0|5),8.382,1.2754]\n",
      "[2024-05-01T06:43:14,2024-05-01T06:47:50,N,1.0,1.0,0.7,6.5,0.0,1.5,0.0,0.0,1.0,8.0,1.0,0.0,4.6,2024.0,[0|5),0.0,0.0,4.6,100.0,2.5,2.5,[2|4),1.41,9.29,[0|5),[0|5),10.9277,3.622]\n",
      "[2024-05-01T06:12:58,2024-05-01T06:21:30,N,1.0,1.0,2.23,12.1,0.0,0.5,3.27,0.0,1.0,19.62,1.0,2.75,8.5333,2024.0,[5|10),0.0,0.0,8.5333,100.0,2.5,4.25,[4|6),1.42,5.43,[0|5),[0|5),8.0143,1.264]\n",
      "[2024-05-01T06:49:19,2024-05-01T06:57:09,N,1.0,1.0,1.4,10.0,0.0,0.5,2.3,0.0,1.0,13.8,1.0,0.0,7.8333,2024.0,[5|10),0.0,0.0,7.8333,100.0,2.5,1.5,[0|2),1.28,7.14,[0|5),[0|5),8.382,1.2754]\n",
      "[2024-05-01T06:16:17,2024-05-01T06:21:15,N,1.0,1.0,1.3,7.9,0.0,1.5,1.0,0.0,1.0,10.4,1.0,0.0,4.9667,2024.0,[0|5),0.0,0.0,4.9667,100.0,2.5,2.5,[2|4),1.59,6.08,[0|5),[0|5),10.9277,3.622]\n",
      "[2024-05-01T06:38:32,2024-05-01T06:52:05,N,1.0,1.0,1.9,14.2,2.75,1.5,3.14,0.0,1.0,21.59,1.0,2.75,13.55,2024.0,[10|15),0.0,0.0,13.55,100.0,2.5,5.25,[4|6),1.05,7.47,[0|5),[0|5),7.4269,1.1227]\n",
      "[2024-05-01T06:42:21,2024-05-01T07:05:32,N,1.0,1.0,9.32,40.1,0.0,0.5,8.32,0.0,1.0,49.92,1.0,0.0,23.1833,2024.0,[20|25),0.0,0.0,23.1833,100.0,2.5,1.5,[0|2),1.73,4.3,[5|10),[0|5),4.8126,1.4422]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.broadcast.Broadcast\n",
       "broadcastAvgPrices: org.apache.spark.broadcast.Broadcast[Map[String,(Double, Double)]] = Broadcast(5)\n",
       "applyJoin: (rdd: org.apache.spark.rdd.RDD[(String, org.apache.spark.sql.Row)], broadcastMap: org.apache.spark.broadcast.Broadcast[Map[String,(Double, Double)]])org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[25] at flatMap at <console>:51\n",
       "headers: Seq[String] = ArraySeq(lpep_pickup_datetime, lpep_dropoff_datetime, store_and_fwd_flag, ratecodeid, passenger_count, trip_distance, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, payment_type, congestion_surcharge, duration_minutes, year, duration_minutes_bin_label, overnight_dur...\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Add price comparison",
   "id": "b97ec9d1cf2b66bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:37.152902Z",
     "start_time": "2025-05-13T16:56:35.131018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def addPriceComparison(rdd: RDD[Row], headers: Seq[String], colPriceDistance: String, colAvgPriceDistance: String, colPriceTime: String, colAvgPriceTime: String, decimals: Int) = {\n",
    "  rdd.map { row =>\n",
    "    val priceColsToAdd: Seq[Double] = Seq((colPriceDistance, colAvgPriceDistance), (colPriceTime, colAvgPriceTime))\n",
    "      .flatMap { case (colPrice, colAvgPrice) =>\n",
    "        val price = row.getAs[Double](headers.indexOf(colPrice))\n",
    "        val priceAvg = row.getAs[Double](headers.indexOf(colAvgPrice))\n",
    "        val priceDiff = BigDecimal(price - priceAvg).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "        val priceDiffPcg = BigDecimal(priceDiff / priceAvg * 100).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "\n",
    "        Seq(priceDiff, priceDiffPcg)\n",
    "      }\n",
    "    Row.fromSeq(row.toSeq ++ priceColsToAdd)\n",
    "  }\n",
    "}\n",
    "\n",
    "rdd = addPriceComparison(rdd, headers, colPricePerDistance, colAvgPricePerDistance, colPricePerTime, colAvgPricePerTime, decimals)\n",
    "headers = headers ++ Seq(colPricePerDistanceDiff, colPricePerDistanceDiffPcg, colPricePerTimeDiff, colPricePerTimeDiffPcg)"
   ],
   "id": "794982d2cb593fd2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addPriceComparison: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], headers: Seq[String], colPriceDistance: String, colAvgPriceDistance: String, colPriceTime: String, colAvgPriceTime: String, decimals: Int)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[26] at map at <console>:51\n",
       "headers: Seq[String] = ArraySeq(lpep_pickup_datetime, lpep_dropoff_datetime, store_and_fwd_flag, ratecodeid, passenger_count, trip_distance, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, payment_type, congestion_surcharge, duration_minutes, year, duration_minutes_bin_label, overnight_duration, overnight_duration_pcg, regular_duration, regular_duration_pcg, weekday_surcharge, fees...\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Bin price difference per time and distance",
   "id": "988b3766e90e9789"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:37.426661Z",
     "start_time": "2025-05-13T16:56:37.156932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rdd = binColByStepValue(rdd,headers.indexOf(colPricePerDistanceDiffPcg), 5)\n",
    "rdd = binColByStepValue(rdd, headers.indexOf(colPricePerTimeDiffPcg), 5)\n",
    "\n",
    "headers = headers ++ Seq(colPricePerDistanceDiffPcgLabel, colPricePerTimeDiffPcgLabel)"
   ],
   "id": "d43c4991694d18f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[28] at map at <console>:35\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[28] at map at <console>:35\n",
       "headers: Seq[String] = ArraySeq(lpep_pickup_datetime, lpep_dropoff_datetime, store_and_fwd_flag, ratecodeid, passenger_count, trip_distance, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, payment_type, congestion_surcharge, duration_minutes, year, duration_minutes_bin_label, overnight_duration, overnight_duration_pcg, regular_duration, regular_duration_pcg, weekday_surcharge, fees, agg_fee_bin_label, cost_per_time, cost_per_distance, distance_bin_label, overnight_duration_pcg_label, avg_cost_per_distance, avg_cost_per_time, cost_per_distance_...\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Reduce to analysis columns only",
   "id": "d55dcfcdf1b91809"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:56:43.006993Z",
     "start_time": "2025-05-13T16:56:37.430809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val headersForAnalysis = headers.filter(head => colsForClassification.contains(head.toLowerCase))\n",
    "\n",
    "def reduceToAnalysis(rdd: RDD[Row], headers: Seq[String]): RDD[Row] = {\n",
    "  rdd.map { row =>\n",
    "    Row.fromSeq(headers.indices.map(row.get))\n",
    "  }\n",
    "}\n",
    "\n",
    "rdd = reduceToAnalysis(rdd, headersForAnalysis)\n",
    "\n",
    "val totalCount = rdd.count()"
   ],
   "id": "c56a14f69be905a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headersForAnalysis: Seq[String] = ArraySeq(store_and_fwd_flag, passenger_count, payment_type, year, duration_minutes_bin_label, agg_fee_bin_label, distance_bin_label, overnight_duration_pcg_label, cost_per_distance_diff_pcg_label, cost_per_time_diff_pcg_label)\n",
       "reduceToAnalysis: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], headers: Seq[String])org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[29] at map at <console>:45\n",
       "totalCount: Long = 587441\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Group by feature value",
   "id": "9f6cdebc4d24c9de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:57:16.372927Z",
     "start_time": "2025-05-13T16:56:43.010988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def groupByFeatures(rdd: RDD[Row], colForValuesAnalysis: Seq[String], colPriceDistanceDiffPcgLabel: String, colPriceTimeDiffPcgLabel: String, headersAnalysis: Seq[String], decimals: Int, totalCount: Long): Seq[RDD[Row]] = {\n",
    "  colForValuesAnalysis.map { colName =>\n",
    "    val groupCols = Seq(colPriceDistanceDiffPcgLabel, colPriceTimeDiffPcgLabel):+ colName\n",
    "    val grouped = rdd.map { row =>\n",
    "      val key = groupCols.map(col => row.get(headersAnalysis.indexOf(col.toLowerCase)))\n",
    "      (key, 1)\n",
    "    }.reduceByKey(_ + _).map { case (keySeq, count) =>\n",
    "      val value = keySeq.last.toString\n",
    "      val costDistLabel = keySeq(0).toString\n",
    "      val costTimeLabel = keySeq(1).toString\n",
    "      val pcg = BigDecimal(count.toDouble / totalCount * 100).setScale(decimals, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "      Row.fromSeq(Seq(colName, value, count, pcg, costDistLabel, costTimeLabel))\n",
    "    }\n",
    "    grouped\n",
    "  }\n",
    "}\n",
    "\n",
    "val rddFeatures = groupByFeatures(rdd, colsForValuesAnalysis, colPricePerDistanceDiffPcgLabel, colPricePerTimeDiffPcgLabel, headersForAnalysis, decimals, totalCount)\n",
    "\n",
    "rddFeatures.foreach(rdd => rdd.take(1).foreach(println))"
   ],
   "id": "298184e06179fe88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[passenger_count,2024-03-04T17:08:32,1,2.0E-4,0.5,0.0]\n",
      "[store_and_fwd_flag,2024-04-26T12:26:05,1,2.0E-4,0.5,0.0]\n",
      "[payment_type,N,108,0.0184,0.5,0.11]\n",
      "[agg_fee_bin_label,2.34,1,2.0E-4,0.0,4.8]\n",
      "[duration_minutes_bin_label,1.0,34,0.0058,0.5,2.59]\n",
      "[distance_bin_label,26.8,2,3.0E-4,0.5,8.58]\n",
      "[year,1.0,35,0.006,0.5,2.59]\n",
      "[overnight_duration_pcg_label,1.0,26,0.0044,0.5,2.59]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "groupByFeatures: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], colForValuesAnalysis: Seq[String], colPriceDistanceDiffPcgLabel: String, colPriceTimeDiffPcgLabel: String, headersAnalysis: Seq[String], decimals: Int, totalCount: Long)Seq[org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]]\n",
       "rddFeatures: Seq[org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]] = List(MapPartitionsRDD[32] at map at <console>:52, MapPartitionsRDD[35] at map at <console>:52, MapPartitionsRDD[38] at map at <console>:52, MapPartitionsRDD[41] at map at <console>:52, MapPartitionsRDD[44] at map at <console>:52, MapPartitionsRDD[47] at map at <console>:52, MapPartitionsRDD[50] at map at <console>:52, MapPartitionsRDD[53] at map at <console>:52)\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Reduce to single rdd and write output",
   "id": "795fbb90ba3dc7a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:57:29.304551Z",
     "start_time": "2025-05-13T16:57:16.384922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val headersForSchema = Seq(\n",
    "  StructField(\"feature\", StringType),\n",
    "  StructField(\"value\", StringType),\n",
    "  StructField(\"count\", IntegerType),\n",
    "  StructField(\"pcg\", DoubleType),\n",
    "  StructField(\"cost_distance_label\", StringType),\n",
    "  StructField(\"cost_time_label\", StringType)\n",
    ")\n",
    "\n",
    "val schema = StructType(headersForSchema)\n",
    "\n",
    "val dfForAnalysis = spark.createDataFrame(rddFeatures.reduce(_ union _), schema)\n",
    "dfForAnalysis.show(10)\n",
    "dfForAnalysis.write.mode(\"overwrite\").parquet(getDatasetPath(outputDir + f\"/$name\"))"
   ],
   "id": "f147c54fa9f01d8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----+------+-------------------+---------------+\n",
      "|        feature|              value|count|   pcg|cost_distance_label|cost_time_label|\n",
      "+---------------+-------------------+-----+------+-------------------+---------------+\n",
      "|passenger_count|2024-03-04T17:08:32|    1|2.0E-4|                0.5|            0.0|\n",
      "|passenger_count|2024-06-09T10:00:59|    1|2.0E-4|                0.5|           2.01|\n",
      "|passenger_count|2024-05-29T07:50:36|    1|2.0E-4|                0.5|            1.1|\n",
      "|passenger_count|2024-06-02T15:46:54|    1|2.0E-4|                1.5|            2.0|\n",
      "|passenger_count|2024-06-05T15:06:53|    1|2.0E-4|                0.5|            0.0|\n",
      "|passenger_count|2024-12-12T15:30:33|    1|2.0E-4|                0.5|            0.0|\n",
      "|passenger_count|2024-02-29T15:40:56|    1|2.0E-4|                0.5|           1.87|\n",
      "|passenger_count|2024-09-27T10:46:26|    1|2.0E-4|                0.5|            0.0|\n",
      "|passenger_count|2024-04-03T09:38:07|    1|2.0E-4|                0.5|            0.0|\n",
      "|passenger_count|2024-05-15T19:30:23|    1|2.0E-4|                0.5|            0.0|\n",
      "+---------------+-------------------+-----+------+-------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "headersForSchema: Seq[org.apache.spark.sql.types.StructField] = List(StructField(feature,StringType,true), StructField(value,StringType,true), StructField(count,IntegerType,true), StructField(pcg,DoubleType,true), StructField(cost_distance_label,StringType,true), StructField(cost_time_label,StringType,true))\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(feature,StringType,true),StructField(value,StringType,true),StructField(count,IntegerType,true),StructField(pcg,DoubleType,true),StructField(cost_distance_label,StringType,true),StructField(cost_time_label,StringType,true))\n",
       "dfForAnalysis: org.apache.spark.sql.DataFrame = [feature: string, value: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:57:29.312818Z",
     "start_time": "2025-05-13T16:57:29.311635Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d924cc6a632e2a1c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
