{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cefe988a9aff7538",
   "metadata": {},
   "source": [
    "# Big Data project A.Y. 2024-2025\n",
    "\n",
    "## Members\n",
    "\n",
    "- Giovanni Antonioni\n",
    "- Luca Rubboli - 0001083742\n",
    "\n",
    "## First job"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Define useful parameters\n",
    "\n",
    "- Dataset location\n",
    "- Iterator (defined like this to overcome different names for same columns in dataset)\n",
    "- Time zones for overnight calculation"
   ],
   "id": "72ce3e263b662de6"
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-04-22T20:32:45.606400Z",
     "start_time": "2025-04-22T20:32:29.757293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val datasetDir = \"../../../dataset/dataset_csv\"\n",
    "val yellowDatasetDir = s\"$datasetDir/dataset_yellow\"\n",
    "val greenDatasetDir = s\"$datasetDir/dataset_green\"\n",
    "val fhvDatasetDir = s\"$datasetDir/dataset_fhv\"\n",
    "val fhvhvDatasetDir = s\"$datasetDir/dataset_fhvhv\"\n",
    "val datasetDirMap: Map[String, String] = Map(\"yellow\" -> yellowDatasetDir, \"green\" -> greenDatasetDir,\n",
    "\"fhv\" -> fhvDatasetDir, \"fhvhv\" -> fhvhvDatasetDir)\n",
    "val datasetIterator: Iterable[(String, String, String)] = Seq(\n",
    "    (\"yellow\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\"),\n",
    "    (\"green\", \"lpep_dropoff_datetime\", \"lpep_pickup_datetime\"),\n",
    "    // (\"fhv\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\"),\n",
    "    // (\"fhvhv\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\"),\n",
    ")\n",
    "val outputDir = \"/output/firstJobOutput\"\n",
    "val timeZoneOver: String = \"overnight\"\n",
    "val timeZones = Map(timeZoneOver -> (20, 6), \"regular\" -> (6, 20))"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://mac.fritz.box:4040\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1745353954459)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "datasetDir: String = ../../../dataset/dataset_csv\n",
       "yellowDatasetDir: String = ../../../dataset/dataset_csv/dataset_yellow\n",
       "greenDatasetDir: String = ../../../dataset/dataset_csv/dataset_green\n",
       "fhvDatasetDir: String = ../../../dataset/dataset_csv/dataset_fhv\n",
       "fhvhvDatasetDir: String = ../../../dataset/dataset_csv/dataset_fhvhv\n",
       "datasetDirMap: Map[String,String] = Map(yellow -> ../../../dataset/dataset_csv/dataset_yellow, green -> ../../../dataset/dataset_csv/dataset_green, fhv -> ../../../dataset/dataset_csv/dataset_fhv, fhvhv -> ../../../dataset/dataset_csv/dataset_fhvhv)\n",
       "datasetIterator: Iterable[(String, String, String)] = List((yellow,tpep_dropoff_datetime,tpep_pickup_datetime), (green,lpep_dropoff_datetime,lpep_pickup_datetime))\n",
       "outputDir: String = /output/firstJobOutput\n",
       "timeZoneOver: St...\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Define preprocess rules\n",
    "- Filters for columns\n",
    "- Columns used in classification for average price calculation\n",
    "- Columns which values are used in analysis"
   ],
   "id": "6251e1a01e2d34a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T20:32:56.707484Z",
     "start_time": "2025-04-22T20:32:53.992285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.Column\n",
    "\n",
    "val featureFilters: Map[String, Column] = Map(\n",
    "    \"passenger_count\" -> (col(\"passenger_count\") > 0),\n",
    "    \"trip_distance\" -> (col(\"trip_distance\") > 0),\n",
    "    \"RatecodeID\" -> (col(\"RatecodeID\").between(1, 6) || col(\"RatecodeID\") === 99),\n",
    "    \"store_and_fwd_flag\" -> (col(\"store_and_fwd_flag\") === \"Y\" || col(\"store_and_fwd_flag\") === \"N\"),\n",
    "    \"payment_type\" -> col(\"payment_type\").between(1, 6),\n",
    "    \"fare_amount\" -> (col(\"fare_amount\") > 0),\n",
    "    \"tolls_amount\" -> (col(\"tolls_amount\") < 200),\n",
    ")\n",
    "val taxFilter: Column => Column = _ >= 0\n",
    "\n",
    "val colsForClassification: Seq[String] = Seq(\n",
    "    \"passenger_count\",\n",
    "    \"store_and_fwd_flag\",\n",
    "    \"payment_type\",\n",
    "    \"aggregate_fee_bin_label\",\n",
    "    \"duration_minutes_bin_label\",\n",
    "    \"trip_distance_bin_label\",\n",
    "    \"year\",\n",
    "    s\"duration_minutes_${timeZoneOver}_pcg_bin\"\n",
    ")\n",
    "val colsForValuesAnalysis: Seq[String] = Seq(\n",
    "    \"passenger_count\",\n",
    "    \"store_and_fwd_flag\",\n",
    "    \"payment_type\",\n",
    "    \"aggregate_fee_bin_label\",\n",
    "    \"duration_minutes_bin_label\",\n",
    "    \"trip_distance_bin_label\",\n",
    "    \"year\",\n",
    "    s\"duration_minutes_${timeZoneOver}_pcg_bin\"\n",
    ")"
   ],
   "id": "f3c66b94d590de92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Column\n",
       "featureFilters: Map[String,org.apache.spark.sql.Column] = Map(trip_distance -> (trip_distance > 0), tolls_amount -> (tolls_amount < 200), RatecodeID -> (((RatecodeID >= 1) AND (RatecodeID <= 6)) OR (RatecodeID = 99)), payment_type -> ((payment_type >= 1) AND (payment_type <= 6)), fare_amount -> (fare_amount > 0), passenger_count -> (passenger_count > 0), store_and_fwd_flag -> ((store_and_fwd_flag = Y) OR (store_and_fwd_flag = N)))\n",
       "taxFilter: org.apache.spark.sql.Column => org.apache.spark.sql.Column = $Lambda$2191/0x0000000800f50040@354a296e\n",
       "colsForClassification: Seq[String] = List(passenger_count, store_and_fwd_flag, payment_type, aggregate_fee_bin_label, duration_minutes_bin_label, trip_distance_bin_label, year, duration_minutes_overnight_pcg_bin)\n",
       "c...\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T20:32:58.924348Z",
     "start_time": "2025-04-22T20:32:58.328641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark\n",
    "\n",
    "val spark = SparkSession.builder\n",
    "  .appName(\"First job\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val sc = spark.SparkContext.getOrCreate()"
   ],
   "id": "4a10206a09016a01",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\n",
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@72686277\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "object CsvParser {\n",
    "\n",
    "    val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "\n",
    "    // (VendorID 0, pickup_time 1, dropoff_time 2, passengers 3, distance 4, ratecodeID 5, store_fwd 6, PUloc 7, DOloc 8, pay_type 9, fare_amount 10, extra 11, mta_tax 12, tip 13, tolls 14, improvement 15, tot_amount 16, cong_surcharge 17, airportfee 18)\n",
    "    def parseYellowDataLine(line: String): Option[(String, String, Int)] = {\n",
    "        try {\n",
    "            val input = line.split(commaRegex)\n",
    "\n",
    "            val intIndices = Set(0, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18)\n",
    "            val strIndices = Set(1, 2, 6)\n",
    "\n",
    "            val result = for {\n",
    "                i <- 0 to 18 if i != 15  // you skipped input(15)\n",
    "            } yield {\n",
    "                val value = i\n",
    "                if (intIndices.contains(i)) value else value\n",
    "            }\n",
    "\n",
    "            println(result)\n",
    "\n",
    "            Some(input(0).trim.toInt, input(1).trim, input(2).trim, input(3).trim.toInt, input(4).trim.toInt, input(5).trim.toInt, input(6).trim,\n",
    "                input(7).trim.toInt, input(8).trim.toInt, input(9).trim.toInt, input(10).trim.toInt, input(11).trim.toInt, input(12).trim.toInt, input(13).trim.toInt, input(14).trim.toInt, input(16).trim.toInt, input(17).trim.toInt, input(18).trim.toInt)\n",
    "        } catch {\n",
    "            case _: Exception => None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // (\"VendorID\" 0,\"lpep_pickup_datetime\" 1,\"lpep_dropoff_datetime\" 2,\"store_and_fwd_flag\" 3,\"RatecodeID\" 4,\"PULocationID\" 5,\"DOLocationID\" 6,\"passenger_count\" 7,\"trip_distance\" 8,\"fare_amount\" 9,\"extra\" 10,\"mta_tax\" 11,\"tip_amount\" 12,\"tolls_amount\" 13,\"ehail_fee\" 14,\"improvement_surcharge\" 15,\"total_amount\" 16,\"payment_type\" 17,\"trip_type\" 18,\"congestion_surcharge\" 19)\n",
    "    def parseGreenDataLine(line: String): Option[(String, String, Int)] = {\n",
    "        try {\n",
    "            val input = line.split(commaRegex)\n",
    "            Some(input(0).trim.toInt, input(1).trim, input(2).trim, input(3).trim, input(4).trim.toInt, input(5).trim.toInt, input(6).trim.toInt,\n",
    "            input(7).trim.toInt, input(8).trim.toInt, input(9).trim.toInt, input(10).trim.toInt, input(11).trim.toInt, input(12).trim.toInt, input(13).trim.toInt, input(15).trim.toInt, input(16).trim.toInt, input(17).trim.toInt, input(18).trim.toInt)\n",
    "        } catch {\n",
    "            case _: Exception => None\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "id": "822f764179babb48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T21:13:38.088404Z",
     "start_time": "2025-04-22T21:13:35.273081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "var yellowFile = spark.read\n",
    "  .parquet(Commons.getDatasetPath(deploymentMode, datasetDirMap(name)))\n",
    "\n",
    "dataset.columns.foreach(println)\n",
    "\n",
    "val myRdd = dataset.rdd\n",
    "\n",
    "val yellowFile = sc.textFile(f\"${greenDatasetDir}/green-tripdata-2024-01.csv\")\n",
    "val header = yellowFile.first() // primacol, secondacol, ...\n",
    "println(header)"
   ],
   "id": "cfb99aef020692d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"VendorID\",\"lpep_pickup_datetime\",\"lpep_dropoff_datetime\",\"store_and_fwd_flag\",\"RatecodeID\",\"PULocationID\",\"DOLocationID\",\"passenger_count\",\"trip_distance\",\"fare_amount\",\"extra\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\"ehail_fee\",\"improvement_surcharge\",\"total_amount\",\"payment_type\",\"trip_type\",\"congestion_surcharge\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "yellowFile: org.apache.spark.rdd.RDD[String] = ../../../dataset/dataset_csv/dataset_green/green-tripdata-2024-01.csv MapPartitionsRDD[5] at textFile at <console>:29\n",
       "header: String = \"VendorID\",\"lpep_pickup_datetime\",\"lpep_dropoff_datetime\",\"store_and_fwd_flag\",\"RatecodeID\",\"PULocationID\",\"DOLocationID\",\"passenger_count\",\"trip_distance\",\"fare_amount\",\"extra\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\"ehail_fee\",\"improvement_surcharge\",\"total_amount\",\"payment_type\",\"trip_type\",\"congestion_surcharge\"\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T20:33:02.765663Z",
     "start_time": "2025-04-22T20:33:01.187794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val data = yellowFile.filter(_ != header)\n",
    "val parsed = data.map(_.split(\",\"))\n"
   ],
   "id": "fd5e2b3a30245b1c",
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Task not serializable",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Task not serializable",
      "  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:444)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:416)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2674)",
      "  at org.apache.spark.rdd.RDD.$anonfun$filter$1(RDD.scala:435)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)",
      "  at org.apache.spark.rdd.RDD.filter(RDD.scala:434)",
      "  ... 38 elided",
      "Caused by: java.io.NotSerializableException: org.apache.spark.SparkContext",
      "Serialization stack:",
      "\t- object not serializable (class: org.apache.spark.SparkContext, value: org.apache.spark.SparkContext@72686277)",
      "\t- field (class: $iw, name: sc, type: class org.apache.spark.SparkContext)",
      "\t- object (class $iw, $iw@15805d33)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@8880635)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@3287b405)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@4bbae3a4)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@48dbbf0a)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@7323b41f)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@6271463e)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@3d2381cd)",
      "\t- field (class: $read, name: $iw, type: class $iw)",
      "\t- object (class $read, $read@6c9bbb8)",
      "\t- field (class: $iw, name: read, type: class $read)",
      "\t- object (class $iw, $iw@33413967)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@22d31642)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@7f837581)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@4043bc20)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@662c4f84)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@4dac6aca)",
      "\t- field (class: $iw, name: $iw, type: class $iw)",
      "\t- object (class $iw, $iw@78e55a35)",
      "\t- field (class: $read, name: $iw, type: class $iw)",
      "\t- object (class $read, $read@302f8470)",
      "\t- field (class: $iw, name: read, type: class $read)",
      "\t- object (class $iw, $iw@51e30af0)",
      "\t- field (class: $iw, name: $outer, type: class $iw)",
      "\t- object (class $iw, $iw@3bda937e)",
      "\t- element of array (index: 0)",
      "\t- array (class [Ljava.lang.Object;, size 1)",
      "\t- field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;)",
      "\t- object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class $iw, functionalInterfaceMethod=scala/Function1.apply:(Ljava/lang/Object;)Ljava/lang/Object;, implementation=invokeStatic $anonfun$data$1$adapted:(L$iw;Ljava/lang/String;)Ljava/lang/Object;, instantiatedMethodType=(Ljava/lang/String;)Ljava/lang/Object;, numCaptured=1])",
      "\t- writeReplace data (class: java.lang.invoke.SerializedLambda)",
      "\t- object (class $Lambda$2870/0x00000008011b9040, $Lambda$2870/0x00000008011b9040@1e9a2ebc)",
      "  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)",
      "  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:49)",
      "  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)",
      "  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:441)",
      "  ... 46 more",
      ""
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7a05a60d734daf56"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
