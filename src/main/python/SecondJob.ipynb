{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Big Data project A.Y. 2024-2025\n",
    "\n",
    "## Members\n",
    "\n",
    "- Giovanni Antonioni\n",
    "- Luca Rubboli - 0001083742\n",
    "\n",
    "## Second job"
   ],
   "id": "562ba4dfdebe9b45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:41:38.602520Z",
     "start_time": "2025-07-06T13:41:25.905648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder\n",
    "  .appName(\"Second job with RDDs\")\n",
    "  .getOrCreate()\n",
    "\n",
    "\n",
    "val sc = spark.sparkContext"
   ],
   "id": "69d266c225a59a5d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://localhost:4040\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1751809290250)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7f2fa690\n",
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@1c395d8c\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Definition of parameters for the job\n",
    "\n",
    "Here are defined the variables used for the snippet.\n"
   ],
   "id": "572fc8ae7ed1263c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:41:39.033737Z",
     "start_time": "2025-07-06T13:41:38.650747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val datasetName = \"green\"\n",
    "val datasetFolder = \"./dataset\"\n",
    "val outputDir = s\"/output/secondJobRDD/$datasetName\"\n",
    "val pathToFiles = s\"$datasetFolder/$datasetName\"\n",
    "val weatherData = s\"$datasetFolder/weather/weather_data_2017_2024.csv\"\n",
    "val weatherWmoLookup = s\"$datasetFolder/weather/wmo_lookup_codes.csv\""
   ],
   "id": "3a3e52e6a1ed4e06",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasetName: String = green\n",
       "datasetFolder: String = ./dataset\n",
       "outputDir: String = /output/secondJobRDD/green\n",
       "pathToFiles: String = ./dataset/green\n",
       "weatherData: String = ./dataset/weather/weather_data_2017_2024.csv\n",
       "weatherWmoLookup: String = ./dataset/weather/wmo_lookup_codes.csv\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Columns for the analysis",
   "id": "34fe46529ca49fad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:41:39.566198Z",
     "start_time": "2025-07-06T13:41:39.065284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val commonFields = List(\n",
    "  StructField(\"VendorID\", IntegerType),\n",
    "  StructField(\"fare_amount\", DoubleType),\n",
    "  StructField(\"tip_amount\", DoubleType),\n",
    "  StructField(\"payment_type\", LongType),\n",
    "  StructField(\"trip_distance\", DoubleType),\n",
    "  StructField(\"total_amount\", DoubleType)\n",
    ")\n",
    "\n",
    "val schemaYellow = StructType(\n",
    "  StructField(\"tpep_pickup_datetime\", TimestampType) ::\n",
    "  StructField(\"tpep_dropoff_datetime\", TimestampType) ::\n",
    "  commonFields\n",
    ")\n",
    "\n",
    "val schemaGreen = StructType(\n",
    "  StructField(\"lpep_pickup_datetime\", TimestampType) ::\n",
    "  StructField(\"lpep_dropoff_datetime\", TimestampType) ::\n",
    "  commonFields\n",
    ")"
   ],
   "id": "1a46b90b7485a526",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "commonFields: List[org.apache.spark.sql.types.StructField] = List(StructField(VendorID,IntegerType,true), StructField(fare_amount,DoubleType,true), StructField(tip_amount,DoubleType,true), StructField(payment_type,LongType,true), StructField(trip_distance,DoubleType,true), StructField(total_amount,DoubleType,true))\n",
       "schemaYellow: org.apache.spark.sql.types.StructType = StructType(StructField(tpep_pickup_datetime,TimestampType,true),StructField(tpep_dropoff_datetime,TimestampType,true),StructField(VendorID,IntegerType,true),StructField(fare_amount,DoubleType,true),StructField(tip_amount,DoubleType,true),StructField(payment_type,LongType,true),StructField(trip_distance,DoubleType,true),StructField(total_amount,DoubleType,true))\n",
       "schemaGreen: org.apache.sp...\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Load Datasets\n",
    "\n",
    "First we want to load the dataset relative to the taxi data."
   ],
   "id": "b9d6ad666c105aab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:41:39.895356Z",
     "start_time": "2025-07-06T13:41:39.583526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val projectDir: String = \"/Users/giovanniantonioni/IdeaProjects/Drivers\"\n",
    "def getDatasetPath(localPath: String): String = {\n",
    "  \"file://\" + projectDir + \"/\" + localPath\n",
    "}"
   ],
   "id": "834fad309135f509",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "projectDir: String = /Users/giovanniantonioni/IdeaProjects/Drivers\n",
       "getDatasetPath: (localPath: String)String\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:41:47.789312Z",
     "start_time": "2025-07-06T13:41:39.915153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val (schema, pickupCol, dropoffCol) = datasetName match {\n",
    "  case \"yellow\" => (schemaYellow, \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\")\n",
    "  case _        => (schemaGreen, \"lpep_pickup_datetime\", \"lpep_dropoff_datetime\")\n",
    "}\n",
    "\n",
    "val loadedDataset = spark.read\n",
    "  .schema(schema)\n",
    "  .option(\"recursiveFileLookup\", \"true\")\n",
    "  .parquet(getDatasetPath(pathToFiles))\n",
    "  .select(\n",
    "    $\"VendorID\",\n",
    "    col(pickupCol).alias(\"pickup_datetime\"),\n",
    "    col(dropoffCol).alias(\"dropoff_datetime\"),\n",
    "    $\"fare_amount\",\n",
    "    $\"tip_amount\",\n",
    "    $\"payment_type\",\n",
    "    $\"trip_distance\",\n",
    "    $\"total_amount\"\n",
    "  )\n",
    "  .na.drop()\n",
    "  .dropDuplicates()\n",
    "  .rdd"
   ],
   "id": "508836e80d246fc1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(lpep_pickup_datetime,TimestampType,true),StructField(lpep_dropoff_datetime,TimestampType,true),StructField(VendorID,IntegerType,true),StructField(fare_amount,DoubleType,true),StructField(tip_amount,DoubleType,true),StructField(payment_type,LongType,true),StructField(trip_distance,DoubleType,true),StructField(total_amount,DoubleType,true))\n",
       "pickupCol: String = lpep_pickup_datetime\n",
       "dropoffCol: String = lpep_dropoff_datetime\n",
       "loadedDataset: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[8] at rdd at <console>:50\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n",
   "id": "2c5c97a56b23c4d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Filtering\n",
   "id": "2ef47d3e2b55cb20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:41:48.331155Z",
     "start_time": "2025-07-06T13:41:47.812186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "def filterDataset(dataset: RDD[Row], name: String): RDD[Row] = {\n",
    "  val allowedYellowVendorId = Set(1, 2, 6, 7)\n",
    "  val allowedGreenVendorId = Set(1, 2, 6)\n",
    "\n",
    "  dataset.filter { case row =>\n",
    "        val allowedIds = if (name == \"yellow\") allowedYellowVendorId else allowedGreenVendorId\n",
    "        val vendorId = row.getInt(0)\n",
    "        allowedIds.contains(vendorId)\n",
    "      }\n",
    "      .filter(row => row.getDouble(3) > 0)\n",
    "      .filter(row => row.getDouble(4) >= 0)\n",
    "      .filter(row => row.getDouble(4) <= row.getDouble(3) * 1.5)\n",
    "      .filter(row => row.getDouble(6) > 0)\n",
    "      .filter{ row =>\n",
    "        val dropOffDateTime = row.getTimestamp(2)\n",
    "        val pickupDateTime = row.getTimestamp(1)\n",
    "        dropOffDateTime.after(pickupDateTime)\n",
    "      }\n",
    "}\n",
    "\n",
    "val filtered = filterDataset(loadedDataset, datasetName)"
   ],
   "id": "5f5978230df8424",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.rdd.RDD\n",
       "filterDataset: (dataset: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], name: String)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "filtered: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[14] at filter at <console>:45\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:41:48.728959Z",
     "start_time": "2025-07-06T13:41:48.347806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val withTripDuration = filtered.map { row =>\n",
    "  val durationMin = (row.getTimestamp(2).getTime - row.getTimestamp(1).getTime).toDouble / (1000 * 60)\n",
    "  (row, durationMin)\n",
    "}"
   ],
   "id": "4f8db2e31b1828dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "withTripDuration: org.apache.spark.rdd.RDD[(org.apache.spark.sql.Row, Double)] = MapPartitionsRDD[15] at map at <console>:30\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:41:52.208838Z",
     "start_time": "2025-07-06T13:41:48.756698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val tripDistances = withTripDuration.map { case (row, _) => row.getLong(5).toInt }\n",
    "val tripDurations = withTripDuration.map { case (_, duration) => duration }\n",
    "\n",
    "val distanceDF = tripDistances.toDF(\"trip_distance\")\n",
    "val durationDF = tripDurations.toDF(\"trip_duration\")\n",
    "\n",
    "val tripDistanceOutlier = distanceDF.stat.approxQuantile(\"trip_distance\", Array(0.02, 0.98), 0.01)\n",
    "val tripDurationOutlier = durationDF.stat.approxQuantile(\"trip_duration\", Array(0.02, 0.98), 0.01)\n"
   ],
   "id": "b0b19e08ccc143ae",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tripDistances: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[16] at map at <console>:30\n",
       "tripDurations: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[17] at map at <console>:31\n",
       "distanceDF: org.apache.spark.sql.DataFrame = [trip_distance: int]\n",
       "durationDF: org.apache.spark.sql.DataFrame = [trip_duration: double]\n",
       "tripDistanceOutlier: Array[Double] = Array(1.0, 2.0)\n",
       "tripDurationOutlier: Array[Double] = Array(2.25, 42.083333333333336)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:41:52.771268Z",
     "start_time": "2025-07-06T13:41:52.240919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filterOutOutlier(\n",
    "  dataset: RDD[(Row, Double)],\n",
    "  tripDistanceOutlier: Array[Double],\n",
    "  tripDurationOutlier: Array[Double]\n",
    "): RDD[(Row, Double)] = {\n",
    "\n",
    "  val distanceLower = tripDistanceOutlier(0)\n",
    "  val distanceUpper = tripDistanceOutlier(1)\n",
    "  val durationLower = tripDurationOutlier(0)\n",
    "  val durationUpper = tripDurationOutlier(1)\n",
    "\n",
    "  dataset.filter { case (row, duration) =>\n",
    "    val tripDistance = row.getLong(5).toInt\n",
    "    tripDistance >= distanceLower && tripDistance <= distanceUpper &&\n",
    "    duration >= durationLower && duration <= durationUpper\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "val filteredOut = filterOutOutlier(withTripDuration, tripDistanceOutlier, tripDurationOutlier)\n",
    "val enriched = filteredOut.map { case (row, duration) =>\n",
    "  val pickupCalendar = java.util.Calendar.getInstance()\n",
    "  pickupCalendar.setTime(row.getTimestamp(1))\n",
    "\n",
    "  val hourOfDay = pickupCalendar.get(java.util.Calendar.HOUR_OF_DAY)\n",
    "\n",
    "  val tipAmount = row.getDouble(4)\n",
    "  val totalAmount = row.getDouble(7)\n",
    "\n",
    "  val tipPercentage = if (totalAmount != 0) (tipAmount / totalAmount) * 100 else 0.0\n",
    "\n",
    "  val tripDistance = row.getDouble(6)\n",
    "  val speedMph = if (duration > 0) tripDistance / (duration / 60.0) else 0.0\n",
    "\n",
    "  (row, duration, hourOfDay, tipPercentage, speedMph)\n",
    "}"
   ],
   "id": "1127139b9d75cad3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterOutOutlier: (dataset: org.apache.spark.rdd.RDD[(org.apache.spark.sql.Row, Double)], tripDistanceOutlier: Array[Double], tripDurationOutlier: Array[Double])org.apache.spark.rdd.RDD[(org.apache.spark.sql.Row, Double)]\n",
       "filteredOut: org.apache.spark.rdd.RDD[(org.apache.spark.sql.Row, Double)] = MapPartitionsRDD[36] at filter at <console>:43\n",
       "enriched: org.apache.spark.rdd.RDD[(org.apache.spark.sql.Row, Double, Int, Double, Double)] = MapPartitionsRDD[37] at map at <console>:52\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:41:52.798773Z",
     "start_time": "2025-07-06T13:41:52.794220Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "76820f82f5a4cedb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:41:53.572979Z",
     "start_time": "2025-07-06T13:41:52.833712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val binned =  enriched.map { case (row, duration, hourOfDay, tipPercentage, speedMph) =>\n",
    "  val binConfigs = Map(\n",
    "    \"trip_distance\" -> (Seq(1.0, 3.0, 6.0), Seq(\"0-1\", \"1-3\", \"3-6\", \"6+\")),\n",
    "    \"trip_duration_min\" -> (Seq(5.0, 15.0, 30.0), Seq(\"0-5\", \"5-15\", \"15-30\", \"30+\")),\n",
    "    \"fare_amount\" -> (Seq(5.0, 10.0, 20.0, 40.0), Seq(\"0-5\", \"5-10\", \"10-20\", \"20-40\", \"40+\")),\n",
    "    \"tip_percentage\" -> (Seq(5.0, 10.0, 20.0, 30.0), Seq(\"0-5%\", \"5-10%\", \"10-20%\", \"20-30%\", \"30%+\")),\n",
    "    \"speed_mph\" -> (Seq(5.0, 15.0, 30.0), Seq(\"0-5mph\", \"5-15mph\", \"15-30mph\", \"30mph+\"))\n",
    "  )\n",
    "\n",
    "  def assignBin(value: Double, bins: Seq[Double], labels: Seq[String]): String = {\n",
    "    require(labels.length == bins.length + 1, \"You need one more label than bin thresholds.\")\n",
    "\n",
    "    if (value < bins.head) labels.head\n",
    "    else {\n",
    "      val idx = bins.indexWhere(b => value < b)\n",
    "      if (idx == -1) labels.last\n",
    "      else labels(idx)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  val tripDistance = row.getDouble(6)\n",
    "  val tripDistanceBin = assignBin(\n",
    "    tripDistance,\n",
    "    binConfigs(\"trip_distance\")._1,\n",
    "    binConfigs(\"trip_distance\")._2\n",
    "  )\n",
    "\n",
    "  val tripDurationBin = assignBin(\n",
    "    duration,\n",
    "    binConfigs(\"trip_duration_min\")._1,\n",
    "    binConfigs(\"trip_duration_min\")._2\n",
    "  )\n",
    "\n",
    "  val fareAmount = row.getDouble(3)\n",
    "  val fareAmountBin = assignBin(\n",
    "    fareAmount,\n",
    "    binConfigs(\"fare_amount\")._1,\n",
    "    binConfigs(\"fare_amount\")._2\n",
    "  )\n",
    "\n",
    "  val speedBin = assignBin(\n",
    "    speedMph,\n",
    "    binConfigs(\"speed_mph\")._1,\n",
    "    binConfigs(\"speed_mph\")._2\n",
    "  )\n",
    "\n",
    "  def tripHourBucket(hour: Int): String = hour match {\n",
    "    case h if h >= 0 && h <= 5  => \"late_night\"\n",
    "    case h if h >= 6 && h <= 9  => \"morning\"\n",
    "    case h if h >= 10 && h <= 15 => \"midday\"\n",
    "    case h if h >= 16 && h <= 19 => \"evening\"\n",
    "    case _ => \"night\"\n",
    "  }\n",
    "\n",
    "  val hourBin = tripHourBucket(hourOfDay)\n",
    "  (row, tripDistanceBin, tripDurationBin, fareAmountBin, speedBin, hourBin, tipPercentage)\n",
    "}"
   ],
   "id": "dc6ac93429a89172",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binned: org.apache.spark.rdd.RDD[(org.apache.spark.sql.Row, String, String, String, String, String, Double)] = MapPartitionsRDD[38] at map at <console>:30\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:41:54.738273Z",
     "start_time": "2025-07-06T13:41:53.591880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val weatherFileRDD = spark.read\n",
    "  .format(\"CSV\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(getDatasetPath(weatherData))\n",
    "  .rdd\n",
    "  .map { row =>\n",
    "    val code = row.getString(1).trim.toInt\n",
    "    val date = row.getString(0).trim\n",
    "    (code, date)\n",
    "  }\n",
    "\n",
    "val wmoLookupFile = spark.read\n",
    "  .format(\"CSV\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(getDatasetPath(weatherWmoLookup))\n",
    "  .rdd\n",
    "\n",
    "val wmoLookupPairRDD = wmoLookupFile.map { row =>\n",
    "  val data = row.getString(0).split(\";\")\n",
    "  val code =data(0).trim.toInt\n",
    "  val description = data(1).trim\n",
    "  (code, description)\n",
    "}\n",
    "\n",
    "import java.time.LocalDate\n",
    "import java.sql.Timestamp\n",
    "\n",
    "val transformedWeatherClassRDD = weatherFileRDD\n",
    "  .join(wmoLookupPairRDD)\n",
    "  .map(row => {\n",
    "    val (id, (date, description)) = row\n",
    "    val formattedDate  = LocalDate.parse(date)\n",
    "    val timestamp = Timestamp.valueOf(formattedDate.atStartOfDay())\n",
    "    (id, timestamp, description)\n",
    "})\n"
   ],
   "id": "91a1d801e21bde7a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weatherFileRDD: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[54] at map at <console>:38\n",
       "wmoLookupFile: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[69] at rdd at <console>:47\n",
       "wmoLookupPairRDD: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[70] at map at <console>:50\n",
       "import java.time.LocalDate\n",
       "import java.sql.Timestamp\n",
       "transformedWeatherClassRDD: org.apache.spark.rdd.RDD[(Int, java.sql.Timestamp, String)] = MapPartitionsRDD[74] at map at <console>:62\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:41:55.084706Z",
     "start_time": "2025-07-06T13:41:54.758510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val weatherByDate = transformedWeatherClassRDD.map { row =>\n",
    "  (row._2.toLocalDateTime.toLocalDate, row)\n",
    "}\n",
    "\n",
    "val rideByDate = binned.map {data  =>\n",
    "  val pickupDateTime = data._1.getTimestamp(1).toLocalDateTime.toLocalDate\n",
    "  (pickupDateTime, data)\n",
    "}"
   ],
   "id": "9c007802694ba58f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weatherByDate: org.apache.spark.rdd.RDD[(java.time.LocalDate, (Int, java.sql.Timestamp, String))] = MapPartitionsRDD[75] at map at <console>:33\n",
       "rideByDate: org.apache.spark.rdd.RDD[(java.time.LocalDate, (org.apache.spark.sql.Row, String, String, String, String, String, Double))] = MapPartitionsRDD[76] at map at <console>:37\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Join weather and Ride data\n",
   "id": "16125d5aa598f314"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:41:55.522624Z",
     "start_time": "2025-07-06T13:41:55.101194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val joinedWeather = rideByDate.join(weatherByDate).map {\n",
    "  case (_, (ride, weather)) => (ride, weather)\n",
    "}\n",
    "\n",
    "val finalRDD = joinedWeather.map { case (ride, weather) =>\n",
    "  def generalWeatherLabel(wmoCode: Int): String = wmoCode match {\n",
    "    case c if Seq(0, 1).contains(c)              => \"clear\"\n",
    "    case c if Seq(2, 3, 4).contains(c)           => \"cloudy\"\n",
    "    case c if Seq(45, 48).contains(c)            => \"foggy\"\n",
    "    case c if (50 to 67).contains(c)       => \"rainy\"\n",
    "    case c if (70 to 77).contains(c)       => \"snowy\"\n",
    "    case c if (80 to 99).contains(c)       => \"stormy\"\n",
    "    case _                                       => \"unknown\"\n",
    "  }\n",
    "\n",
    "  val generalWeather = generalWeatherLabel(weather._1)\n",
    "  (ride, weather._1, generalWeather)\n",
    "}"
   ],
   "id": "6b5df61c5a95c87b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinedWeather: org.apache.spark.rdd.RDD[((org.apache.spark.sql.Row, String, String, String, String, String, Double), (Int, java.sql.Timestamp, String))] = MapPartitionsRDD[80] at map at <console>:33\n",
       "finalRDD: org.apache.spark.rdd.RDD[((org.apache.spark.sql.Row, String, String, String, String, String, Double), Int, String)] = MapPartitionsRDD[81] at map at <console>:37\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Export the results",
   "id": "96e5c4043bdad273"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:41:55.949861Z",
     "start_time": "2025-07-06T13:41:55.542456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val binFields = Seq(\n",
    "  \"tripDistanceBin\",\n",
    "  \"tripDurationBin\",\n",
    "  \"fareAmountBin\",\n",
    "  \"speedBin\"\n",
    ")\n",
    "\n",
    "val keyedRDD = finalRDD.flatMap{ case (ride, code, generalWeather) =>\n",
    "  binFields.map { field =>\n",
    "    val bin = field match {\n",
    "      case \"fareAmountBin\" => ride._4\n",
    "      case \"tripDistanceBin\" => ride._2\n",
    "      case \"tripDurationBin\" => ride._3\n",
    "      case \"speedBin\" => ride._5\n",
    "    }\n",
    "    val key = s\"${field}_$bin\"\n",
    "    (key, ride)\n",
    "  }\n",
    "}\n"
   ],
   "id": "d4b4cddc458351ef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binFields: Seq[String] = List(tripDistanceBin, tripDurationBin, fareAmountBin, speedBin)\n",
       "keyedRDD: org.apache.spark.rdd.RDD[(String, (org.apache.spark.sql.Row, String, String, String, String, String, Double))] = MapPartitionsRDD[82] at flatMap at <console>:39\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:42:11.062531Z",
     "start_time": "2025-07-06T13:41:55.979221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val distributedKeyedRDD = keyedRDD.mapValues{ case ride =>\n",
    "  val tipPercentage = ride._7\n",
    "  (tipPercentage, 1L)\n",
    "}\n",
    "\n",
    "val avgTipRDD = distributedKeyedRDD\n",
    "  .reduceByKey((a, b) => (a._1 + b._1, a._2 + b._2))\n",
    "  .map {\n",
    "    case (id, (sum, count)) => Row(id, sum / count)\n",
    "  }\n",
    "\n",
    "\n",
    "val allTipByBinSchema = StructType(Seq(\n",
    "  StructField(\"feature\", StringType),\n",
    "  StructField(\"avg_tip_pct\", DoubleType)\n",
    "))\n",
    "\n",
    "spark.createDataFrame(avgTipRDD, allTipByBinSchema)\n",
    ".write\n",
    ".mode(\"overwrite\")\n",
    ".parquet(getDatasetPath(s\"$outputDir/tip_avg_per_bin/all_features\"))\n"
   ],
   "id": "e7428c29f1ae2cc0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distributedKeyedRDD: org.apache.spark.rdd.RDD[(String, (Double, Long))] = MapPartitionsRDD[83] at mapValues at <console>:35\n",
       "avgTipRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[85] at map at <console>:42\n",
       "allTipByBinSchema: org.apache.spark.sql.types.StructType = StructType(StructField(feature,StringType,true),StructField(avg_tip_pct,DoubleType,true))\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T13:42:17.734419Z",
     "start_time": "2025-07-06T13:42:11.093501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val avgTipByWeather = finalRDD\n",
    "    .map(r => (r._3, (r._1._7, 1L)))\n",
    "    .reduceByKey((a, b) => (a._1 + b._1, a._2 + b._2))\n",
    "    .map { case (weather, (sumTip, count)) => Row(weather, sumTip / count) }\n",
    "\n",
    "val weatherSchema = StructType(Seq(\n",
    "  StructField(\"weather\", StringType),\n",
    "  StructField(\"avg_tip_pct\", DoubleType)\n",
    "))\n",
    "\n",
    "spark.createDataFrame(avgTipByWeather, weatherSchema)\n",
    "  .write\n",
    "  .mode(\"overwrite\")\n",
    "  .parquet(getDatasetPath(s\"$outputDir/avg_tip_by_weather\"))"
   ],
   "id": "498f0a7294af7370",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgTipByWeather: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[91] at map at <console>:38\n",
       "weatherSchema: org.apache.spark.sql.types.StructType = StructType(StructField(weather,StringType,true),StructField(avg_tip_pct,DoubleType,true))\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
