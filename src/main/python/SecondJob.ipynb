{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Big Data project A.Y. 2024-2025\n",
    "\n",
    "## Members\n",
    "\n",
    "- Giovanni Antonioni\n",
    "- Luca Rubboli - 0001083742\n",
    "\n",
    "## Second job"
   ],
   "id": "562ba4dfdebe9b45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T20:31:54.423482Z",
     "start_time": "2025-04-16T20:31:54.141664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SparkSession"
   ],
   "id": "1520dfc7ff08795b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T18:53:29.856449Z",
     "start_time": "2025-04-14T18:53:29.465842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// DO\n",
    "NOT\n",
    "EXECUTE - this is needed\n",
    "just\n",
    "to\n",
    "avoid\n",
    "showing\n",
    "errors in the\n",
    "following\n",
    "cells\n",
    "sc = spark.SparkContext.getOrCreate()"
   ],
   "id": "a469215f165bfba1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@595c127b\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T20:20:36.141699Z",
     "start_time": "2025-04-16T20:20:34.418874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val\n",
    "spark = SparkSession.builder\n",
    ".appName(\"First job\")\n",
    ".getOrCreate()"
   ],
   "id": "a8f0f1e80087b6fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2b3ffbe9\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Load Dataset\n",
    "Define a custom function for loading the parquet files"
   ],
   "id": "2aedbb13065ce3a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T20:20:42.754001Z",
     "start_time": "2025-04-16T20:20:42.193513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "\n",
    "def dfFromParquetFolder(folderPath: String): DataFrame = spark.read\n",
    "\n",
    ".option(\"recursiveFileLookup\", \"true\")\n",
    ".parquet(folderPath)\n"
   ],
   "id": "12d34d17036a03ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "dfFromParquetFolder: (folderPath: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We set here the path to the files",
   "id": "4d19026dabcdb808"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T20:20:45.352442Z",
     "start_time": "2025-04-16T20:20:44.832792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val\n",
    "datasetFolder = \"../../../dataset/\"\n",
    "val\n",
    "yellowCab = s\n",
    "\"$datasetFolder/yellow_cab/\"\n",
    "val\n",
    "greenCab = s\n",
    "\"$datasetFolder/green_cab/\"\n"
   ],
   "id": "e58045172e76e450",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasetFolder: String = ../../../dataset/\n",
       "yellowCab: String = ../../../dataset//yellow_cab/\n",
       "greenCab: String = ../../../dataset//green_cab/\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Cleanup\n",
    "\n",
    "Now we start to cleanup the various dataset that are present in our project."
   ],
   "id": "c5f4e4f5572011c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T20:20:51.717853Z",
     "start_time": "2025-04-16T20:20:48.086796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val\n",
    "dfYellowCab = dfFromParquetFolder(yellowCab)\n",
    "val\n",
    "dfDroppedNa = dfYellowCab.na.drop()"
   ],
   "id": "356e384b03ad6e9a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfYellowCab: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: timestamp_ntz ... 17 more fields]\n",
       "dfDroppedNa: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: timestamp_ntz ... 17 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T20:34:18.562081Z",
     "start_time": "2025-04-16T20:32:36.128076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "val\n",
    "dfFinal = dfDroppedNa.withColumn(\"VendorID\", dfDroppedNa(\"VendorID\").cast(\"int\"))\n",
    "dfFinal.count()"
   ],
   "id": "831497fc9cf12d47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[848.322s][warning][gc,alloc] Executor task launch worker for task 6.0 in stage 2.0 (TID 24): Retried waiting for GCLocker too often allocating 121115 words\n",
      "[848.328s][warning][gc,alloc] Executor task launch worker for task 10.0 in stage 2.0 (TID 28): Retried waiting for GCLocker too often allocating 131221 words\n",
      "[848.334s][warning][gc,alloc] Executor task launch worker for task 11.0 in stage 2.0 (TID 29): Retried waiting for GCLocker too often allocating 12122 words\n",
      "[848.335s][warning][gc,alloc] Executor task launch worker for task 1.0 in stage 2.0 (TID 19): Retried waiting for GCLocker too often allocating 10292 words\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e801bddaa66d7691"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
