{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Big Data project A.Y. 2024-2025\n",
    "\n",
    "## Members\n",
    "\n",
    "- Giovanni Antonioni\n",
    "- Luca Rubboli - 0001083742\n",
    "\n",
    "## Second job"
   ],
   "id": "562ba4dfdebe9b45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:59:22.328400Z",
     "start_time": "2025-06-03T07:59:21.900602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder\n",
    "    .appName(\"Second job optimization\")\n",
    "    .getOrCreate()\n",
    "\n",
    "val sc = spark.sparkContext"
   ],
   "id": "69d266c225a59a5d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@66755c3\n",
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@4fe78b9\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Definition of parameters for the job\n",
    "\n",
    "Here are defined the variables used for the snippet.\n"
   ],
   "id": "572fc8ae7ed1263c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:59:22.601932Z",
     "start_time": "2025-06-03T07:59:22.412971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val datasetFolder = \"./dataset\"\n",
    "val outputDir = \"/output/secondJobRDD\"\n",
    "val yellowCab = s\"$datasetFolder/yellow_cab\"\n",
    "val greenCab = s\"$datasetFolder/green_cab\"\n",
    "val weatherData = s\"$datasetFolder/weather/weather_data_2017_2024.csv\"\n",
    "val weatherWmoLookup = s\"$datasetFolder/weather/wmo_lookup_codes.csv\""
   ],
   "id": "3a3e52e6a1ed4e06",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasetFolder: String = ./dataset\n",
       "outputDir: String = /output/secondJobRDD\n",
       "yellowCab: String = ./dataset/yellow_cab\n",
       "greenCab: String = ./dataset/green_cab\n",
       "weatherData: String = ./dataset/weather/weather_data_2017_2024.csv\n",
       "weatherWmoLookup: String = ./dataset/weather/wmo_lookup_codes.csv\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Columns for the analysis",
   "id": "34fe46529ca49fad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:59:22.868526Z",
     "start_time": "2025-06-03T07:59:22.610989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val commonFields = List(\n",
    "  StructField(\"VendorID\", IntegerType),\n",
    "  StructField(\"fare_amount\", DoubleType),\n",
    "  StructField(\"tip_amount\", DoubleType),\n",
    "  StructField(\"payment_type\", LongType),\n",
    "  StructField(\"trip_distance\", DoubleType),\n",
    "  StructField(\"total_amount\", DoubleType),\n",
    "  StructField(\"passenger_count\", LongType)\n",
    ")\n",
    "\n",
    "val schemaYellow = StructType(\n",
    "  StructField(\"tpep_pickup_datetime\", TimestampType) ::\n",
    "  StructField(\"tpep_dropoff_datetime\", TimestampType) ::\n",
    "  commonFields\n",
    ")\n",
    "\n",
    "val schemaGreen = StructType(\n",
    "  StructField(\"lpep_pickup_datetime\", TimestampType) ::\n",
    "  StructField(\"lpep_dropoff_datetime\", TimestampType) ::\n",
    "  commonFields\n",
    ")"
   ],
   "id": "12672e3455f6f34c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "commonFields: List[org.apache.spark.sql.types.StructField] = List(StructField(VendorID,IntegerType,true), StructField(fare_amount,DoubleType,true), StructField(tip_amount,DoubleType,true), StructField(payment_type,LongType,true), StructField(trip_distance,DoubleType,true), StructField(total_amount,DoubleType,true), StructField(passenger_count,LongType,true))\n",
       "schemaYellow: org.apache.spark.sql.types.StructType = StructType(StructField(tpep_pickup_datetime,TimestampType,true),StructField(tpep_dropoff_datetime,TimestampType,true),StructField(VendorID,IntegerType,true),StructField(fare_amount,DoubleType,true),StructField(tip_amount,DoubleType,true),StructField(payment_type,LongType,true),StructField(trip_distance,DoubleType,true),StructField(total_amount,...\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Load Datasets\n",
    "\n",
    "First we want to load the dataset relative to the taxi data."
   ],
   "id": "b9d6ad666c105aab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:59:23.177458Z",
     "start_time": "2025-06-03T07:59:22.886325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import java.sql.Timestamp\n",
    "\n",
    "case class Ride(\n",
    "  vendorId: Int,\n",
    "  pickupDatetime: Timestamp,\n",
    "  dropoffDatetime: Timestamp,\n",
    "  fareAmount: Double,\n",
    "  tipAmount: Double,\n",
    "  paymentType: Int,\n",
    "  tripDistance: Double,\n",
    "  totalAmount: Double,\n",
    "  passengerCount: Int,\n",
    "  serviceType: String\n",
    ")"
   ],
   "id": "ac8f1cddcf1f2d31",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.sql.Timestamp\n",
       "defined class Ride\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:59:23.412367Z",
     "start_time": "2025-06-03T07:59:23.201643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val projectDir: String = \"/Users/giovanniantonioni/IdeaProjects/Drivers\"\n",
    "def getDatasetPath(localPath: String): String = {\n",
    "  \"file://\" + projectDir + \"/\" + localPath\n",
    "}"
   ],
   "id": "834fad309135f509",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "projectDir: String = /Users/giovanniantonioni/IdeaProjects/Drivers\n",
       "getDatasetPath: (localPath: String)String\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:59:23.678107Z",
     "start_time": "2025-06-03T07:59:23.455529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "val numPartitions = spark.sparkContext.defaultParallelism\n",
    "val partitioner = new HashPartitioner(numPartitions)"
   ],
   "id": "724ff5ddc8c21481",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\n",
       "import org.apache.spark.storage.StorageLevel\n",
       "numPartitions: Int = 12\n",
       "partitioner: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@c\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:59:32.234914Z",
     "start_time": "2025-06-03T07:59:23.731092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import java.time.ZoneId\n",
    "val yellowDataset = spark.read\n",
    "  .schema(schemaYellow)\n",
    "  .option(\"recursiveFileLookup\", \"true\")\n",
    "  .parquet(getDatasetPath(yellowCab))\n",
    "  .select(\n",
    "    $\"VendorID\",\n",
    "    $\"tpep_pickup_datetime\".alias(\"pickup_datetime\"),\n",
    "    $\"tpep_dropoff_datetime\".alias(\"dropoff_datetime\"),\n",
    "    $\"fare_amount\",\n",
    "    $\"tip_amount\",\n",
    "    $\"payment_type\",\n",
    "    $\"trip_distance\",\n",
    "    $\"total_amount\",\n",
    "    $\"passenger_count\"\n",
    "  )\n",
    "\n",
    "  .na.drop()\n",
    "  .dropDuplicates()\n",
    "  .rdd\n",
    "  .map(r => Ride(\n",
    "    r.getInt(0),\n",
    "    r.getTimestamp(1),\n",
    "    r.getTimestamp(2),\n",
    "    r.getDouble(3),\n",
    "    r.getDouble(4),\n",
    "    r.getLong(5).toInt,\n",
    "    r.getDouble(6),\n",
    "    r.getDouble(7),\n",
    "    r.getLong(8).toInt,\n",
    "    \"yellow\"\n",
    "  ))\n",
    "  .keyBy(ride => ride.pickupDatetime.toInstant.atZone(ZoneId.systemDefault()).toLocalDate)\n",
    "\n",
    "val greenDataset = spark.read\n",
    "  .schema(schemaGreen)\n",
    "  .option(\"recursiveFileLookup\", \"true\")\n",
    "  .parquet(getDatasetPath(greenCab))\n",
    "  .select(\n",
    "    $\"VendorID\",\n",
    "    $\"lpep_pickup_datetime\".alias(\"pickup_datetime\"),\n",
    "    $\"lpep_dropoff_datetime\".alias(\"dropoff_datetime\"),\n",
    "    $\"fare_amount\",\n",
    "    $\"tip_amount\",\n",
    "    $\"payment_type\",\n",
    "    $\"trip_distance\",\n",
    "    $\"total_amount\",\n",
    "    $\"passenger_count\"\n",
    "  )\n",
    "  .na.drop()\n",
    "  .dropDuplicates()\n",
    "  .rdd\n",
    "  .map(r => Ride(\n",
    "    r.getInt(0),\n",
    "    r.getTimestamp(1),\n",
    "    r.getTimestamp(2),\n",
    "    r.getDouble(3),\n",
    "    r.getDouble(4),\n",
    "    r.getLong(5).toInt,\n",
    "    r.getDouble(6),\n",
    "    r.getDouble(7),\n",
    "    r.getLong(8).toInt,\n",
    "    \"green\"\n",
    "  ))\n",
    "  .keyBy(ride => ride.pickupDatetime.toInstant.atZone(ZoneId.systemDefault()).toLocalDate)\n",
    "\n",
    "val joined = yellowDataset.union(greenDataset)\n",
    "                          .partitionBy(partitioner)\n",
    "                          .persist(StorageLevel.MEMORY_ONLY)\n"
   ],
   "id": "508836e80d246fc1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.ZoneId\n",
       "yellowDataset: org.apache.spark.rdd.RDD[(java.time.LocalDate, Ride)] = MapPartitionsRDD[154] at keyBy at <console>:86\n",
       "greenDataset: org.apache.spark.rdd.RDD[(java.time.LocalDate, Ride)] = MapPartitionsRDD[164] at keyBy at <console>:118\n",
       "joined: org.apache.spark.rdd.RDD[(java.time.LocalDate, Ride)] = ShuffledRDD[166] at partitionBy at <console>:121\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once the Taxi data is loaded we procede to load the data relative to the weather\n",
   "id": "2c5c97a56b23c4d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:59:32.447326Z",
     "start_time": "2025-06-03T07:59:32.272844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "case class WeatherInfo(\n",
    "  wmoCode: Int,\n",
    "  dateOfRelevation: Timestamp,\n",
    "  description: String\n",
    ")"
   ],
   "id": "a065d40a31a06f01",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class WeatherInfo\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:59:33.013092Z",
     "start_time": "2025-06-03T07:59:32.463708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val weatherFileRDD = spark.read\n",
    "  .format(\"CSV\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(getDatasetPath(weatherData))\n",
    "  .rdd\n",
    "  .map(row => (row.getString(1).trim.toInt, row.getString(0).trim))\n",
    "\n",
    "val wmoLookupFileRDD = spark.read\n",
    "  .format(\"CSV\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(getDatasetPath(weatherWmoLookup))\n",
    "  .rdd\n",
    "  .map(row => {\n",
    "    val Array(codeStr, desc) = row.getString(0).split(\";\")\n",
    "    (codeStr.trim.toInt, desc.trim)\n",
    "  })\n",
    "\n",
    "val broadcastWmo = spark.sparkContext.broadcast(wmoLookupFileRDD.collectAsMap())"
   ],
   "id": "ce3b5d19f709ce5e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weatherFileRDD: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[182] at map at <console>:54\n",
       "wmoLookupFileRDD: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[198] at map at <console>:61\n",
       "broadcastWmo: org.apache.spark.broadcast.Broadcast[scala.collection.Map[Int,String]] = Broadcast(37)\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:59:33.179981Z",
     "start_time": "2025-06-03T07:59:33.026470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import java.time.LocalDate\n",
    "\n",
    "def mapWeatherRDD(\n",
    "    rdd: org.apache.spark.rdd.RDD[(Int, String)],\n",
    "    wmo: org.apache.spark.broadcast.Broadcast[scala.collection.Map[Int,String]],\n",
    "): org.apache.spark.rdd.RDD[WeatherInfo] = {\n",
    "\n",
    "  rdd.map {\n",
    "    case (code, dateStr) =>\n",
    "      val description = wmo.value.getOrElse(code, \"unknown\")\n",
    "      val timestamp = Timestamp.valueOf(LocalDate.parse(dateStr).atStartOfDay())\n",
    "      WeatherInfo(code, timestamp, description)\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "val transformedWeatherClassRDD =  mapWeatherRDD(weatherFileRDD, broadcastWmo)\n",
    "                                   .persist(StorageLevel.MEMORY_ONLY)"
   ],
   "id": "247701e0e523a6d1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.LocalDate\n",
       "mapWeatherRDD: (rdd: org.apache.spark.rdd.RDD[(Int, String)], wmo: org.apache.spark.broadcast.Broadcast[scala.collection.Map[Int,String]])org.apache.spark.rdd.RDD[WeatherInfo]\n",
       "transformedWeatherClassRDD: org.apache.spark.rdd.RDD[WeatherInfo] = MapPartitionsRDD[199] at map at <console>:56\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Filtering\n",
   "id": "2ef47d3e2b55cb20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:59:33.372329Z",
     "start_time": "2025-06-03T07:59:33.193447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val filtered = joined\n",
    "  .map (record => record._2)\n",
    "  .filter { ride =>\n",
    "      (ride.serviceType == \"yellow\" && Set(1, 2, 6, 7).contains(ride.vendorId)) ||\n",
    "      (ride.serviceType == \"green\" && Set(1, 2, 6).contains(ride.vendorId))\n",
    "  }\n",
    "  .filter(ride => ride.fareAmount > 0)\n",
    "  .filter(ride => ride.tipAmount >= 0)\n",
    "  .filter(ride => ride.tipAmount <= ride.fareAmount * 1.5)\n",
    "  .filter(ride => ride.paymentType >= 1 && ride.paymentType <= 6)\n",
    "  .filter(ride => ride.tripDistance > 0)\n",
    "  .filter(ride => ride.dropoffDatetime.after(ride.pickupDatetime))\n",
    "  .persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "joined.unpersist()"
   ],
   "id": "5f5978230df8424",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filtered: org.apache.spark.rdd.RDD[Ride] = MapPartitionsRDD[207] at filter at <console>:57\n",
       "res12: joined.type = ShuffledRDD[166] at partitionBy at <console>:121\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:59:33.538562Z",
     "start_time": "2025-06-03T07:59:33.385537Z"
    }
   },
   "cell_type": "code",
   "source": "case class RideWithDurationMinutes(info: Ride, durationMinutes: Double)",
   "id": "672170f5eee8ccfd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class RideWithDurationMinutes\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:59:33.699883Z",
     "start_time": "2025-06-03T07:59:33.552840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val withTripDuration = filtered.map(ride => {\n",
    "  val durationMin = (ride.dropoffDatetime.getTime - ride.pickupDatetime.getTime).toDouble / (1000 * 60)\n",
    "  RideWithDurationMinutes(ride, durationMin)\n",
    "})\n"
   ],
   "id": "4f8db2e31b1828dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "withTripDuration: org.apache.spark.rdd.RDD[RideWithDurationMinutes] = MapPartitionsRDD[208] at map at <console>:48\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:59:33.868393Z",
     "start_time": "2025-06-03T07:59:33.714275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def getQuantile(sortedRDD: org.apache.spark.rdd.RDD[(Long, Double)], quantile: Double, count: Long): Double = {\n",
    "  val idx = (quantile * count).toLong\n",
    "  sortedRDD.lookup(idx).headOption.getOrElse(sortedRDD.map(_._2).takeOrdered(1).head)\n",
    "}"
   ],
   "id": "c4bd112e54f8e9d1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getQuantile: (sortedRDD: org.apache.spark.rdd.RDD[(Long, Double)], quantile: Double, count: Long)Double\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:59:54.052199Z",
     "start_time": "2025-06-03T07:59:33.882674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val tripDistances = withTripDuration.map { case ride => ride.info.tripDistance }\n",
    "val tripDurations = withTripDuration.map { case ride => ride.durationMinutes }\n",
    "\n",
    "val distanceSorted = tripDistances.sortBy(identity).zipWithIndex().map(_.swap)\n",
    "val durationSorted = tripDurations.sortBy(identity).zipWithIndex().map(_.swap)"
   ],
   "id": "b0b19e08ccc143ae",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tripDistances: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[209] at map at <console>:48\n",
       "tripDurations: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[210] at map at <console>:49\n",
       "distanceSorted: org.apache.spark.rdd.RDD[(Long, Double)] = MapPartitionsRDD[217] at map at <console>:51\n",
       "durationSorted: org.apache.spark.rdd.RDD[(Long, Double)] = MapPartitionsRDD[224] at map at <console>:52\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:00:01.339593Z",
     "start_time": "2025-06-03T07:59:54.112849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val count = withTripDuration.count()\n",
    "val distanceLower = getQuantile(distanceSorted, 0.02, count)\n",
    "val distanceUpper = getQuantile(distanceSorted, 0.98, count)\n",
    "val durationLower = getQuantile(durationSorted, 0.02, count)\n",
    "val durationUpper = getQuantile(durationSorted, 0.98, count)\n",
    "\n",
    "val filteredWithoutOutliers = withTripDuration.filter { case ride =>\n",
    "  ride.info.tripDistance >= distanceLower && ride.info.tripDistance <= distanceUpper &&\n",
    "  ride.durationMinutes >= durationLower && ride.durationMinutes <= durationUpper\n",
    "}"
   ],
   "id": "1127139b9d75cad3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count: Long = 2627298\n",
       "distanceLower: Double = 0.37\n",
       "distanceUpper: Double = 19.08\n",
       "durationLower: Double = 2.533333333333333\n",
       "durationUpper: Double = 61.0\n",
       "filteredWithoutOutliers: org.apache.spark.rdd.RDD[RideWithDurationMinutes] = MapPartitionsRDD[233] at filter at <console>:60\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:00:01.623040Z",
     "start_time": "2025-06-03T08:00:01.348817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "case class RideWithEnrichedInformation(\n",
    "  rideWithMinutes: RideWithDurationMinutes,\n",
    "  hourOfDay: Int,\n",
    "  dayOfWeek: Int,\n",
    "  monthOfYear: Int,\n",
    "  year: Int,\n",
    "  isWeekend: Int,\n",
    "  tripHourBucket: String,\n",
    "  tipPercentage: Double,\n",
    "  speedMph: Double,\n",
    "  isRushHour: Boolean,\n",
    "  isLongTrip: Boolean\n",
    ")"
   ],
   "id": "76820f82f5a4cedb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class RideWithEnrichedInformation\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:00:01.819874Z",
     "start_time": "2025-06-03T08:00:01.666430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val enriched = filteredWithoutOutliers.map { case ride =>\n",
    "  val pickupCalendar = java.util.Calendar.getInstance()\n",
    "  pickupCalendar.setTime(ride.info.pickupDatetime)\n",
    "\n",
    "  val hourOfDay = pickupCalendar.get(java.util.Calendar.HOUR_OF_DAY)\n",
    "  val dayOfWeek = pickupCalendar.get(java.util.Calendar.DAY_OF_WEEK)\n",
    "  val monthOfYear = pickupCalendar.get(java.util.Calendar.MONTH)\n",
    "  val year = pickupCalendar.get(java.util.Calendar.YEAR)\n",
    "\n",
    "  val isWeekend = if (dayOfWeek == java.util.Calendar.SATURDAY || dayOfWeek == java.util.Calendar.SUNDAY) 1 else 0\n",
    "\n",
    "  val tripHourBucket = hourOfDay match {\n",
    "  case h if h >= 0 && h <= 5  => \"late_night\"\n",
    "  case h if h >= 6 && h <= 9  => \"morning\"\n",
    "  case h if h >= 10 && h <= 15 => \"midday\"\n",
    "  case h if h >= 16 && h <= 19 => \"evening\"\n",
    "  case _ => \"night\"\n",
    "  }\n",
    "\n",
    "  val tipPercentage = if (ride.info.totalAmount != 0) (ride.info.tipAmount / ride.info.totalAmount) * 100 else 0.0\n",
    "  val speedMph = if (ride.durationMinutes > 0) ride.info.tripDistance / (ride.durationMinutes / 60.0) else 0.0\n",
    "\n",
    "  val isRushHour = (dayOfWeek >= java.util.Calendar.MONDAY && dayOfWeek <= java.util.Calendar.FRIDAY) &&\n",
    "  ((hourOfDay >= 7 && hourOfDay <= 9) || (hourOfDay >= 16 && hourOfDay <= 18))\n",
    "\n",
    "  val isLongTrip = ride.info.tripDistance > 5.0 || ride.durationMinutes  > 20.0\n",
    "\n",
    "  RideWithEnrichedInformation(\n",
    "    ride,\n",
    "    hourOfDay,\n",
    "    dayOfWeek,\n",
    "    monthOfYear,\n",
    "    year,\n",
    "    isWeekend,\n",
    "    tripHourBucket,\n",
    "    tipPercentage,\n",
    "    speedMph,\n",
    "    isRushHour,\n",
    "    isLongTrip\n",
    "  )\n",
    "}"
   ],
   "id": "dc6ac93429a89172",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enriched: org.apache.spark.rdd.RDD[RideWithEnrichedInformation] = MapPartitionsRDD[234] at map at <console>:48\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:00:01.982795Z",
     "start_time": "2025-06-03T08:00:01.827872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "case class RideWithBins(\n",
    "  enrichedInfo: RideWithEnrichedInformation,\n",
    "  tripDistanceBin: String,\n",
    "  tripDurationBin: String,\n",
    "  fareAmountBin: String,\n",
    "  tipPercentageBin: String,\n",
    "  speedBin: String\n",
    ")"
   ],
   "id": "91a1d801e21bde7a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class RideWithBins\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:00:02.190286Z",
     "start_time": "2025-06-03T08:00:01.998079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val binned = enriched.map {\n",
    "  case ride =>\n",
    "\n",
    "    val binConfigs = Map(\n",
    "      \"trip_distance\" -> (Seq(1.0, 3.0, 6.0), Seq(\"0-1\", \"1-3\", \"3-6\", \"6+\")),\n",
    "      \"trip_duration_min\" -> (Seq(5.0, 15.0, 30.0), Seq(\"0-5\", \"5-15\", \"15-30\", \"30+\")),\n",
    "      \"fare_amount\" -> (Seq(5.0, 10.0, 20.0, 40.0), Seq(\"0-5\", \"5-10\", \"10-20\", \"20-40\", \"40+\")),\n",
    "      \"tip_percentage\" -> (Seq(5.0, 10.0, 20.0, 30.0), Seq(\"0-5%\", \"5-10%\", \"10-20%\", \"20-30%\", \"30%+\")),\n",
    "      \"speed_mph\" -> (Seq(5.0, 15.0, 30.0), Seq(\"0-5mph\", \"5-15mph\", \"15-30mph\", \"30mph+\"))\n",
    "    )\n",
    "\n",
    "    def assignBin(value: Double, bins: Seq[Double], labels: Seq[String]): String = {\n",
    "      require(labels.length == bins.length + 1, \"You need one more label than bin thresholds.\")\n",
    "      if (value < bins.head) labels.head\n",
    "      else {\n",
    "        val idx = bins.indexWhere(b => value < b)\n",
    "        if (idx == -1) labels.last\n",
    "        else labels(idx)\n",
    "      }\n",
    "    }\n",
    "\n",
    "    val tripDurationMin =\n",
    "    (\n",
    "      ride.rideWithMinutes.info.dropoffDatetime.getTime - ride.rideWithMinutes.info.pickupDatetime.getTime\n",
    "    ).toDouble / (1000 * 60)\n",
    "\n",
    "    val tripDistanceBin = assignBin(\n",
    "      ride.rideWithMinutes.info.tripDistance,\n",
    "      binConfigs(\"trip_distance\")._1,\n",
    "      binConfigs(\"trip_distance\")._2\n",
    "    )\n",
    "\n",
    "    val tripDurationBin = assignBin(\n",
    "      tripDurationMin,\n",
    "      binConfigs(\"trip_duration_min\")._1,\n",
    "      binConfigs(\"trip_duration_min\")._2\n",
    "    )\n",
    "\n",
    "    val fareAmountBin = assignBin(\n",
    "      ride.rideWithMinutes.info.fareAmount,\n",
    "      binConfigs(\"fare_amount\")._1,\n",
    "      binConfigs(\"fare_amount\")._2\n",
    "    )\n",
    "\n",
    "    val tipPercentageBin = assignBin(\n",
    "      ride.tipPercentage,\n",
    "      binConfigs(\"tip_percentage\")._1,\n",
    "      binConfigs(\"tip_percentage\")._2\n",
    "    )\n",
    "    val speedBin = assignBin(\n",
    "      ride.speedMph,\n",
    "      binConfigs(\"speed_mph\")._1,\n",
    "      binConfigs(\"speed_mph\")._2\n",
    "    )\n",
    "\n",
    "    RideWithBins(\n",
    "      ride,\n",
    "      tripDistanceBin,\n",
    "      tripDurationBin,\n",
    "      fareAmountBin,\n",
    "      tipPercentageBin,\n",
    "      speedBin\n",
    "    )\n",
    "}"
   ],
   "id": "9c007802694ba58f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binned: org.apache.spark.rdd.RDD[RideWithBins] = MapPartitionsRDD[235] at map at <console>:48\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Join weather and Ride data\n",
   "id": "16125d5aa598f314"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:00:02.382094Z",
     "start_time": "2025-06-03T08:00:02.205677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "case class RideWithWeather(\n",
    "  ride: RideWithBins,\n",
    "  weatherInfo: WeatherInfo\n",
    ")\n",
    "\n",
    "case class RideFinalOutput(\n",
    "  ride: RideWithBins,\n",
    "  weather: WeatherInfo,\n",
    "  generalWeather: String\n",
    ")"
   ],
   "id": "6b5df61c5a95c87b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class RideWithWeather\n",
       "defined class RideFinalOutput\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:00:02.690834Z",
     "start_time": "2025-06-03T08:00:02.433889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val weatherByDate = transformedWeatherClassRDD.keyBy(_.dateOfRelevation.toLocalDateTime.toLocalDate)\n",
    "val rideByDate = binned.keyBy(_.enrichedInfo.rideWithMinutes.info.pickupDatetime.toLocalDateTime.toLocalDate)\n",
    "\n",
    "val partitionedWeather = weatherByDate.partitionBy(partitioner).persist()\n",
    "val partitionedRides = rideByDate.partitionBy(partitioner).persist()\n",
    "\n",
    "val joinedWeather = partitionedRides.join(partitionedWeather).map {\n",
    "  case (_, (ride, weather)) => RideWithWeather(ride, weather)\n",
    "}\n",
    "\n",
    "partitionedWeather.unpersist()\n",
    "partitionedRides.unpersist()\n",
    "\n",
    "\n",
    "val finalRDD = joinedWeather.map { r =>\n",
    "\n",
    "  def generalWeatherLabel(wmoCode: Int): String = wmoCode match {\n",
    "    case c if Seq(0, 1).contains(c)              => \"clear\"\n",
    "    case c if Seq(2, 3, 4).contains(c)           => \"cloudy\"\n",
    "    case c if Seq(45, 48).contains(c)            => \"foggy\"\n",
    "    case c if (50 to 67).contains(c)             => \"rainy\"\n",
    "    case c if (70 to 77).contains(c)             => \"snowy\"\n",
    "    case c if (80 to 99).contains(c)             => \"stormy\"\n",
    "    case _                                       => \"unknown\"\n",
    "  }\n",
    "\n",
    "  val generalWeather = generalWeatherLabel(r.weatherInfo.wmoCode)\n",
    "  RideFinalOutput(r.ride, r.weatherInfo, generalWeather)\n",
    "}\n",
    "\n",
    "finalRDD.cache()"
   ],
   "id": "a9b9322b5113df85",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weatherByDate: org.apache.spark.rdd.RDD[(java.time.LocalDate, WeatherInfo)] = MapPartitionsRDD[236] at keyBy at <console>:58\n",
       "rideByDate: org.apache.spark.rdd.RDD[(java.time.LocalDate, RideWithBins)] = MapPartitionsRDD[237] at keyBy at <console>:59\n",
       "partitionedWeather: org.apache.spark.rdd.RDD[(java.time.LocalDate, WeatherInfo)] = ShuffledRDD[238] at partitionBy at <console>:61\n",
       "partitionedRides: org.apache.spark.rdd.RDD[(java.time.LocalDate, RideWithBins)] = ShuffledRDD[239] at partitionBy at <console>:62\n",
       "joinedWeather: org.apache.spark.rdd.RDD[RideWithWeather] = MapPartitionsRDD[243] at map at <console>:64\n",
       "finalRDD: org.apache.spark.rdd.RDD[RideFinalOutput] = MapPartitionsRDD[244] at map at <console>:72\n",
       "res13: finalRDD.type = MapPartitionsRDD[244] at map at <console>:72\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Export the results",
   "id": "96e5c4043bdad273"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:00:02.930501Z",
     "start_time": "2025-06-03T08:00:02.710365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val binFields = Seq(\n",
    "  \"tripDistanceBin\",\n",
    "  \"tripDurationBin\",\n",
    "  \"fareAmountBin\",\n",
    "  \"tipPercentageBin\",\n",
    "  \"speedBin\"\n",
    ")\n",
    "\n",
    "val binFieldPairs = for {\n",
    "  x <- binFields\n",
    "  y <- binFields\n",
    "  if x != y\n",
    "} yield (x, y)\n",
    "\n",
    "val combinationRDD: RDD[Row] = finalRDD.flatMap { row =>\n",
    "  binFieldPairs.map { case (x, y) =>\n",
    "    val matchVal : String => String = {\n",
    "      case \"tripDistanceBin\" => row.ride.tripDistanceBin\n",
    "      case \"tripDurationBin\" => row.ride.tripDurationBin\n",
    "      case \"fareAmountBin\" => row.ride.fareAmountBin\n",
    "      case \"tipPercentageBin\" => row.ride.tipPercentageBin\n",
    "      case \"speedBin\" => row.ride.speedBin\n",
    "    }\n",
    "\n",
    "    val binX = matchVal(x)\n",
    "    val binY = matchVal(y)\n",
    "\n",
    "    ((x, y, binX, binY), (row.ride.enrichedInfo.tipPercentage, 1L))\n",
    "  }\n",
    "}\n",
    ".reduceByKey((a, b) => (a._1 + b._1, a._2 + b._2))\n",
    ".map { case ((fieldX, fieldY, binX, binY), (sumTip, count)) =>\n",
    "  Row(fieldX, fieldY, binX, binY, sumTip / count)\n",
    "}"
   ],
   "id": "d4b4cddc458351ef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.rdd.RDD\n",
       "binFields: Seq[String] = List(tripDistanceBin, tripDurationBin, fareAmountBin, tipPercentageBin, speedBin)\n",
       "binFieldPairs: Seq[(String, String)] = List((tripDistanceBin,tripDurationBin), (tripDistanceBin,fareAmountBin), (tripDistanceBin,tipPercentageBin), (tripDistanceBin,speedBin), (tripDurationBin,tripDistanceBin), (tripDurationBin,fareAmountBin), (tripDurationBin,tipPercentageBin), (tripDurationBin,speedBin), (fareAmountBin,tripDistanceBin), (fareAmountBin,tripDurationBin), (fareAmountBin,tipPercentageBin), (fareAmountBin,speedBin), (tipPercentageBin,tripDistanceBin), (tipPercentageBin,tripDurationBin), (tipPercentageBin,fareAmountBin), (tipPercentageBin,speedBin), (speedBin,tripDistanceBin), (speedBin,tripDurationBin), (...\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:00:03.188982Z",
     "start_time": "2025-06-03T08:00:02.979239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val binFields = Seq(\n",
    "  \"tripDistanceBin\",\n",
    "  \"tripDurationBin\",\n",
    "  \"fareAmountBin\",\n",
    "  \"tipPercentageBin\",\n",
    "  \"speedBin\"\n",
    ")\n",
    "\n",
    "val allTipByBinRDD: RDD[Row] = finalRDD.flatMap { row =>\n",
    "  binFields.map { binFeature =>\n",
    "    val bin = binFeature match {\n",
    "      case \"fareAmountBin\" => row.ride.fareAmountBin\n",
    "      case \"tripDistanceBin\" => row.ride.tripDistanceBin\n",
    "      case \"tripDurationBin\" => row.ride.tripDurationBin\n",
    "      case \"tipPercentageBin\" => row.ride.tipPercentageBin\n",
    "      case \"speedBin\" => row.ride.speedBin\n",
    "    }\n",
    "    ((binFeature, bin), (row.ride.enrichedInfo.tipPercentage, 1L))\n",
    "  }\n",
    "}\n",
    ".reduceByKey((a, b) => (a._1 + b._1, a._2 + b._2))\n",
    ".map { case ((feature, bin), (sumTip, count)) =>\n",
    "  Row(feature, bin, sumTip / count)\n",
    "}"
   ],
   "id": "e7428c29f1ae2cc0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binFields: Seq[String] = List(tripDistanceBin, tripDurationBin, fareAmountBin, tipPercentageBin, speedBin)\n",
       "allTipByBinRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[250] at map at <console>:71\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:00:03.465069Z",
     "start_time": "2025-06-03T08:00:03.236165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val avgTipByWeather = finalRDD\n",
    "    .map(r => (r.generalWeather, (r.ride.enrichedInfo.tipPercentage, 1L)))\n",
    "    .reduceByKey((a, b) => (a._1 + b._1, a._2 + b._2))\n",
    "    .map { case (weather, (sumTip, count)) => Row(weather, sumTip / count) }"
   ],
   "id": "498f0a7294af7370",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgTipByWeather: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[253] at map at <console>:52\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:00:03.693072Z",
     "start_time": "2025-06-03T08:00:03.517914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val tipByHourBucket = finalRDD\n",
    "    .map(r => (r.ride.enrichedInfo.tripHourBucket, (r.ride.enrichedInfo.tipPercentage, 1L)))\n",
    "    .reduceByKey((a, b) => (a._1 + b._1, a._2 + b._2))\n",
    "    .map { case (bucket, (sumTip, count)) => Row(bucket, sumTip / count) }\n"
   ],
   "id": "cc54e6b6e178f411",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tipByHourBucket: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[256] at map at <console>:52\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:00:03.936538Z",
     "start_time": "2025-06-03T08:00:03.702810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val allTipByBinSchema = StructType(Seq(\n",
    "  StructField(\"feature\", StringType),\n",
    "  StructField(\"bin\", StringType),\n",
    "  StructField(\"avg_tip_pct\", DoubleType)\n",
    "))\n",
    "\n",
    "val dfAllTipByBinDF = spark.createDataFrame(allTipByBinRDD, allTipByBinSchema)\n",
    "\n",
    "val schema = StructType(Seq(\n",
    "  StructField(\"featureX\", StringType),\n",
    "  StructField(\"featureY\", StringType),\n",
    "  StructField(\"binX\", StringType),\n",
    "  StructField(\"binY\", StringType),\n",
    "  StructField(\"avg_tip_pct\", DoubleType)\n",
    "))\n",
    "\n",
    "val combinationDF = spark.createDataFrame(combinationRDD, schema)"
   ],
   "id": "1fa270aa009fd5e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allTipByBinSchema: org.apache.spark.sql.types.StructType = StructType(StructField(feature,StringType,true),StructField(bin,StringType,true),StructField(avg_tip_pct,DoubleType,true))\n",
       "dfAllTipByBinDF: org.apache.spark.sql.DataFrame = [feature: string, bin: string ... 1 more field]\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(featureX,StringType,true),StructField(featureY,StringType,true),StructField(binX,StringType,true),StructField(binY,StringType,true),StructField(avg_tip_pct,DoubleType,true))\n",
       "combinationDF: org.apache.spark.sql.DataFrame = [featureX: string, featureY: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:00:04.125765Z",
     "start_time": "2025-06-03T08:00:03.975446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val weatherSchema = StructType(Seq(\n",
    "  StructField(\"weather\", StringType),\n",
    "  StructField(\"avg_tip_pct\", DoubleType)\n",
    "))\n",
    "\n",
    "val weatherDF = spark.createDataFrame(avgTipByWeather, weatherSchema)"
   ],
   "id": "1603d21218a904e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weatherSchema: org.apache.spark.sql.types.StructType = StructType(StructField(weather,StringType,true),StructField(avg_tip_pct,DoubleType,true))\n",
       "weatherDF: org.apache.spark.sql.DataFrame = [weather: string, avg_tip_pct: double]\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:00:04.357046Z",
     "start_time": "2025-06-03T08:00:04.134761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val bucketSchema = StructType(Seq(\n",
    "  StructField(\"hour_bucket\", StringType),\n",
    "  StructField(\"avg_tip_pct\", DoubleType)\n",
    "))\n",
    "\n",
    "val hourBucketDF = spark.createDataFrame(tipByHourBucket, bucketSchema)"
   ],
   "id": "a5d5170b37676ee5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bucketSchema: org.apache.spark.sql.types.StructType = StructType(StructField(hour_bucket,StringType,true),StructField(avg_tip_pct,DoubleType,true))\n",
       "hourBucketDF: org.apache.spark.sql.DataFrame = [hour_bucket: string, avg_tip_pct: double]\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
