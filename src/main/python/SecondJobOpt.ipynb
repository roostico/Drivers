{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Big Data project A.Y. 2024-2025\n",
    "\n",
    "## Members\n",
    "\n",
    "- Giovanni Antonioni\n",
    "- Luca Rubboli - 0001083742\n",
    "\n",
    "## Second job"
   ],
   "id": "562ba4dfdebe9b45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:12.955798Z",
     "start_time": "2025-07-06T14:22:12.326909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder\n",
    "  .appName(\"Second job with RDDs\")\n",
    "  .getOrCreate()\n",
    "\n",
    "\n",
    "val sc = spark.sparkContext"
   ],
   "id": "69d266c225a59a5d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6a007083\n",
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@66755c3\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Definition of parameters for the job\n",
    "\n",
    "Here are defined the variables used for the snippet.\n"
   ],
   "id": "572fc8ae7ed1263c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:13.361941Z",
     "start_time": "2025-07-06T14:22:13.059617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val datasetName = \"green\"\n",
    "val datasetFolder = \"./dataset\"\n",
    "val outputDir = s\"/output/secondJobRDD/$datasetName\"\n",
    "val pathToFiles = s\"$datasetFolder/$datasetName\"\n",
    "val weatherData = s\"$datasetFolder/weather/weather_data_2017_2024.csv\"\n",
    "val weatherWmoLookup = s\"$datasetFolder/weather/wmo_lookup_codes.csv\""
   ],
   "id": "3a3e52e6a1ed4e06",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasetName: String = green\n",
       "datasetFolder: String = ./dataset\n",
       "outputDir: String = /output/secondJobRDD/green\n",
       "pathToFiles: String = ./dataset/green\n",
       "weatherData: String = ./dataset/weather/weather_data_2017_2024.csv\n",
       "weatherWmoLookup: String = ./dataset/weather/wmo_lookup_codes.csv\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Columns for the analysis",
   "id": "34fe46529ca49fad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:13.935577Z",
     "start_time": "2025-07-06T14:22:13.466195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val commonFields = List(\n",
    "  StructField(\"VendorID\", IntegerType),\n",
    "  StructField(\"fare_amount\", DoubleType),\n",
    "  StructField(\"tip_amount\", DoubleType),\n",
    "  StructField(\"payment_type\", LongType),\n",
    "  StructField(\"trip_distance\", DoubleType),\n",
    "  StructField(\"total_amount\", DoubleType)\n",
    ")\n",
    "\n",
    "val schemaYellow = StructType(\n",
    "  StructField(\"tpep_pickup_datetime\", TimestampType) ::\n",
    "  StructField(\"tpep_dropoff_datetime\", TimestampType) ::\n",
    "  commonFields\n",
    ")\n",
    "\n",
    "val schemaGreen = StructType(\n",
    "  StructField(\"lpep_pickup_datetime\", TimestampType) ::\n",
    "  StructField(\"lpep_dropoff_datetime\", TimestampType) ::\n",
    "  commonFields\n",
    ")"
   ],
   "id": "1a46b90b7485a526",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "commonFields: List[org.apache.spark.sql.types.StructField] = List(StructField(VendorID,IntegerType,true), StructField(fare_amount,DoubleType,true), StructField(tip_amount,DoubleType,true), StructField(payment_type,LongType,true), StructField(trip_distance,DoubleType,true), StructField(total_amount,DoubleType,true))\n",
       "schemaYellow: org.apache.spark.sql.types.StructType = StructType(StructField(tpep_pickup_datetime,TimestampType,true),StructField(tpep_dropoff_datetime,TimestampType,true),StructField(VendorID,IntegerType,true),StructField(fare_amount,DoubleType,true),StructField(tip_amount,DoubleType,true),StructField(payment_type,LongType,true),StructField(trip_distance,DoubleType,true),StructField(total_amount,DoubleType,true))\n",
       "schemaGreen: org.apache.sp...\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Load Datasets\n",
    "\n",
    "First we want to load the dataset relative to the taxi data."
   ],
   "id": "b9d6ad666c105aab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:14.404866Z",
     "start_time": "2025-07-06T14:22:14.063129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val projectDir: String = \"/Users/giovanniantonioni/IdeaProjects/Drivers\"\n",
    "def getDatasetPath(localPath: String): String = {\n",
    "  \"file://\" + projectDir + \"/\" + localPath\n",
    "}"
   ],
   "id": "834fad309135f509",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "projectDir: String = /Users/giovanniantonioni/IdeaProjects/Drivers\n",
       "getDatasetPath: (localPath: String)String\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:16.474123Z",
     "start_time": "2025-07-06T14:22:14.490028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val (schema, pickupCol, dropoffCol) = datasetName match {\n",
    "  case \"yellow\" => (schemaYellow, \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\")\n",
    "  case _        => (schemaGreen, \"lpep_pickup_datetime\", \"lpep_dropoff_datetime\")\n",
    "}\n",
    "\n",
    "val loadedDataset = spark.read\n",
    "  .schema(schema)\n",
    "  .option(\"recursiveFileLookup\", \"true\")\n",
    "  .parquet(getDatasetPath(pathToFiles))\n",
    "  .select(\n",
    "    $\"VendorID\",\n",
    "    col(pickupCol).alias(\"pickup_datetime\"),\n",
    "    col(dropoffCol).alias(\"dropoff_datetime\"),\n",
    "    $\"fare_amount\",\n",
    "    $\"tip_amount\",\n",
    "    $\"payment_type\",\n",
    "    $\"trip_distance\",\n",
    "    $\"total_amount\"\n",
    "  )\n",
    "  .na.drop()\n",
    "  .dropDuplicates()\n",
    "  .rdd"
   ],
   "id": "508836e80d246fc1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(lpep_pickup_datetime,TimestampType,true),StructField(lpep_dropoff_datetime,TimestampType,true),StructField(VendorID,IntegerType,true),StructField(fare_amount,DoubleType,true),StructField(tip_amount,DoubleType,true),StructField(payment_type,LongType,true),StructField(trip_distance,DoubleType,true),StructField(total_amount,DoubleType,true))\n",
       "pickupCol: String = lpep_pickup_datetime\n",
       "dropoffCol: String = lpep_dropoff_datetime\n",
       "loadedDataset: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[279] at rdd at <console>:84\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n",
   "id": "2c5c97a56b23c4d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Filtering\n",
   "id": "2ef47d3e2b55cb20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:16.789827Z",
     "start_time": "2025-07-06T14:22:16.493886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "def filterDataset(dataset: RDD[Row], name: String): RDD[Row] = {\n",
    "  val allowedYellowVendorId = Set(1, 2, 6, 7)\n",
    "  val allowedGreenVendorId = Set(1, 2, 6)\n",
    "\n",
    "  dataset.filter { case row =>\n",
    "        val allowedIds = if (name == \"yellow\") allowedYellowVendorId else allowedGreenVendorId\n",
    "        val vendorId = row.getInt(0)\n",
    "        allowedIds.contains(vendorId)\n",
    "      }\n",
    "      .filter(row => row.getDouble(3) > 0)\n",
    "      .filter(row => row.getDouble(4) >= 0)\n",
    "      .filter(row => row.getDouble(4) <= row.getDouble(3) * 1.5)\n",
    "      .filter(row => row.getDouble(6) > 0)\n",
    "      .filter{ row =>\n",
    "        val dropOffDateTime = row.getTimestamp(2)\n",
    "        val pickupDateTime = row.getTimestamp(1)\n",
    "        dropOffDateTime.after(pickupDateTime)\n",
    "      }\n",
    "}\n",
    "\n",
    "val filtered = filterDataset(loadedDataset, datasetName)"
   ],
   "id": "5f5978230df8424",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.rdd.RDD\n",
       "filterDataset: (dataset: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row], name: String)org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "filtered: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[285] at filter at <console>:77\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:16.965603Z",
     "start_time": "2025-07-06T14:22:16.818755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val withTripDuration = filtered.map { row =>\n",
    "  val durationMin = (row.getTimestamp(2).getTime - row.getTimestamp(1).getTime).toDouble / (1000 * 60)\n",
    "  (row, durationMin)\n",
    "}"
   ],
   "id": "4f8db2e31b1828dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "withTripDuration: org.apache.spark.rdd.RDD[(org.apache.spark.sql.Row, Double)] = MapPartitionsRDD[286] at map at <console>:61\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:18.417542Z",
     "start_time": "2025-07-06T14:22:16.995949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val tripDistances = withTripDuration.map { case (row, _) => row.getLong(5).toInt }\n",
    "val tripDurations = withTripDuration.map { case (_, duration) => duration }\n",
    "\n",
    "val distanceDF = tripDistances.toDF(\"trip_distance\")\n",
    "val durationDF = tripDurations.toDF(\"trip_duration\")\n",
    "\n",
    "val tripDistanceOutlier = distanceDF.stat.approxQuantile(\"trip_distance\", Array(0.02, 0.98), 0.01)\n",
    "val tripDurationOutlier = durationDF.stat.approxQuantile(\"trip_duration\", Array(0.02, 0.98), 0.01)\n"
   ],
   "id": "b0b19e08ccc143ae",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tripDistances: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[287] at map at <console>:65\n",
       "tripDurations: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[288] at map at <console>:66\n",
       "distanceDF: org.apache.spark.sql.DataFrame = [trip_distance: int]\n",
       "durationDF: org.apache.spark.sql.DataFrame = [trip_duration: double]\n",
       "tripDistanceOutlier: Array[Double] = Array(1.0, 2.0)\n",
       "tripDurationOutlier: Array[Double] = Array(2.25, 42.083333333333336)\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:34.409067Z",
     "start_time": "2025-07-06T14:22:34.060719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filterOutOutlier(\n",
    "  dataset: RDD[(Row, Double)],\n",
    "  tripDistanceOutlier: Array[Double],\n",
    "  tripDurationOutlier: Array[Double]\n",
    "): RDD[(Row, Double)] = {\n",
    "\n",
    "  val distanceLower = tripDistanceOutlier(0)\n",
    "  val distanceUpper = tripDistanceOutlier(1)\n",
    "  val durationLower = tripDurationOutlier(0)\n",
    "  val durationUpper = tripDurationOutlier(1)\n",
    "\n",
    "  dataset.filter { case (row, duration) =>\n",
    "    val tripDistance = row.getLong(5).toInt\n",
    "    tripDistance >= distanceLower && tripDistance <= distanceUpper &&\n",
    "    duration >= durationLower && duration <= durationUpper\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "val filteredOut = filterOutOutlier(withTripDuration, tripDistanceOutlier, tripDurationOutlier).cache()\n",
    "val enriched = filteredOut.map { case (row, duration) =>\n",
    "  val pickupCalendar = java.util.Calendar.getInstance()\n",
    "  pickupCalendar.setTime(row.getTimestamp(1))\n",
    "\n",
    "  val hourOfDay = pickupCalendar.get(java.util.Calendar.HOUR_OF_DAY)\n",
    "\n",
    "  val tipAmount = row.getDouble(4)\n",
    "  val totalAmount = row.getDouble(7)\n",
    "\n",
    "  val tipPercentage = if (totalAmount != 0) (tipAmount / totalAmount) * 100 else 0.0\n",
    "\n",
    "  val tripDistance = row.getDouble(6)\n",
    "  val speedMph = if (duration > 0) tripDistance / (duration / 60.0) else 0.0\n",
    "\n",
    "  (row, duration, hourOfDay, tipPercentage, speedMph)\n",
    "}"
   ],
   "id": "1127139b9d75cad3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterOutOutlier: (dataset: org.apache.spark.rdd.RDD[(org.apache.spark.sql.Row, Double)], tripDistanceOutlier: Array[Double], tripDurationOutlier: Array[Double])org.apache.spark.rdd.RDD[(org.apache.spark.sql.Row, Double)]\n",
       "filteredOut: org.apache.spark.rdd.RDD[(org.apache.spark.sql.Row, Double)] = MapPartitionsRDD[366] at filter at <console>:81\n",
       "enriched: org.apache.spark.rdd.RDD[(org.apache.spark.sql.Row, Double, Int, Double, Double)] = MapPartitionsRDD[367] at map at <console>:90\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:19.348255Z",
     "start_time": "2025-07-06T14:22:19.007839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val binned =  enriched.map { case (row, duration, hourOfDay, tipPercentage, speedMph) =>\n",
    "  val binConfigs = Map(\n",
    "    \"trip_distance\" -> (Seq(1.0, 3.0, 6.0), Seq(\"0-1\", \"1-3\", \"3-6\", \"6+\")),\n",
    "    \"trip_duration_min\" -> (Seq(5.0, 15.0, 30.0), Seq(\"0-5\", \"5-15\", \"15-30\", \"30+\")),\n",
    "    \"fare_amount\" -> (Seq(5.0, 10.0, 20.0, 40.0), Seq(\"0-5\", \"5-10\", \"10-20\", \"20-40\", \"40+\")),\n",
    "    \"tip_percentage\" -> (Seq(5.0, 10.0, 20.0, 30.0), Seq(\"0-5%\", \"5-10%\", \"10-20%\", \"20-30%\", \"30%+\")),\n",
    "    \"speed_mph\" -> (Seq(5.0, 15.0, 30.0), Seq(\"0-5mph\", \"5-15mph\", \"15-30mph\", \"30mph+\"))\n",
    "  )\n",
    "\n",
    "  def assignBin(value: Double, bins: Seq[Double], labels: Seq[String]): String = {\n",
    "    require(labels.length == bins.length + 1, \"You need one more label than bin thresholds.\")\n",
    "\n",
    "    if (value < bins.head) labels.head\n",
    "    else {\n",
    "      val idx = bins.indexWhere(b => value < b)\n",
    "      if (idx == -1) labels.last\n",
    "      else labels(idx)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  val tripDistance = row.getDouble(6)\n",
    "  val tripDistanceBin = assignBin(\n",
    "    tripDistance,\n",
    "    binConfigs(\"trip_distance\")._1,\n",
    "    binConfigs(\"trip_distance\")._2\n",
    "  )\n",
    "\n",
    "  val tripDurationBin = assignBin(\n",
    "    duration,\n",
    "    binConfigs(\"trip_duration_min\")._1,\n",
    "    binConfigs(\"trip_duration_min\")._2\n",
    "  )\n",
    "\n",
    "  val fareAmount = row.getDouble(3)\n",
    "  val fareAmountBin = assignBin(\n",
    "    fareAmount,\n",
    "    binConfigs(\"fare_amount\")._1,\n",
    "    binConfigs(\"fare_amount\")._2\n",
    "  )\n",
    "\n",
    "  val speedBin = assignBin(\n",
    "    speedMph,\n",
    "    binConfigs(\"speed_mph\")._1,\n",
    "    binConfigs(\"speed_mph\")._2\n",
    "  )\n",
    "\n",
    "  def tripHourBucket(hour: Int): String = hour match {\n",
    "    case h if h >= 0 && h <= 5  => \"late_night\"\n",
    "    case h if h >= 6 && h <= 9  => \"morning\"\n",
    "    case h if h >= 10 && h <= 15 => \"midday\"\n",
    "    case h if h >= 16 && h <= 19 => \"evening\"\n",
    "    case _ => \"night\"\n",
    "  }\n",
    "\n",
    "  val hourBin = tripHourBucket(hourOfDay)\n",
    "  (row, tripDistanceBin, tripDurationBin, fareAmountBin, speedBin, hourBin, tipPercentage)\n",
    "}"
   ],
   "id": "dc6ac93429a89172",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binned: org.apache.spark.rdd.RDD[(org.apache.spark.sql.Row, String, String, String, String, String, Double)] = MapPartitionsRDD[309] at map at <console>:61\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:20.256737Z",
     "start_time": "2025-07-06T14:22:19.395610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val weatherFileRDD = spark.read\n",
    "  .format(\"CSV\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(getDatasetPath(weatherData))\n",
    "  .rdd\n",
    "  .map { row =>\n",
    "    val code = row.getString(1).trim.toInt\n",
    "    val date = row.getString(0).trim\n",
    "    (code, date)\n",
    "  }\n",
    "\n",
    "val wmoLookupFile = spark.read\n",
    "  .format(\"CSV\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(getDatasetPath(weatherWmoLookup))\n",
    "  .rdd\n",
    "\n",
    "val wmoLookupPairRDD = wmoLookupFile.map { row =>\n",
    "  val data = row.getString(0).split(\";\")\n",
    "  val code =data(0).trim.toInt\n",
    "  val description = data(1).trim\n",
    "  (code, description)\n",
    "}\n",
    "\n",
    "import java.time.LocalDate\n",
    "import java.sql.Timestamp\n",
    "\n",
    "val transformedWeatherClassRDD = weatherFileRDD\n",
    "  .join(wmoLookupPairRDD)\n",
    "  .map(row => {\n",
    "    val (id, (date, description)) = row\n",
    "    val formattedDate  = LocalDate.parse(date)\n",
    "    val timestamp = Timestamp.valueOf(formattedDate.atStartOfDay())\n",
    "    (id, timestamp, description)\n",
    "})\n"
   ],
   "id": "91a1d801e21bde7a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weatherFileRDD: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[325] at map at <console>:72\n",
       "wmoLookupFile: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[340] at rdd at <console>:81\n",
       "wmoLookupPairRDD: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[341] at map at <console>:84\n",
       "import java.time.LocalDate\n",
       "import java.sql.Timestamp\n",
       "transformedWeatherClassRDD: org.apache.spark.rdd.RDD[(Int, java.sql.Timestamp, String)] = MapPartitionsRDD[345] at map at <console>:96\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:21.017622Z",
     "start_time": "2025-07-06T14:22:20.354242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val weatherMap = transformedWeatherClassRDD\n",
    ".map { row => (row._2.toLocalDateTime.toLocalDate, row) }\n",
    ".collectAsMap()\n",
    "\n",
    "val weatherBC = spark.sparkContext.broadcast(weatherMap)\n",
    "\n",
    "val rideByDate = binned.map {data  =>\n",
    "  val pickupDateTime = data._1.getTimestamp(1).toLocalDateTime.toLocalDate\n",
    "  (pickupDateTime, data)\n",
    "}"
   ],
   "id": "9c007802694ba58f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weatherMap: scala.collection.Map[java.time.LocalDate,(Int, java.sql.Timestamp, String)] = Map(2020-06-11 -> (65,2020-06-11 00:00:00.0,Heavy rain), 2018-06-25 -> (3,2018-06-25 00:00:00.0,Clouds generally forming or developing), 2020-11-05 -> (3,2020-11-05 00:00:00.0,Clouds generally forming or developing), 2018-01-04 -> (75,2018-01-04 00:00:00.0,Heavy continuous fall of snowflakes), 2018-11-19 -> (3,2018-11-19 00:00:00.0,Clouds generally forming or developing), 2023-04-15 -> (63,2023-04-15 00:00:00.0,Moderate rain), 2024-12-03 -> (1,2024-12-03 00:00:00.0,Clouds generally dissolving or becoming less developed), 2021-04-29 -> (55,2021-04-29 00:00:00.0,Heavy Drizzle), 2018-01-13 -> (63,2018-01-13 00:00:00.0,Moderate rain), 2018-12-27 -> (3,2018-12-27 00:00:00.0,Clouds generally forming or d...\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Join weather and Ride data\n",
   "id": "16125d5aa598f314"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:21.396464Z",
     "start_time": "2025-07-06T14:22:21.091949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.broadcast.Broadcast\n",
    "\n",
    "def joinWithBroadcast(\n",
    "    dataset: RDD[(Row, String, String, String, String, String, Double)],\n",
    "    broadcast:  Broadcast[scala.collection.Map[LocalDate, (Int, Timestamp, String)]]\n",
    ") = {\n",
    "  dataset.map { data =>\n",
    "    val pickupDateTime = data._1.getTimestamp(1).toLocalDateTime.toLocalDate\n",
    "    val weatherOpt = broadcast.value.get(pickupDateTime)\n",
    "    (data, weatherOpt)\n",
    "  }\n",
    "}\n",
    "\n",
    "val joinedWeather = joinWithBroadcast(binned, weatherBC).filter(_._2.isDefined)\n",
    "\n",
    "val joinedWeatherCleaned = joinedWeather.map {\n",
    "  case (ride, Some(weather)) => (ride, weather)\n",
    "}\n",
    "\n",
    "val finalRDD = joinedWeatherCleaned.map { case (ride, weather) =>\n",
    "  def generalWeatherLabel(wmoCode: Int): String = wmoCode match {\n",
    "    case c if Seq(0, 1).contains(c)              => \"clear\"\n",
    "    case c if Seq(2, 3, 4).contains(c)           => \"cloudy\"\n",
    "    case c if Seq(45, 48).contains(c)            => \"foggy\"\n",
    "    case c if (50 to 67).contains(c)       => \"rainy\"\n",
    "    case c if (70 to 77).contains(c)       => \"snowy\"\n",
    "    case c if (80 to 99).contains(c)       => \"stormy\"\n",
    "    case _                                       => \"unknown\"\n",
    "  }\n",
    "\n",
    "  val generalWeather = generalWeatherLabel(weather._1)\n",
    "  (ride, weather._1, generalWeather)\n",
    "}"
   ],
   "id": "6b5df61c5a95c87b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:82: warning: match may not be exhaustive.\n",
       "It would fail on the following input: (_, None)\n",
       "       val joinedWeatherCleaned = joinedWeather.map {\n",
       "                                                    ^\n",
       "import org.apache.spark.broadcast.Broadcast\n",
       "joinWithBroadcast: (dataset: org.apache.spark.rdd.RDD[(org.apache.spark.sql.Row, String, String, String, String, String, Double)], broadcast: org.apache.spark.broadcast.Broadcast[scala.collection.Map[java.time.LocalDate,(Int, java.sql.Timestamp, String)]])org.apache.spark.rdd.RDD[((org.apache.spark.sql.Row, String, String, String, String, String, Double), Option[(Int, java.sql.Timestamp, String)])]\n",
       "joinedWeather: org.apache.spark.rdd.RDD[((org.apache.spark.sql.Row, String, String, String, String, String, Double), Option[(Int, java.sql.Timestamp, String)])] = MapPartitionsRDD[349] at filter at <console>:80\n",
       "joinedWeatherCleaned: org.apache.spark.rdd.RDD[((org.apache.spark.sql.Row, String, String, String, String, String, Double), (Int, java.sql.T...\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Export the results",
   "id": "96e5c4043bdad273"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:21.699866Z",
     "start_time": "2025-07-06T14:22:21.414415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val binFields = Seq(\n",
    "  \"tripDistanceBin\",\n",
    "  \"tripDurationBin\",\n",
    "  \"fareAmountBin\",\n",
    "  \"speedBin\"\n",
    ")\n",
    "\n",
    "val keyedRDD = finalRDD.flatMap{ case (ride, code, generalWeather) =>\n",
    "  binFields.map { field =>\n",
    "    val bin = field match {\n",
    "      case \"fareAmountBin\" => ride._4\n",
    "      case \"tripDistanceBin\" => ride._2\n",
    "      case \"tripDurationBin\" => ride._3\n",
    "      case \"speedBin\" => ride._5\n",
    "    }\n",
    "    val key = s\"${field}_$bin\"\n",
    "    (key, ride)\n",
    "  }\n",
    "}\n"
   ],
   "id": "d4b4cddc458351ef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binFields: Seq[String] = List(tripDistanceBin, tripDurationBin, fareAmountBin, speedBin)\n",
       "keyedRDD: org.apache.spark.rdd.RDD[(String, (org.apache.spark.sql.Row, String, String, String, String, String, Double))] = MapPartitionsRDD[352] at flatMap at <console>:72\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:22.040872Z",
     "start_time": "2025-07-06T14:22:21.730609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "val numPartitions = spark.sparkContext.defaultParallelism\n",
    "val partitioner = new HashPartitioner(numPartitions)\n",
    "\n",
    "val distributedKeyedRDD = keyedRDD.mapValues{ case ride =>\n",
    "  val tipPercentage = ride._7\n",
    "  (tipPercentage, 1L)\n",
    "}\n",
    ".partitionBy(partitioner)\n",
    ".persist(StorageLevel.MEMORY_ONLY)"
   ],
   "id": "e7428c29f1ae2cc0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\n",
       "import org.apache.spark.storage.StorageLevel\n",
       "numPartitions: Int = 12\n",
       "partitioner: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@c\n",
       "distributedKeyedRDD: org.apache.spark.rdd.RDD[(String, (Double, Long))] = ShuffledRDD[354] at partitionBy at <console>:77\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:27.547700Z",
     "start_time": "2025-07-06T14:22:22.066610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val avgTipRDD = distributedKeyedRDD\n",
    ".reduceByKey((a, b) => (a._1 + b._1, a._2 + b._2))\n",
    ".map {\n",
    "case (id, (sum, count)) => Row(id, sum / count)\n",
    "}\n",
    "\n",
    "distributedKeyedRDD.unpersist()\n",
    "\n",
    "val allTipByBinSchema = StructType(Seq(\n",
    "  StructField(\"feature\", StringType),\n",
    "  StructField(\"avg_tip_pct\", DoubleType)\n",
    "))\n",
    "\n",
    "spark.createDataFrame(avgTipRDD, allTipByBinSchema)\n",
    ".write\n",
    ".mode(\"overwrite\")\n",
    ".parquet(getDatasetPath(s\"$outputDir/tip_avg_per_bin/all_features\"))"
   ],
   "id": "d02443bf0f0a59e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgTipRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[356] at map at <console>:73\n",
       "allTipByBinSchema: org.apache.spark.sql.types.StructType = StructType(StructField(feature,StringType,true),StructField(avg_tip_pct,DoubleType,true))\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:22:29.448626Z",
     "start_time": "2025-07-06T14:22:27.570458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val avgTipByWeather = finalRDD\n",
    "    .map(r => (r._3, (r._1._7, 1L)))\n",
    "    .reduceByKey((a, b) => (a._1 + b._1, a._2 + b._2))\n",
    "    .map { case (weather, (sumTip, count)) => Row(weather, sumTip / count) }\n",
    "\n",
    "val weatherSchema = StructType(Seq(\n",
    "  StructField(\"weather\", StringType),\n",
    "  StructField(\"avg_tip_pct\", DoubleType)\n",
    "))\n",
    "\n",
    "spark.createDataFrame(avgTipByWeather, weatherSchema)\n",
    "  .write\n",
    "  .mode(\"overwrite\")\n",
    "  .parquet(getDatasetPath(s\"$outputDir/avg_tip_by_weather\"))"
   ],
   "id": "498f0a7294af7370",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgTipByWeather: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[362] at map at <console>:74\n",
       "weatherSchema: org.apache.spark.sql.types.StructType = StructType(StructField(weather,StringType,true),StructField(avg_tip_pct,DoubleType,true))\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
